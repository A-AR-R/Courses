\documentclass[11pt]{article}
\input{/Users/markwang/.preamble}
\begin{document}

% arg1=pdfurl arg2=pagenum arg3=sectiontitle
\newcommand{\linksection}[3][../../bishop_pattern_recognition_and_machine_learning.pdf]{
    \subsection*{\href[page=#2]{#1}{#3}}
}

\renewcommand{\norm}[1]{\left\lVert#1\right\rVert}
\renewcommand{\E}[2][]{\mathbb{E}_{#1}\{#2\}}
\newcommand{\var}[1]{var\{#1\}}
\newcommand{\cov}[1]{cov\{#1\}}
\newcommand{\normal}[1]{\mathcal{N}\left(#1\right)}
\newcommand{\exponents}[1]{exp\left\{#1\right\}}

\newcommand{\bmu}{\boldsymbol{\mu}}

\newcommand{\calR}{\mathcal{R}}
\newcommand{\calC}{\mathcal{C}}


\linksection{19}{Chapter 1 Introduction}
\linksection{22}{1.1 Example: Polynomial Curve Fitting}


\begin{example}
    Given $\matr{x} = (x_1,\cdots, x_N)^T$ where $x_n \in [0,1]$ and $\matr{t} = (t_1, \cdots, t_N)^T$. We generate target with 
    \[
        t_n = \sin{(2\pi x_n)} + \epsilon
    \]
    where $\epsilon$ are Gaussian noises. The dataset capture the property that it possess an underlying regularity, which we wish to learn, but that individual observations are corrupted by random noise. We fit the data with a polynomial function of the form 
    \[ 
        y(x, \matr{w}) = w_0 + w_1 x + w_2 x^2 + \cdots + w_M x^M = \sum_{j=0}^M w_j x^j
    \]
    where $M$ is order of the polynomial. Functions which are linear in the unknown parameters are called \textbf{linear models}. Value of coefficients determined by minimizing an \textbf{error function} measuring the misfit between function $y(x,\matr{w})$, for a given $\matr{w}$, and the training set data points 
    \[
        E(\matr{w}) = \frac{1}{2} \sum_{n=1}^N (y(x_n, \matr{w}) - t_n)^2
    \]
    We find $\matr{w}^*$ such that $E(\matr{w^*})$ is minimized. Over-fitting problem can be understood as a general property of maximum likelihood. One etchnique used to contrl over-fitting phenomenon is \textbf{regularization}, which involves adding a penalty term to the error function in order to discourage the coefficients from reaching large values 
    \[
        \widetilde{E}(\matr{w}) = 
        \frac{1}{2} \sum_{n=1}^N (y(x_n, \matr{w}) - t_n)^2 + \frac{\lambda}{2} \norm{\matr{w}}^2
    \]
    where $\norm{\matr{w}} = \matr{w^Tw} = w_0^2 + w_1^2 + \cdots + w_M^2$. Techniques such as this are known as \textbf{shrinkage} methods because they reduce the value of coefficients. The particular case of a quadratic regularizer is called \textbf{ridge regression} or \textbf{weight decay} in the context of neural networks. $\lambda$ controls the effective complexity of the model and hence determines the degree of over-fitting. We can divide data into \textbf{training set}, used to determine coefficients $\matr{w}$, and a separate \textbf{validation set}, used to optimize model complexity (either $M$ or $\lambda$). 
\end{example}


\linksection{30}{1.2 Probability Theory}

\begin{defn*}
    \textbf{Sum and Product Rules of Probability} Let $X \in \{x_i\}$ where $i=1,\cdots, M$ and $Y\in \{y_j\}$, where $j=1,\cdots,L$. Consider a total of $N$ trials in which we sample both $X$ and $Y$, and let number of such trials in which $X=x_i$ and $Y=y_j$ be $n_{ij}$. Let number of trials in which $X$ takes the value $x_i$ be $c_i$ and let number of trials in which $Y$ takes $y_j$ be denoted by $r_j$. So we have joint, marginal, and conditional probability as follows 
    \[
        p(X=x_i, Y=y_j) = \frac{n_{ij}}{N}
        \quad \quad 
        p(X=x_i) = \frac{c_i}{N}
        \quad \quad 
        p(Y=y_j | X=x_i) = \frac{n_{ij}}{c_i}    
    \]
    We have 
    \begin{align*}
        p(X) &= \sum_{Y} p(X,Y) \tag{\textbf{sum rule}}\\
        p(X,Y) &= p(Y|X)p(X) \tag{\textbf{product rule}}\\
    \end{align*}
    Together we have \textbf{Bayes' theorem}
    \[
        p(Y|X)
        = \frac{p(X|Y)p(Y)}{p(X)}
        = \frac{p(X|Y)p(Y)}{\sum_Y p(X|Y)p(Y)}
    \]
    where we have $A$ as proposition and $B$ as fixed evidence. $P(A|B)$ is called the \textbf{posterior}, or the degree of belief after observing $B$. $P(A)$ is called \textbf{prior}, the initial degree of belief on $A$ available before the observation. and $\rfrac{p(B|A)}{p(B)}$ is the \textbf{likelihood} supporting $B$ given $A$. In essence, we can interpret the formula as posterior is proportional to prior times likelihood.
\end{defn*}


\begin{defn*}
    \textbf{Probability Density} Probability with respect to continuous variables. If probability of a real-valued variable $x$ falling in interval $(x,x+\delta x)$ is given by $p(x)\delta x$ for $\delta \to 0$, then $p(x)$ is the \textbf{probability density} over $x$. 
    \[
        p(x\in (a,b)) = \int_a^b p(x)dx
    \]
    satisfying 
    \[
        p(x) \geq 0 
        \quad \quad \quad \quad 
        \int_{-\infty}^{\infty} p(x)dx = 1
    \]
    \textbf{Cumulative distribution function} is the probability that $x$ lies in the interval $(-\infty, z)$
    \[
        P(z) = \int_{-\infty}^z p(x)dx    
    \]
    which satisfies $P'(x) = p(x)$. \\
    For several continuous variabes $x_1,\cdots,x_D$, denoted by $\matr{x}$, we define a \textbf{joint probability density} as $p(\matr{x}) = p(x_1,\cdots, x_D)$ be probability of $\matr{x}$ falling in an infinitesimal volumne $\delta \matr{x}$ containing the point given by $p(\matr{x})\delta(x)$. The multivariate probability density satisfy
    \[
        p(\matr{x}) \geq 0
        \quad \quad 
        \int p(\matr{x}) d\matr{x}=  1
    \]
    The \textbf{sum and product rule} for continous variables are given by 
    \[
        p(x) = \int p(x,y) dy    
        \quad \quad \quad \quad 
        p(x,y) = (y|x)p(x)
    \]
\end{defn*}


\begin{defn*}
    \textbf{Expectations and Covariances} 
    \begin{enumerate}
        \item \textbf{Expectation} The average value of some function $f(x)$ under a probability distribution $p(x)$ is called the expectation of $f(x)$, denoted by $\E{f}$
        \[
            \E{f} = \sum_x p(x)f(x) 
            \quad \quad \quad \quad 
            \E{f} = \int p(x)f(x) dx 
        \]
        \item \textbf{Expectation Over Several Variables} We use subscript to indicate which variable is being averaged over so $\E{f(x,y)}$ denotes the average of the function $f(x,y)$ with respect to the distribution of $x$, but note it is a function of $y$. 
        \item \textbf{Conditioanl Expectation} The conditional expectation with respect to a conditional distribution is given by 
        \[
            \E[x]{f|y} =\sum_x p(x|y) f(x)
            \quad \quad \quad \quad 
            \E[x]{f|y} = \int p(x|y)f(x)dx
        \]
        \item \textbf{Variance} is a measure f how much variability there is in $f(x)$ around its mean value $\E{f(x)}$
        \[  
            \var{f} 
            = \E{(f(x) - \E{f(x)})^2}
            = \E{f(x)^2} - \E{x}^2
        \]
        \item \textbf{Covariance} is a measure to which $x$ and $y$ vary together, covariance is zero if $x$ and $y$ are independent
        \[
            \cov{x,y} 
            = \E[x,y]{(x-\E{x})(y-\E{y})} 
            = \E[x,y]{xy} - \E{x}\E{y}
        \]
        For vectors $\matr{x}, \matr{y}$ 
        \[
            \cov{\matr{x,y}}
            = \E[\matr{x,y}]{(\matr{x}-\E{\matr{x}})(\matr{y^T} - \E{\matr{y^T}})}
            = \E[\matr{x,y}]{\matr{xy^T}}-  \E{\matr{x}}\E{\matr{y^T}}
        \]
        Note $\cov{x} = \cov{x,x}$
    \end{enumerate}
\end{defn*}


\begin{defn*}
    \textbf{Bayesian Probabilities}
    \begin{enumerate}
        \item \textbf{Motivation} For uncertain events which does not repeat numerous times (like arctic ice cap melt by end of centuary ...). Idea is we want to quantify expression of uncertainty and make precise revisions of uncertainty in light of new evidence, as well as subsequently be able to make optimal actions or decisions as a consequence. Then central idea is the use of probability to represent uncertainty. 
        \item \textbf{Bayes' Theorem} We capture assumptions about $\matr{w}$, before observing data in the form of \textbf{prior distribution} $p(\matr{w})$. The effect of observed data $\mathcal{D}=\{t_1,\cdots,t_N\}$ is expressed through $p(\mathcal{D}|\matr{w})$. We can evaluate the uncertainty in $\matr{w}$ after we have observed $\mathcal{D}$ in the form of \textbf{posterior probability} $p(\matr{w}|\mathcal{D})$
        \[
            p(\matr{w}|\mathcal{D}) = 
            \frac{p(\mathcal{D}|\matr{w}) p(\matr{w})}{p(\mathcal{D})} 
        \]
        $p(\mathcal{D}|\matr{w})$ is evaluated for the dataset, and is a function of the parameter vector $\matr{w}$, in which case it is called the \textbf{likelihood function}. It expresses how probable that observed data is for different settings of parameter vector $\matr{w}$. $p(\mathcal{D})$ is a normalizing constaint ensuring that posterior distribution is a valid probability density (integrates to 1)
        \[
            \text{posterior} \propto \text{likelihood}\times \text{prior}    
        \]
        all of which are functions of $\matr{w}$. Bayesian and frequentist interpret likelihood $p(\mathcal{D}|\matr{w})$ differently 
        \item \textbf{Frequentist} $\matr{w}$ is a fixed parameter, whose value is determined by some estimator, whose distribution is obtained by considering distribution of possible datasets $\mathcal{D}$. Maximum likelihood estimator chooses $\matr{w}$ for which the probability of observed dataset is maximized, i.e. maximizes the likelihood function $p(\mathcal{D}|\matr{w})$. The neagative logarithm of likelihood function is the \textbf{error function}.
        \item \textbf{Bayesian} There is only a single dataset $\mathcal{D}$, and the uncertainty in the parameter is expressed through a probability distribution over $\matr{w}$. One advantage of Bayesian viewpoint is the inclusion of prior konwledge. Critics of Bayesian approach states that prior distribution is often selected based on mathematical convenience rather than as a reflection of prior beliefs. The Bayesian framework is limited by the difficulty in carrying out Bayesian procedure, particularly in the need to marginalize over the whole parameter space. The development of Markov chain Monte Carlo opens up to practical use of Bayesian techniques. Variational Bayes and Expectation progapation developed for deterministic approximation
    \end{enumerate}
\end{defn*}


\begin{defn*}
    \textbf{The Gaussian Distribution}
    \begin{enumerate}
        \item \textbf{Single-Valued}
        \[
            \normal{x|\mu, \sigma^2}
            = 
            \frac{1}{(2\pi \sigma^2)^{1/2}}
            \exponents{-\frac{1}{2\sigma^2}(x-\mu)^2}
        \]
        where $\mu$ is the mean, $\sigma^2$ is the variacne, $\beta = \rfrac{1}{\sigma^2}$ is called the precision. Note
        \[
            \normal{x|\mu, \sigma^2} > 0
            \quad \quad 
            \int_{-\infty}^{\infty} = \normal{x|\mu, \sigma^2} dx = 1
        \]
        with 
        \[
            \E{x} = \int_{-\infty}^{\infty} \normal{x|\mu, \sigma^2} x dx = \mu
            \quad \quad 
            \E{x^2} = \mu^2 + \sigma^2 
            \quad \quad 
            \var{x} = \sigma^2
        \]
        \item \textbf{Multivariate}
        \[
            \normal{\matr{x|\boldsymbol{\mu}, \Sigma}}
            = 
            \frac{1}{(2\pi)^{D/2}}
            \frac{1}{|\matr{\Sigma}|^{1/2}}  
            \exponents{-\frac{1}{2} 
                \matr{(x-\bmu)^T \Sigma^{-1} (x-\bmu)}
            }
        \]
        where D-dimensional vector $\boldsymbol{\mu}$ is called the mean and $D\times D$ matrix $\matr{\Sigma}$ is called the covaraince. $|\matr{\Sigma}|$ is the determinant of $\matr{\Sigma}$
        \item \textbf{Likelihood Function} Let $\matr{x} = (x_1, \cdots, x_N)^T \overset{i.i.d.}{\sim} \normal{\mu, \sigma^2}$ 
        \[
            p(\matr{x}|\mu, \sigma^2) = 
            \prod_{n=1}^N \normal{x_n | \mu, \sigma^2}    
        \]
        We maximize log likelihood,
        \[
            \ln{p(\matr{x}|\mu, \sigma^2)} = 
            - \frac{1}{2\sigma^2} \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}{2} \ln{\sigma^2} - \frac{N}{2}\ln{2\pi}
        \]
        We derive the maximum likelihood estimators 
        \[
            \mu_{mle} = \frac{1}{N} \sum_{n=1}^N x_n 
            \quad \quad \quad \quad 
            \sigma^2_{mle} = \frac{1}{N}\sum_{n=1}^N (x_n - \mu_{mle})^2
        \]
        Note maximum likelihood approach systematically underestimates variance, because of introduction of biases. Note maximum likelihood variance underestimates the true variance 
        \[
            \E{\mu_{mle}} = \mu 
            \quad \quad \quad \quad 
            \E{\sigma^2_{mle}} = \frac{N-1}{N} \sigma^2
        \]
        An unbiased estimator is given by 
        \[
            \widetilde{\sigma}^2
            = \frac{1}{N-1}
            \sum_{n=1}^N (x_n - \mu_{mle})^2
        \]
        For large sample, this is not a problem. In models where there are many parameters, this problem is much more severe. Issue of bias is root of overfitting problem 
        \item \textbf{Polynomial Fitting Revisited} We want to express uncertainty over value of target variable using a probability distribution. We assume, given $x$, the corresponding value of $t$ has Gaussian distirbution with mean equal to $y(x,\matr{w})$, so 
        \[
            p(\mathcal{D}|\matr{w}) = 
            p(t|x,\matr{w},\beta) = 
            \normal{t \,\, |\, y(x,\matr{w}), \beta^{-1}}
        \]
        \begin{center}
            \includegraphics[width=6cm]{gaussian_conditional_poly_fitting.jpg}
        \end{center}
        Given training data $\{\matr{x}, \matr{t}\}$, we want to determine values for $\matr{w}$ and $\beta$ by maximum likelihood. The likelihood function is given by 
        \[
            p(\matr{t}|\matr{x}, \matr{w}, \beta) = 
            \prod_{n=1}^N \normal{t_n | y(x_n, \matr{w}), \beta^{-1}}    
        \]
        Maximizing likelihood is equivalent to minimizing sum-of-squares error function
        \[
            \ln{p(\matr{t}|\matr{x}, \matr{w}, \beta)}  
            = -\frac{\beta}{2} \sum_{n=1}^N (y(x_n, \matr{w}) - t_n)^2 + \frac{N}{2}\ln{\beta} -  \frac{N}{2} \ln{2\pi}  
        \]
        We can find maximum likelihood estimator $\matr{w}_{mle}$ and the corresponding precision given by 
        \[
            \frac{1}{\beta_{mle}} = \frac{1}{N}\sum_{n=1}^N (y(x_n, \matr{w}_{mle} ) - t_n)^2
        \]
        Having determined parameter $\matr{w}$ and $\beta$, we can make predictions for new values of $x$ by expressing in terms of the predictive distribution. 
        \[
            p(t|x,w_{mle}, \beta_{mle})
            = 
            \normal{t|y(x,\matr{w}_{mle}), \beta^{-1}_{mle}}
        \]
        We can introduce a prior distribution over polynomial coefficients $\matr{w}$
        \[
            p(\matr{w}|\alpha) = 
            \normal{\matr{w}| \matr{0}, \alpha^{-1}\matr{I}} = 
            \left( \frac{\alpha}{2\pi} \right)^{(M+1)/2}
            \exponents{-\frac{\alpha}{2} \matr{w^Tw}}
        \]
        where $\alpha$ is precision of prior distribution and $M+1$ is the total number of elements in $\matr{w}$ for $M$th order polynomial. Also note $\matr{\Sigma}^{-1} = (\alpha^{-1}\matr{I})^{-1} = \alpha\matr{I}$. Using Bayes' theorem, we have 
        \[
            p(\matr{w}|\matr{x},\matr{t}, \alpha, \beta)
            \propto 
            p(\matr{t}|\matr{x}, \matr{w}, \beta) p(\matr{w}|\alpha)
        \]
        We can determine $\matr{w}$ by finding most probable value of $\matr{w}$ given the data, by maximizing the posterior distribution. Finding \textbf{maximum posterior} (MAP) is equivalent to minimizing negative logarithm of 
        \[
            \ln{p(\matr{w}|\matr{x},\matr{t}, \alpha, \beta)} \propto \frac{\beta}{2} \sum_{n=1}^N (y(x_n, \matr{w}) - t_n)^2 + \frac{\alpha}{2} \matr{w^Tw}    
        \]
        which is equivalent to minimize regularized sum-of-squares erorr fucntion with $\lambda = \alpha/\beta$
    \end{enumerate}
\end{defn*}

\begin{defn*}
    \textbf{Model Selection} 
    \begin{enumerate}
        \item \textbf{Motivation} Order of polynomial or regularization coefficients govern complexity of model. We want to determine the value of hyperparameters such that we achieve best predictive performance of new data. Idea is to use an independent data from \textbf{training set}, called \textbf{validation set}, and select one having the best performance. However if model is iterated many times, then some overfitting to validation data can occur., so may want to keep a third \textbf{test set} on which performance of selected model is evaluated. 
        \item \textbf{Cross-Validation} Allows proportion of $(S-1)/S$ of data to be used for training. The drwaback is 
        \begin{enumerate}
            \item Number of training runs increased by a factor of $S$, and can prove problematic for model in which the training itself is computationally expensive.
            \item If there are multiple complexity parameter in the model, exploring combinations of settings of such parameter could, in worst case, require a number of training runs that is exponential in the number of parameters. 
        \end{enumerate}
        So want to find a measure of performance which depends only on the trainin gdata and which does not suffer from bias due to overfitting. AIC/BIC are such models
    \end{enumerate}
\end{defn*}


\linksection{51}{1.4 The Curse of Dimensionality}


\begin{defn*}
    \textbf{Curse of Dimensionality} 
    \begin{enumerate}
        \item \textbf{Grid Search} Divide region of space into regular cells, number of cell grow exponential with dimensionality of space. Need exponentially large quantity of training data to ensure cell not empty
        \item \textbf{Polynomial Fitting} As dimension $D$ increases, number of independent coefficients grows proportionally to $D^n$ for n-degree polynomial
    \end{enumerate}
\end{defn*}


\linksection{56}{1.5 Decision Theory}

\begin{defn*}
    \textbf{Motivation} 
    \begin{enumerate}
        \item \textbf{Statistical inference} is the process of deducing properties of an underlying probability distribution by analysis of data. More concretely, it deals with determination of $p(\matr{x},\matr{t})$, the joint distribution providing a complete summary of uncertainty associated with these variables. 
        \item \textbf{Decision theory} deals with making optimal decisions given the appropriate probabilities, which is usually simple once we solved the inference problem. 
    \end{enumerate}
\end{defn*}

\begin{defn*}
    \textbf{Minimizing Misclassification Rate} We want to come up with a rule that divide the input space into regiosn $\calR_k$ called decision regions, one for each class, such that all points in $\calR_k$ are assigned to class $\calC_k$. The boundary between them are called decision boundaries. The probability of missclassification which we want to minimize is given by
    \[
        p(mistake) = p(\matr{x}\in \calR_1, \calC_2) + p(\matr{x}\in \calR_2, \calC_1) = \int_{\calR_1} p(\matr{x}, \calC_2) d\matr{x} + \int_{\calR_2} p(\matr{x}, \calC_1) d\matr{x}
    \]
    Generally for $K$ classes, it's easier maximize the probability of being correct, given by 
    \[
        p(correct) = \sum_{k=1}^K p(\matr{x}\in \calR_k, \calC_k) = \sum_{k=1}^K \int_{\calR_k} p(\matr{x}, \calC_k) d\matr{x}  \propto \sum_{k=1}^K \int_{\calR_k} p(\calC_k | \matr{x}) d\matr{x}
    \]
    Since $p(\matr{x})$ is common to all terms, we see that each $\matr{x}$ should be assfigned to class having largest posterior probability $p(C_k | \matr{x})$. 
    \begin{center}
        \includegraphics[width=10cm]{minimize_misclassification.jpg}
    \end{center}
\end{defn*}

\begin{defn*}
    \textbf{Minimizing the Expected Loss} 
    \begin{enumerate}
        \item \textbf{Loss Function} is a single, overall measure of loss incurred in taking any of the available decisions or actions. 
        \item \textbf{Loss Matrix} For a new value of $\matr{x}$, the true class is $\calC_k$ and we assign $\matr{x}$ to class $\calC_j$, incurring a loss that we denote by $L_{kj}$, an element in the loss matrix. 
        \item \textbf{Expected Loss} is the sum of the values of all possible losses, each multiplied by the probability of that loss occurring. Note loss function depends on the true class, which is unknown, so we instead try to minimize the expected loss
        \[
            \E{L} = \sum_k \sum_j \int_{x\in \calR_j} L_{kj} p(\matr{x}, \calC_k) d\matr{x}
        \]
        Note each $\matr{x}$ is independently assigned to one of decision region $\calR_j$. The goal is to pick a region $\calR_j$ in order to minimize the expected loss. The decision rule that minimizes the expected loss is one that assigns each new $\matr{x}$ to the class $j$ for which 
        \[
            \sum_k L_{kj} p(\matr{x}, \calC_j) \propto \sum_k L_{kj} p(\calC_j | \matr{x})    
        \]
        is minimum. 
    \end{enumerate}
\end{defn*}

\begin{defn*}
    \textbf{The rejection option} Classification erorr arise form regions of input space wherew largest of posterior probability $p(\calC_k|\matr{x})$ is significantly less than unity, or equivalently where the joint distribution $p(\matr{x},\calC_k)$ have comparable values. These are the region we are uncertain about class membership. 
\end{defn*}


\begin{defn*}
    \textbf{Inference and Decision} \\
    3 approaches to solve the decision problems 
    \begin{enumerate}
        \item \textbf{Generative Models} is a model for generating all values for a phenomenon, both those that can be observed in the world and "target" variables that can only be computed from those observed. Simply, generative models generate both inputs and outputs, typically given some hidden parameters. More precisely, it is an approach that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models. We either 
        \begin{enumerate}
            \item determine \textbf{class-conditional densities} $p(\matr{x}|\calC_k)$, for each class $\calC_k$ individually, infer prior $p(\calC_k)$, then compute \textbf{posterior probability} with Bayes rule.
            \item model joint distribution $p(\matr{x}, \calC_k)$ directly and then normalize to obtain the \textbf{posterior probabilities}.
        \end{enumerate}
        then apply decision theory to determine class membership 
        \item \textbf{Discriminative Models} is a model only for the target variable(s), generating them by analyzing the observed variables. In other word, approach that model the posterior probabilities $p(\calC_k | \matr{x})$ directly. Simply, discriminative models infer outputs based on inputs.
        \item \textbf{Discriminative Function} maps input $\matr{x}$ onto class label. Probabilities play no role
    \end{enumerate}
    \begin{center}
        \includegraphics[width=10cm]{classconditional_vs_posterior.jpg}
    \end{center}
\end{defn*}

\begin{defn*}
    \textbf{Combining Models} We can combiner outputs of different model systematically using rule of probability. If distribution of input features are independent, then by conditional independence
    \[
        p(\matr{x_a, x_b} | \calC_k) = p(\matr{x_a} | \calC_k) p(\matr{x_b} | \calC_k)
    \]
    Posterior of combined model 
    \[
        p(\calC_k | \matr{x_a, x_b}) \propto p(\matr{x_a, x_b}|\calC_k) p(\calC_k) \propto 
        \frac{p(\calC_k | \matr{x_a}) p(\calC_k | \matr{x_b})}{p(\calC_k)}
    \]
    where $p(\calC_k)$ the class prior can be estimated from the fractions of data points in each class, then normalize the resulting posterior probabilities so that they sum to one. This conditional independence assumption is an example of \textbf{Naive Bayes Model}
\end{defn*}

\begin{defn*}
    \textbf{Loss Function for Regression} Given predictive distribution $y(t|\matr{x})$, the decision theory for regression involves choosing a specific estimate $y(\matr{x})$, the regression function not a point prediction, of the value of target $t$ for each input $\matr{x}$ such that, given loss $L(t,y(\matr{x}))$, the expected loss is minimized
    \[
        \E{L} = \iint L(t,y(\matr{x})) p(\matr{x}, t) d\matr{x} dt
    \]
    If $L(t,y(\matr{x})) = (y(\matr{x}) - t)^2$, then 
    \[
        \E{L} = \iint (y(\matr{x}) - t)^2 p(\matr{x}, t) d\matr{x} dt
        \quad \rightarrow \quad 
        y(\matr{x}) = \E[t]{t|\matr{x}} \quad \text{(by taking derivative)}
    \]
    \begin{center}
        \includegraphics[width=6cm]{regression_func_for_squared_loss.jpg}
    \end{center}
    The regression funtion $y(\matr{x})$ which maximizes the expected squared loss $L$ is given by mean of conditional distribution of $y(t|\matr{x})$, for a completely flexible $y$. \textbf{Minkowski Loss} is a generalization of squared loss, whose expectation is given by 
    \[
        \E{L_q} = \iint |y(\matr{x}) - t|^q p(\matr{x}, t) d\matr{x} dt
    \]
    The minimum for $\E{L_q}$ is given by conditional mean for $q=2$, conditional median for $q=1$, and conditional mode for $q\to 0$
    \begin{center}
        \includegraphics[width=8cm]{minkowski.jpg}
    \end{center}
\end{defn*}


\end{document}
