{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing RNNs and LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/Users/ssydasheng/anaconda3/envs/cp3/lib/python3.6/site-packages')\n",
    "import autograd\n",
    "import autograd.misc.optimizers as optim\n",
    "# from autograd import optimizers as optim\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "You may find the following resources helpful for understanding how RNNs and LSTMs work:\n",
    "\n",
    "* [The Unreasonable Effectiveness of RNNs (Andrej Karpathy)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* [Recurrent Neural Networks Tutorial (Wild ML)](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "* [Understanding LSTM Networks (Chris Olah)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-Level Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "The vocabulary contains [\"'\", 'N', 'f', 'U', 'X', 'q', 'a', '3', 'L', 'H', 'P', 'g', '$', 'z', 'p', 'W', 'K', 'b', 'A', '\\n', 'V', ',', 'Q', 'c', 'E', '-', 'S', 'r', ' ', 'd', 'I', 'J', 'x', ':', 'Y', 'O', 'i', '?', 'h', 'B', ';', 'Z', 't', 'y', 'w', '.', '!', 'M', '&', 'm', 'G', 'n', 'T', 'u', 'o', 'F', 'C', 'v', 'j', 'k', 's', 'l', 'e', 'D', 'R']\n",
      "------------------------------\n",
      "TOTAL NUM CHARACTERS = 1115394\n",
      "NUM UNIQUE CHARACTERS = 65\n",
      "char_to_index {\"'\": 0, 'N': 1, 'f': 2, 'U': 3, 'X': 4, 'q': 5, 'a': 6, '3': 7, 'L': 8, 'H': 9, 'P': 10, 'g': 11, '$': 12, 'z': 13, 'p': 14, 'W': 15, 'K': 16, 'b': 17, 'A': 18, '\\n': 19, 'V': 20, ',': 21, 'Q': 22, 'c': 23, 'E': 24, '-': 25, 'S': 26, 'r': 27, ' ': 28, 'd': 29, 'I': 30, 'J': 31, 'x': 32, ':': 33, 'Y': 34, 'O': 35, 'i': 36, '?': 37, 'h': 38, 'B': 39, ';': 40, 'Z': 41, 't': 42, 'y': 43, 'w': 44, '.': 45, '!': 46, 'M': 47, '&': 48, 'm': 49, 'G': 50, 'n': 51, 'T': 52, 'u': 53, 'o': 54, 'F': 55, 'C': 56, 'v': 57, 'j': 58, 'k': 59, 's': 60, 'l': 61, 'e': 62, 'D': 63, 'R': 64}\n"
     ]
    }
   ],
   "source": [
    "# Load the Shakespeare text\n",
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"------------------------------\")\n",
    "# Print a sample of the text\n",
    "print(text[:100])\n",
    "data_length = len(text)\n",
    "vocab = list(set(text))\n",
    "vocab_size = len(vocab)   # + 1      # The extra + 1 is for the end-of-string token\n",
    "\n",
    "char_to_index = { char:index for (index,char) in enumerate(vocab) }\n",
    "index_to_char = { index:char for (index,char) in enumerate(vocab) }\n",
    "\n",
    "print(\"The vocabulary contains {}\".format(vocab))\n",
    "print(\"------------------------------\")\n",
    "print(\"TOTAL NUM CHARACTERS = {}\".format(data_length))\n",
    "print(\"NUM UNIQUE CHARACTERS = {}\".format(vocab_size))\n",
    "print('char_to_index {}'.format(char_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "![Recurrent Neural Network Diagram](data/rnn.jpg)\n",
    "(Image from the [Wild ML RNN Tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/))\n",
    "\n",
    "The update of an RNN is expressed by the following formulas:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(U x_t + W h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = \\text{softmax}(V h_t + b_y)\n",
    "$$\n",
    "\n",
    "Here, each $x_t$ is a _character_---in this example, there are 65 unique characters. Since in each step the model takes as input a character and outputs a prediction for the next character in the sequence, both $x_t$ and $o_t$ are 65-dimensional vectors, i.e., $x_t, o_t \\in \\mathbb{R}^{65}$. We can choose any dimension for the hidden state $h_t$; in this case, we will use $h_t \\in \\mathbb{R}^{100}$. With this setup, the dimensions of $U$, $W$, and $V$ are $100 \\times 65$, $100 \\times 100$, and $65 \\times 100$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a vector $\\mathbf{x}$, we have:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\mathbf{x})_i = \\frac{e^{\\mathbf{x}_i}}{\\sum_j e^{\\mathbf{x}_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numerically stable version\n",
    "def softmax(x):\n",
    "    exponential = np.exp(x - np.max(x))\n",
    "    return exponential / np.sum(exponential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(input_size, hidden_size, output_size):\n",
    "    params = {\n",
    "        'U': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'V': np.random.randn(output_size, hidden_size) * 0.01,\n",
    "        'b_h': np.zeros(hidden_size),\n",
    "        'b_o': np.zeros(output_size)\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_hidden(hidden_size):\n",
    "    return np.zeros(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(params, x, h_prev):\n",
    "    # TODOS\n",
    "    h = np.tanh(np.dot(params['U'], x) + np.dot(params['W'], h_prev) + params['b_h'])\n",
    "    y = softmax(np.dot(params['V'], h_prev) + params['b_o'])\n",
    "    return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def criterion(output, target):\n",
    "    \"\"\"Negative log-likelihood loss. Useful for training a classification problem with n classes.\n",
    "    \"\"\"\n",
    "    output = np.log(output)\n",
    "    return -output[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, input_seq, target_seq, opts):\n",
    "    \"\"\"\n",
    "    Compute the loss of RNN based on data.\n",
    "    \n",
    "    :param params: dict of str: tensor, including keys U, W, v, b_h, b_o.\n",
    "    :param input_seq: list of str. Input string.\n",
    "    :param target_seq: list of str. Target string.\n",
    "    :param opts: dict of str: int. Including keys input_size, hidden_size, output_size.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    hidden = initialize_hidden(opts['hidden_size'])\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(input_seq)):\n",
    "        output, hidden = model(params, input_seq[i], hidden)\n",
    "        loss += criterion(output, target_seq[i])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_grad = grad(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def sgd(grad, init_params, callback=None, num_iters=200, step_size=0.1, mass=0.9):\n",
    "    \"\"\"Stochastic gradient descent with momentum.\n",
    "    grad() must have signature grad(x, i), where i is the iteration number.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_one_hot(j, length):\n",
    "    vec = np.zeros(length)\n",
    "    vec[j] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(params, initial, length, opts):\n",
    "    \"\"\"\n",
    "    Sampling a string with a Recurrent neural network.\n",
    "    \n",
    "    :param params: dict of str: tensor, including keys U, W, v, b_h, b_o\n",
    "    :param initial: str. Beginning character.\n",
    "    :param length: length of the generated string.\n",
    "    :param opts: dict of str: int. Including keys input_size, hidden_size, output_size.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    hidden = initialize_hidden(opts['hidden_size'])\n",
    "    current_char = initial\n",
    "    final_string = initial\n",
    "    \n",
    "    for i in range(length):\n",
    "        x = create_one_hot(char_to_index[current_char], opts['input_size'])\n",
    "        output, hidden = model(params, x, hidden)\n",
    "        \n",
    "        p = output\n",
    "        current_index = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        current_char = index_to_char[current_index]\n",
    "        final_string += current_char\n",
    "    \n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Use non-overlapping 25-character chunks for training\n",
    "    sequence_length = 50\n",
    "    num_epochs = 1\n",
    "    print_every = 100\n",
    "    evaluate_every = 100\n",
    "    lr = 1e-2\n",
    "\n",
    "    opts = {\n",
    "        'input_size': vocab_size,\n",
    "        'hidden_size': 100,\n",
    "        'output_size': vocab_size,\n",
    "    }\n",
    "\n",
    "    params = initialize_params(opts['input_size'], opts['hidden_size'], opts['output_size'])\n",
    "\n",
    "    for ep in range(num_epochs):\n",
    "        # i = 0\n",
    "        # while i * sequence_length + 1 < 10000:\n",
    "        for i in range(data_length // sequence_length):\n",
    "            start = i * sequence_length\n",
    "            end = start + sequence_length + 1\n",
    "            chunk = text[start:end]\n",
    "\n",
    "            input_chars = chunk[:-1]\n",
    "            target_chars = chunk[1:]\n",
    "\n",
    "            input_seq = [char_to_index[c] for c in input_chars]\n",
    "            target_seq = [char_to_index[c] for c in target_chars]\n",
    "\n",
    "            input_seq_one_hot = [create_one_hot(j, vocab_size) for j in input_seq]\n",
    "\n",
    "            example_loss = loss(params, input_seq_one_hot, target_seq, opts)\n",
    "\n",
    "            # Compute gradient\n",
    "            grad_params = loss_grad(params, input_seq_one_hot, target_seq, opts)\n",
    "            for param in params:\n",
    "                # clipping -> stable training\n",
    "                gradient = np.clip(grad_params[param], -5, 5)\n",
    "                params[param] -= lr * gradient\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print(\"LOSS = {}\".format(example_loss))\n",
    "                # print(grad_params)\n",
    "\n",
    "            if i % evaluate_every == 0:\n",
    "                sampled_string = sample(params, initial='a', length=100, opts=opts)\n",
    "                print(sampled_string)\n",
    "\n",
    "            # i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS = 208.73247352616244\n",
      "aZ;Ul'dkbebk3i,hhNCu$3L:.IKdNlHa'On3S&\n",
      "qOhTmL!O;mQ,',i?IezEMOTum-rOfwPXrRfHfkO,IGSBhxgclrd3GNM3Ttmu\n",
      "Y\n",
      "LOSS = 150.39839726293573\n",
      "aZrassattoayr\n",
      " huF ovisr;imThhe   F  ae er,ckheetmeT:Ore  nnew nl-lnFn\n",
      "Jl  W aQon'v b feaestuRrte trh\n",
      "LOSS = 152.3240449138244\n",
      "amerhddloe  tMy;,lsatrt sh fD?onIoa  igasmiw yn risthM litSt stthimI\n",
      "s e; tphWp  oept linW h' ta raht\n",
      "LOSS = 149.93437837781752\n",
      "aOa: dhva Ioeep\n",
      "b, \n",
      "oe rads syie\n",
      " lfanne\n",
      "smfo tomarys\n",
      " .\n",
      "l  au  rlnUeodTf  hlr nhwse\n",
      " bfnt Iose honn \n",
      "LOSS = 158.13005839435\n",
      "atuen\n",
      "foMr IWt BofcswSmeC.st ri yolw \n",
      "e'k\n",
      "I\n",
      "s3Gyruin sac  ahs telysr roum iun \n",
      "ayvdtt,r.o\n",
      ":y Jims;,ox\n",
      "LOSS = 146.60982157613753\n",
      "a\n",
      "sn  hnu teefshrluv'i\n",
      "e, ,ootma tre deisevn eaoat:;hlTuv dirhicMrd' rwsAe iatdwoc nrr no  oue  ovtio\n",
      "LOSS = 141.01865867188184\n",
      "atD\n",
      "ri nor  ufzion ro.e, Plolros\n",
      "\n",
      "hRa vh -owonyt!h  ue lsthsagt -Rafhsier,er-e   uodahdso'le\n",
      "loar  hg\n",
      "LOSS = 135.32441953803053\n",
      "aa  oet\n",
      "aI Qhtm,?edlRi,,CAa\n",
      "Shdtm,e erdlw\n",
      "sh yovta'd eeerss\n",
      " eu bocsl  oeu s  eab mm;,Q'pdobdlotyrzp:\n",
      "LOSS = 131.43402315142174\n",
      "a, uaa  hur,ow ge eoc noubwopo\n",
      " paeestbemAaClui ,Ar, too  hea$G,Harseat;th,chesf ofbeo$a  orteflg3k\n",
      "d\n",
      "LOSS = 145.93863267401596\n",
      "a eusva chamh  nergstol  ohtte oyem  tuew,tshsnytoo  hynmt ohrthIrhhrdewhel,\n",
      "-o snuttebrcodht,oftzll \n",
      "LOSS = 148.26485722235245\n",
      "aM\n",
      "botdlpnOl .a\n",
      "oedtbg io  eanesda\n",
      " atmise\n",
      "cryysiih iaae hadhen mewhsciasr eo \n",
      "hj wietder  feu .fS3\n",
      "o\n",
      "LOSS = 139.1238019655157\n",
      "a   teae drTi eodrc morht hu  avdipl  hn d ciald mor lodocdo\n",
      "rok wanetgeT eo hp .o ioe ehatl\n",
      ",et ri  \n",
      "LOSS = 155.2164541525899\n",
      "aGc sepoe eeiee tirkteIlNbS foi lhr\n",
      "\n",
      "IuHbw:WNfl uh  oacllthI Cerko  oo oavd Wetpeee iub nu goodn ,trm\n",
      "LOSS = 144.84901491091887\n",
      "atgyuuesdniueocnit\n",
      "ahp yiv nstfen tier!'\n",
      "ee brg  ounneeyh  ha;nbttei:eaeurhrnr\n",
      "rMuetr nhsneJ iae ysss\n",
      "LOSS = 144.3034545248658\n",
      "aemc\n",
      "shal   hu bolsIYI Aha.,\n",
      "Ue tfasls  ri aitrydl \n",
      "ne\n",
      "yhntvemb\n",
      "TSavd oin.b\n",
      "CTRIIOS.\n",
      "YSq.OOICTAS:\n",
      "Tot\n",
      "LOSS = 122.53264739971924\n",
      "asiI: Wh theradwT,TAe \n",
      "oo eatia\n",
      ",hU \n",
      "regga\n",
      ",nOireIbAy,\n",
      "LQUt:US:TYih;  edmtI CntaMr\n",
      "\n",
      "CoFrr wu  ooerTyh\n",
      "LOSS = 120.30337826451927\n",
      "aHha\n",
      "rT  oultce :hdlFd t eirt ,ftte seaeud iosr tlruets!\n",
      "AhMnL\n",
      "Sh\n",
      "T ei ghetprn yis nhscoud  oarmele o\n",
      "LOSS = 97.39682595093178\n",
      "a  iyaud \n",
      "ots ohl,r\n",
      " hvi anv\n",
      "fEhI.ITUU::tSenmi'hmdd\n",
      "esndsr\n",
      "TE ,ElihSm\n",
      "\n",
      "oFwartdet\n",
      "Bp nteaswet  piah nh\n",
      "LOSS = 139.39871547715796\n",
      "ah  ea isuytse'uss aote depsy lsrtnenl;cBt\n",
      "shsuml\n",
      "oosl\n",
      "pIns\n",
      "nMfCi\n",
      "ehmn'!s,ti snomsThyt eiano\n",
      "tnim co,\n",
      "LOSS = 140.41852461524545\n",
      "ahd werlp ur oiec reruylu nodr,cInt  uoriwfan:nB !;chure  oosc mh  himhnsrt surfsoeen:qT iooc bp ooa\n",
      "\n",
      "LOSS = 129.0388106639243\n",
      "a aoen, af\n",
      "iIUI:SI\n",
      "fi,hi  ohl  oo ea ehald hom woverH,\n",
      "If is mli e lheuris\n",
      "dmrmy  rhen b'gunld,eY\n",
      "say\n",
      "LOSS = 155.44982738969853\n",
      "aL hisl \n",
      "e yrue ch.sY \n",
      "nof Yot,tWyep\n",
      " sywAlb  neaae ,iueu\n",
      " hr thi ,hoi\n",
      " titdes \n",
      "aoe pforr,'ft ssffmny\n",
      "LOSS = 157.85449129347074\n",
      "a\n",
      "d meuiiet eh ephonle srrpara nhbrhishU shcrrt rioiedtt 'nlcly,tInmasA :.t enrZi soeleath  sorpoae w\n",
      "LOSS = 133.47226051190663\n",
      "acs tart yeuu tay  oe he\n",
      "aidethTio  hetl hvve fewttww rhreh teletgssrCav nhwvr\n",
      "tseiveb ageetcrn  oetv\n",
      "LOSS = 137.6998625626138\n",
      "ateee: boa dnveibotse\n",
      "rSmr aedrs\n",
      "soren\n",
      "\n",
      "hhOenIl\n",
      "$Sg\n",
      "colae \n",
      "'e snw tn dnmic thefp\n",
      "soar suoeJ Uevtnr\n",
      ",T\n",
      "LOSS = 146.30236755533164\n",
      "a,,ill tetmehtm\n",
      "ih tears wo!eT ?t aotewyuup  icnns,wio .or prfectnve\n",
      "!h seowt \n",
      "nh netteets\n",
      "bXsttyi':h\n",
      "LOSS = 109.84286479514604\n",
      "apo. \n",
      "ieour\n",
      "nonsBr, Wt roke :nalosc\n",
      ",hhndt,ibn wrir?sI I ma  oisrwa.;\n",
      "SimsnI :nYte toudtmtolK deukdyw\n",
      "LOSS = 145.2939084385732\n",
      "ai teo' ;iln  trekdsRe--eMgrgnwsl sioim \n",
      "hor  ifor  hwt\n",
      "ep psoa dymu penbeti!t\n",
      "Cec natj :iFet\n",
      "ardnot\n",
      "\n",
      "LOSS = 141.5419346757265\n",
      "aHkTre  oor\n",
      " eeahl \n",
      "frtenwmyU :or cr srnsyAu goit ne duaett\n",
      " huuenwws  orget  oasut to ueRytw\n",
      "tha imh\n",
      "LOSS = 121.85312429272456\n",
      "atbeimh,na\n",
      "  ii  ii seecs .iIhLtIi.:eS \n",
      "i hur eitrissnt\n",
      " nsto\n",
      " n eoi   e nreek\n",
      "herwpll\n",
      ".lk-ehIs\n",
      "eA \n",
      "i\n",
      "LOSS = 129.99324863062935\n",
      "aite\n",
      " ao  dfmn steestn\n",
      "gTiir.t\n",
      "Aud Ohrae: Aa\n",
      "ssmeo ngrfswt,\n",
      "weio chyvgrmna.,\n",
      "fnrrgwcea\n",
      " hu  ooeetgfbh\n",
      "LOSS = 150.46656272102007\n",
      "a,yTmasb remda tuz io dyem.in  yforddor  eoabteaahsw gtebtsress\n",
      " h trhuneo sehssiia  homstaee lol  ei\n",
      "LOSS = 158.8014312590964\n",
      "ar  toortsoTdSnrue oir  nusb eot eo .wOvlcwi-g\n",
      "u reoc\n",
      " otnetpoEdet:iS?iEgIPIel\n",
      "wmlltmtne aay, lriwlol\n",
      "LOSS = 149.23891802610473\n",
      "a euyh\n",
      " itn blt homet, Thgsleg ahswmdto\n",
      "siehe i?;\n",
      "ir didtba\n",
      " regchrri eoae,\n",
      "lfvCstfer.,CPann, oe  hol\n",
      "LOSS = 148.2613989468345\n",
      "ao .oChAH  feued Tnhesifo ,opl,\n",
      "tueo ehetebty \n",
      "ia ;eandt\n",
      "sogs;Io\n",
      "toirt  ou?e uh, yov   iytaod m erreg\n",
      "LOSS = 130.15470936784354\n",
      "aUs:\n",
      "Tyaho shrr, an saae  oo egaooerh\n",
      "red aot wrtl,nuo dhor \n",
      "oo  oyo- \n",
      "ou w rioa,eM la,ysd, he eouio,\n",
      "LOSS = 125.71310444976496\n",
      "asnas  on   aands\n",
      " aiefLrEhyl\n",
      " aothn, Aide \n",
      "oa uo  ngsefnntnn  y uoud Ilfel: Aa yo  aavt ie :oC aetta\n",
      "LOSS = 145.29296288119227\n",
      "aneet\n",
      "\n",
      "ehal yesres aad rnk tUd:a,eAtSAe?A\n",
      "TGfba noe;bTfor pi so pde\n",
      "ylu  yau  \n",
      "li leregt\n",
      "\n",
      "siileag,'Hg\n",
      "LOSS = 127.38012581588056\n",
      "ai ,ormdnt\n",
      "\n",
      "f ovre eultland\n",
      "Thelse moletl. Bo  oae to iscsnS touhfureiwnstcd eodrttou  ho  ithov \n",
      "oa \n",
      "LOSS = 131.96718133323202\n",
      "a teat wkidenrtn\n",
      " ir,it  ie sowc lielurf sucl  eoQs: Ee ,egkdc\n",
      "nhssn\n",
      "\n",
      "Iytyms thmm ao sefsol;.b\n",
      "LiDo l\n",
      "LOSS = 147.47377960048144\n",
      "at eu  aod  rutw\n",
      "rhw th s madd\n",
      "ake kerffdlcmt,\n",
      "Bne  oaeu\n",
      " oes.c\n",
      "reaC \n",
      "issfah rAgeaa hha ,ew!\n",
      "Ywnrs ra\n",
      "LOSS = 134.99842682145288\n",
      "ate geadeyoutbeshr Mhrehrrrmeohg lihero gorK\n",
      "hos?b\n",
      " ns il:,TF:rytuinsyatriEpIhg filg mnd'yId: Whot zf\n",
      "LOSS = 151.311217533684\n",
      "a;kA d Ioa  or lorusbisaeecs uo euttpdihyn\n",
      "ah ioe \n",
      "uFandffErt ing ,hat .uC,ISECNTASE\n",
      "OUyEEIMIRIM:MAWG\n",
      "LOSS = 203.81675662079593\n",
      "aG. roui tnpnRA Wsfatnvt gMfersre\n",
      "ahdn\n",
      "ercbr ,nrmrn:eW,n\n",
      "cdn t untntWblodYte\n",
      "s totnrnjnwt poi ynlg ln\n",
      "LOSS = 175.78737028591632\n",
      "anh mewa duiwdssNoT,woIag:mh-,w:t:whtn\n",
      "Wfeidoa.n\n",
      "\n",
      " afdwe.o\n",
      "h\n",
      "elh e d dyaw,zay;whwawhwt,,waye haawhsd \n",
      "LOSS = 160.17874970967856\n",
      "aMghnal ilfkrl!yaldmwsk :asq,gons ,elcewsd thopi!mte'  im nhtntge,hy riellitdnsdst\n",
      "eUCd li neve aigew\n",
      "LOSS = 236.58407256463477\n",
      "aKd \n",
      "oB a              h       U             o   t                     w     H           r           \n",
      "LOSS = 165.91971882339075\n",
      "a:de,tNao mo tLPsCott\n",
      "UOoyof lOhqssott,\n",
      "oslt eK\n",
      "swretko\n",
      " sstttSOsTol  UOssir l Psos   URhsuet\n",
      "nOsolaI\n",
      "LOSS = 184.37998869829846\n",
      "ardtoruosiousoououiluorsluoyoeseyoruoluircoouuriilrtlroodieldslirtouporloigooTraauruooroooloodoooumll\n",
      "LOSS = 216.89898568963523\n",
      "ay donme.oeimeeeemeemee,neeeeeeeeHeeeeemLeeeieeiie.eeienJmeieeeedneeleeeedem.eideeleeeemeeeeeneneeeee\n",
      "LOSS = 199.0476492512495\n",
      "ak \n",
      "oodiib:leeecct  o  o  oide\n",
      "uoa'u?!jI  ir hheyitu\n",
      "V?3? P     dtdlBdA\n",
      "?efbx! r   aitoee ,o ,oq?w s \n",
      "LOSS = 183.22744101237913\n",
      "atd llullwwlnlslmllsfmhsllsTlrdlwll\n",
      "ilsltfllsnlltlllllllllllwl\n",
      "ll,bllllbmlglllullllrslslgslloitllllls\n",
      "LOSS = 188.98515702111615\n",
      "a do  hoet\n",
      "  a,  mae  emytlt nntnwtr to  ee   tnC walR  hr    c  rt tt yae woe mt  Aeeecn lhtt eB tn \n",
      "LOSS = 195.56865522308976\n",
      "am \n",
      "i daede\n",
      "dectuetdnii,lscada etre tar iea  a reae\n",
      "\n",
      "eaentsn ia \n",
      "na gaa ltre aeiiiia \n",
      "aalis arira.,  \n",
      "LOSS = 216.38881316066994\n",
      "arr IoI lII III clerIkIIwIII  IIc'h \n",
      "I llIc  IIl\n",
      "Il cIotE  IIl   Iec II  IIclw bIII\n",
      "IIIu III  IIp lal\n",
      "LOSS = 183.5225427235966\n",
      "aod,bsilaawlacndaadaaaralalnlidaaanlamaidiaayGidaiiaicanaicnniaaardnaiiiiaIntddahaaidia,iwdi:bldraaid\n",
      "LOSS = 197.02970065187372\n",
      "ase\n",
      "\n",
      "hhhm tyts  lat ta  ho th thu  hhtt hrg ro  f t mr ,hmh ur   os fhs\n",
      "hht rw  ie  hgr,rwr  r  Hhn s\n",
      "LOSS = 166.28152500014158\n",
      "ahceB\n",
      "rrmIuiree reqlare reaeatlne Qh ee rlUoSHTG re vrrrltrrrrdrrHihrtgdirhm \n",
      "reu re :aeoAiEeof: enl \n",
      "LOSS = 181.87778176032978\n",
      "ahh et la i ii  ry  er  Lodh\n",
      "hhmrlW mi.wb.srhekcfilEGg nIA  IIr soe? whI\n",
      "Anhwl dirhd roIu geI .hy  ai\n",
      "LOSS = 220.72044285180112\n",
      "asdoo \n",
      "hKRgtI\n",
      "ImgIIdIWtIttI\n",
      "tmIINh\n",
      "ItIInhItn\n",
      "III\n",
      "I:\n",
      "t\n",
      "s\n",
      "mRmI,tBAt,\n",
      "HIISInuRIEttIyTIEI,F,t,II,TSt?dIgI\n",
      "LOSS = 202.89323997980935\n",
      "afe\n",
      "EtkwnnndeaaRadhaanntBGadwwntthaoaaraRGpcntauDAaaahyatt\n",
      "\n",
      "nnmaatan sacdrtsRdno.atdtpeh\n",
      "WDRGnaTHwSdp\n",
      "LOSS = 204.13130351651193\n",
      "asim  IIdL\n",
      "g III ,IlT, III IoIII  II kII yI HlI IIl mAIs ,mIdPI Iy lII m IyI  IIid III III  IIcHI iaI\n",
      "LOSS = 211.95083926017486\n",
      "a  oeIeIIIIIaCIaemMlI!cI IIIIIfIIRA,IIRRI:sIIIRIIIIIIII,IICrIfIRI,IIRRIIffDIRIIIIIitI.w,IfI,IeIRI,I.R\n",
      "LOSS = 169.25146480587762\n",
      "ae,d'dn hhtml!eDtW,anrchIhN,:mIthswlwi the thaCthfetiahth hi hldpmith etlsShIehWnaSttaerhh\n",
      "mItLeIM Rt\n",
      "LOSS = 181.71593114182562\n",
      "ams,od moshdsiels .hmlAlmlha\n",
      "s\n",
      " dlaasosltl!emlsam \n",
      "dyme  lrphYh \n",
      "lslllT mlll loyl lhaa,fDlAlhdlWely,l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS = 202.46369469457164\n",
      "ahthRlrdo tsusuaoogtToun.sslattaatttaag tleWtaaatoatdnnmlpoostoutfaaolatpaa toatwuattaattaastIaptaada\n",
      "LOSS = 160.65973579980363\n",
      "amgsh\n",
      "hiahAih riarhi'dnphdrevyhirlpir mimmr.nih  uUrHrT\n",
      "mehoOdhehd-isae,grd hirehmgdrdneM mis hire  n\n",
      "LOSS = 156.77798533486182\n",
      "asen Ne aar;rRlgiI eo  ih hht ou hntsrbyt rLrblm aitey GI  it nfrurregkyg aas hn trM di rorsHLatat aa\n",
      "LOSS = 198.59937566314278\n",
      "arnikgosgAoEtAhy  onndb shnudsbvlnpaed hadwdoaprsmnsnfbdy gLha novigd  undn earre naoom:Icy,ddospdoIo\n",
      "LOSS = 216.15762261656963\n",
      "aactbsdf\n",
      "oo \n",
      "o\n",
      " sooooip.dpeed sed wedev mom dObcsed f yeoivootiebwdobodsdonodedwdessdwi yod domemed s\n",
      "LOSS = 210.0406306463147\n",
      "aah  aUe eIm\n",
      "syt tnentwlne,nd\n",
      "\n",
      "fUumewert\n",
      "lre oom tionlng hrtg\n",
      " oaaGa lrg tha lot saane eiupt uee tre \n",
      "LOSS = 214.64526294571326\n",
      "adt soo ,re e ehe l gi\n",
      " lod d ccc . s huI dhd ord drd tim e nhd ete fun vh\n",
      " e m l dhyRlTv fieGm drd;d\n",
      "LOSS = 156.6532920915991\n",
      "a  hree  slSt,,a'w ohs iir srd nbm,d stc sur shd cum npshmp,houyrd nisdspdpf't:\n",
      "gsbsiogt,ifo iIo hofp\n",
      "LOSS = 179.93479671220433\n",
      "aws  lthomw,efgiII\n",
      "er rgod sod  rI rcohwo soecmo, sfb uo soce  ier:rfsrse,us,of lue wo  oo\n",
      "iod tfstno\n",
      "LOSS = 189.4847013536967\n",
      "a groRkWwBR:TN\n",
      "GNaLNYtTR ooLltSA\n",
      "erdSYRElbRRr:BOLR:BASari'EHNM:LI\n",
      "DoAcnua\n",
      "NbBRS\n",
      "rORtlac IBtn\n",
      "riRiREOy\n",
      "LOSS = 199.67995399205992\n",
      "afeeoesdutioeepbgu smnee;er\n",
      "e\n",
      "egreeeoe\n",
      "a\n",
      " Onen eeltwrrye\n",
      ":afNi.dde tosetf' esre eolnaew.dsKwdBrfare\n",
      "e\n",
      "LOSS = 178.77661183958554\n",
      "ate,rGdnde,bdTayd\n",
      "dmn,w,aloandt vha aoa\n",
      "nsdndodYust\n",
      "ahd,a\n",
      "nslrdTaaddretdtdnlb\n",
      "oHhv\n",
      "aena duo\n",
      "shyrU Lfe\n",
      "LOSS = 164.1111823941183\n",
      "ard :h towA\n",
      "tn:Na nwwi\n",
      " onvtc Tbsthy,v wtut Fedusiu taldWw n aawpe tei  Ol.j.tur ty k  heowco Cxkt nh\n",
      "LOSS = 174.2355282699882\n",
      "ae fo\n",
      "ghNa\n",
      "\n",
      "o dhd' ra\n",
      "hoeeo\n",
      "tnwloogCewiAnGI'B\n",
      " eoan oidrgv KomsY\n",
      "\n",
      "hEt oodma\n",
      "SoiYens,f\n",
      "Or lhEsdymoh on\n",
      "LOSS = 156.43646030293544\n",
      "asdec nuni serr wpe y  on hf hav ohio\n",
      "th  lnKrnG kobm,h ' oil' thug hhr ih  nntsBh Mhnt Mhuc  nouba t\n",
      "LOSS = 162.08461841736917\n",
      "as not mr w ard  yi  odn to  eu yioy htew'hrLe\n",
      " Bu Bor  o rewyy ygo ta ehr ,n ieb  rcongba  heie ie  \n",
      "LOSS = 172.42646669128928\n",
      "aots\n",
      "itatli wr \n",
      "ol: Go gh DiskutinaW fh eam \n",
      "hr tnlt s iaetar,ls lovt  oy  inhehirb\n",
      "oi te :o no, oaos\n",
      "LOSS = 178.57334716617765\n",
      "a tysh momtm\n",
      "muerrrfdege'mot e,mt\n",
      " enege eict\n",
      "we ntmse!eslfABn;mcafebcdmImGmuetmle,ebms\n",
      "bea\n",
      "'sIK mImw\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory Networks (LSTMs)\n",
    "\n",
    "![Long Short-Term Memory Networks Diagram](data/LSTM.png)\n",
    "(Image from the [LSTM Tutorial](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update of an LSTM is given by the following equations:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(U_i x_t + W_i h_{t-1} + b_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(U_f x_t + W_f h_{t-1} + b_f)\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(U_o x_t + W_o h_{t-1} + b_o)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(U_C x_t + W_C h_{t-1} + b_C)\n",
    "$$\n",
    "\n",
    "$$\n",
    "C_t = i_t * \\tilde{C}_t + f_t * C_{t-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(input_size, hidden_size, output_size):\n",
    "    params = {\n",
    "        'U_i': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W_i': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'b_i': np.zeros(hidden_size),\n",
    "        \n",
    "        'U_f': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W_f': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'b_f': np.zeros(hidden_size),\n",
    "        \n",
    "        'U_o': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W_o': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'b_o': np.zeros(hidden_size),\n",
    "        \n",
    "        'U_c': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'W_c': np.random.randn(hidden_size, hidden_size) * 0.01,\n",
    "        'b_c': np.zeros(hidden_size),\n",
    "        \n",
    "        'V': np.random.randn(output_size, hidden_size) * 0.01,\n",
    "        'b': np.zeros(output_size)\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "def model(params, x, h_prev, C_prev):\n",
    "    # TODO\n",
    "    i_t = sigmoid(np.dot(params['U_i'], x) + np.dot(params['W_i'], h_prev) + params['b_i'])\n",
    "    f_t = sigmoid(np.dot(params['U_f'], x) + np.dot(params['W_f'], h_prev) + params['b_f'])\n",
    "    o_t = sigmoid(np.dot(params['U_o'], x) + np.dot(params['W_o'], h_prev) + params['b_o'])\n",
    "    \n",
    "    C_t_tilde = np.tanh(np.dot(params['U_C'], x) + np.dot([params['W_C'], h]))\n",
    "    C_t = i_t * C_t_tild + f_t * C_prev\n",
    "    h_t = o_t * np.tanh(C_t)\n",
    "    \n",
    "    y = softmax(np.dot(params['V'], h_t) + params['b'])\n",
    "    \n",
    "    return y, h_t, C_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_hidden(hidden_size):\n",
    "    return np.zeros(hidden_size), np.zeros(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(params, input_seq, target_seq, opts):\n",
    "    \"\"\"\n",
    "    Compute the loss of RNN based on data.\n",
    "    \n",
    "    :param params: dict of str: tensor, including keys U, W, v, b_h, b_o.\n",
    "    :param input_seq: list of str. Input string.\n",
    "    :param target_seq: list of str. Target string.\n",
    "    :param opts: dict of str: int. Including keys input_size, hidden_size, output_size.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    hidden, cell = initialize_hidden(opts['hidden_size'])\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(input_seq)):\n",
    "        output, hidden, cell = model(params, input_seq[i], hidden, cell)\n",
    "        loss += criterion(output, target_seq[i])\n",
    "    return loss\n",
    "\n",
    "loss_grad = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(params, initial, length, opts):\n",
    "    \"\"\"\n",
    "    Sampling a string with a Recurrent neural network.\n",
    "    \n",
    "    :param params: dict of str: tensor, including keys U, W, v, b_h, b_o\n",
    "    :param initial: str. Beginning character.\n",
    "    :param length: length of the generated string.\n",
    "    :param opts: dict of str: int. Including keys input_size, hidden_size, output_size.\n",
    "    \n",
    "    :return final_string: str. \n",
    "    \"\"\"\n",
    "    hidden, cell = initialize_hidden(opts['hidden_size'])\n",
    "    current_char = initial\n",
    "    final_string = initial\n",
    "    \n",
    "    for i in range(length):\n",
    "        x = create_one_hot(char_to_index[current_char], opts['input_size'])\n",
    "        output, hidden, cell = model(params, x, hidden, cell)\n",
    "        \n",
    "        p = output\n",
    "        current_index = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        current_char = index_to_char[current_index]\n",
    "        final_string += current_char\n",
    "    \n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS = 208.71701201159402\n",
      "aIDzPFQ;tktFfjVie--.gy zigGDSUCox'T$RQmnFh?e$bpZyDc\n",
      "e'&N;mletezWpcsUQcGoZwGaw3-seq LQ,bAXjl wVzkKtRVI\n",
      "LOSS = 157.0608221025338\n",
      "aIeslitQehaKM oprMfScNki\n",
      "eKtyU? xnetnoh\n",
      "uqVs c3o MaSt;W3  el keer r$ i:iIts pe  ZaQo ea orhnhi deYej-\n",
      "LOSS = 155.82930581815046\n",
      "aih iadete ZHah\n",
      "&Z rew\n",
      "nbaadiiezST hmtleiOpu frUolbe L eetCs Xllh sR othrIe tBc hhiwr: Spwu'c  iclt e\n",
      "LOSS = 154.89422064955778\n",
      "ae h ee r pf  'onrglonlho u'h:\n",
      "-ioho klosssyhlshhehui\n",
      "tt.edSrt;deifdmsfnmi u e s toowhS nslifHCtimuhn\n",
      "LOSS = 171.95349774193747\n",
      "aaX\n",
      "'\n",
      "Jdum  fh3iupt,twiehI ae folrysoEltaoI .,ihIetflRh  aA,\n",
      "- hceun uIC!?bA\n",
      "relr\n",
      "hLioy IRpi n Ajlo:h\n",
      "LOSS = 156.55424390462903\n",
      "ae he. rhs w\n",
      "mrl pcu sghafotorans it hvahlTs\n",
      "tedosih lay3ew \n",
      "eoumsu aeis asyr Aoh os vi s roAt.  ohh \n",
      "LOSS = 150.09877143822618\n",
      "aHEaishyte recawrsN ts oos\n",
      " eah kaemabh; oto i\n",
      "seru ehrfIe flyVse re thm,nhagsee cRoedbcRwoNhi'\n",
      "trlk:\n",
      "LOSS = 144.21268876246899\n",
      "atUMeIO I,sr, e httf :ho emoro or, \n",
      "eooet nTTofpoaurIl\n",
      " 'erbeawunmyMvp m .ee somhe\n",
      "io abd eg dhnse  d\n",
      "LOSS = 168.78332929006555\n",
      "a e woImosdlshW siisew os rghe a t\n",
      "h oo:tiidootSoelno,nol\n",
      ",Smes hsoyietonloue auwiirdot re hbfNe o Kg\n",
      "LOSS = 143.9709815386821\n",
      "aoon inod srge tra thaer lee t. tu: hpierr tds lma:D ftHeced\n",
      " !eada\n",
      "nIoenN;o ohAnsVetl\n",
      "?ls oQC pion i\n",
      "LOSS = 141.72794049771483\n",
      "aed sSQdo chue oese\n",
      "mt, ubiy ho hafs no'e h\n",
      "nt\n",
      "t\n",
      "te e hh ehat\n",
      "thh\n",
      "utTEewts trd hhraeo a hhSu yo ak wd\n",
      "LOSS = 135.08731992218372\n",
      "aase tt so iowd uTmet tp he has mor the ftoc oeer 'e y m,nod se waorb trhu\n",
      "d tot'en.\n",
      "hei' Mvuk\n",
      " theo \n",
      "LOSS = 152.10790357920706\n",
      "abawi'hA\n",
      "C;hStit mocugitomoceide soul: tauLdt loutl, das fps'th chate podog\n",
      "co toto Usists uoreMeUoh \n",
      "LOSS = 134.01522277838043\n",
      "ans\n",
      "UaVh eo mes hos\n",
      "Rt lasarceny ihe Sosd sonfca. hheng pe sounpe wacoyleoteI Rof tuu uurs Cod yorito\n",
      "LOSS = 134.32839071106477\n",
      "aNolt liv re, lhawsunaWin nf mor urgeitho U weed vs:dinr.t fonrleat oon sgenv cinainllos eitianunu:??\n",
      "LOSS = 141.8118676981399\n",
      "as:r\n",
      "Tk wotnet non' ceew\n",
      "Safy war k pt wmere,\n",
      "SS oiwe\n",
      "SpRwMZ\n",
      "tRhl, wv cuf, ensd t mose pas bat tha co\n",
      "LOSS = 120.85647823237741\n",
      "ats Codisgong woout he yyes harat pe pulSthege,\n",
      "\n",
      "TefiltSnthem.\n",
      "\n",
      "WiIATWIUy\n",
      "CASPU: fiminy bamither'nt b\n",
      "LOSS = 132.8592689623518\n",
      "ar whi? kha;cey\n",
      "Welp:\n",
      "Nr:\n",
      "VCCPC\n",
      "SNEUR\n",
      "WireS.\n",
      "IOOESUOU:\n",
      "OMOU:A\n",
      "HII\n",
      "AM:\n",
      "IMIT:\n",
      "\n",
      "TMwS\n",
      "SNUOnUU\n",
      "CUO\n",
      "IULUL:\n",
      "\n",
      "LOSS = 115.85945797270584\n",
      "aisteolgey thomte 'arowe the whyghel?\n",
      "Andin On iisreI: Bwm, ad gsevg maret, weuther yort ast weiole m\n",
      "LOSS = 118.80231675246262\n",
      "aO, thon, owey\n",
      "nother fowresfimy we nor pit indi, fou's he threracon, srpofephannonad sofs?e- peom in\n",
      "LOSS = 125.22806798058828\n",
      "ag, han fotens\n",
      "O'RUyoAe:\n",
      "\n",
      "AoOOIoS:\n",
      "Thon otoud\n",
      "IFIOS\n",
      "ISIFINLon,\n",
      "ha-sl!\n",
      "CIUIRNIIUS,\n",
      "\n",
      "IRCI'LDins.\n",
      "\n",
      "I:OUI\n",
      "LOSS = 134.64383003885203\n",
      "ace thes th ot tone thellle potiinhes-ew tilidt ls hlide sesit hl dure\n",
      "I thoen womit th is heidons tn\n",
      "LOSS = 129.00656989088966\n",
      "atey\n",
      "T'ly I wovame werderis givinein hascirlanle, afd intey st cath shou bepme andyiteno thiud thou t\n",
      "LOSS = 108.35580032696677\n",
      "asd tot ar hecgaene femar, meam-\n",
      "Anlreply ghet cirs fo, anre sos toure as os, efirerenan, nfisgeto ce\n",
      "LOSS = 122.15554209663424\n",
      "ame apo tht kerleL\n",
      "\n",
      "DOnore nam weo woune cou aad nure mac:\n",
      "Ehe fere?\n",
      "\n",
      "Hrine teud !es ald\n",
      "Lufnenthoe t\n",
      "LOSS = 115.9447132652472\n",
      "arnbyave six ou nolen.\n",
      "\n",
      "Sesebot coul wiand srr ang. and ans enthon,\n",
      "Yo thars ntouUWors hin harerlerS:\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
