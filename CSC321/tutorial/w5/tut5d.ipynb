{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 60.67733383178711)\n",
      "(1, 60.557655334472656)\n",
      "(2, 60.50690460205078)\n",
      "(3, 60.20283508300781)\n",
      "(4, 60.2366943359375)\n",
      "(5, 60.04125213623047)\n",
      "(6, 58.39849090576172)\n",
      "(7, 57.80139923095703)\n",
      "(8, 59.34163284301758)\n",
      "(9, 59.806427001953125)\n",
      "(10, 55.32948684692383)\n",
      "(11, 60.001625061035156)\n",
      "(12, 60.0587043762207)\n",
      "(13, 53.031707763671875)\n",
      "(14, 60.024757385253906)\n",
      "(15, 58.687042236328125)\n",
      "(16, 59.83119583129883)\n",
      "(17, 59.61639404296875)\n",
      "(18, 59.323822021484375)\n",
      "(19, 85.74945831298828)\n",
      "(20, 58.4573974609375)\n",
      "(21, 74.5245132446289)\n",
      "(22, 65.59202575683594)\n",
      "(23, 57.28229904174805)\n",
      "(24, 57.09622573852539)\n",
      "(25, 43.50196075439453)\n",
      "(26, 56.76669692993164)\n",
      "(27, 34.932281494140625)\n",
      "(28, 56.518531799316406)\n",
      "(29, 57.64817810058594)\n",
      "(30, 56.02167510986328)\n",
      "(31, 56.90195083618164)\n",
      "(32, 25.23123550415039)\n",
      "(33, 59.526519775390625)\n",
      "(34, 59.283817291259766)\n",
      "(35, 52.73845291137695)\n",
      "(36, 58.63340759277344)\n",
      "(37, 58.21649169921875)\n",
      "(38, 21.990779876708984)\n",
      "(39, 46.5034294128418)\n",
      "(40, 21.265193939208984)\n",
      "(41, 49.59947967529297)\n",
      "(42, 55.759029388427734)\n",
      "(43, 39.704586029052734)\n",
      "(44, 37.51992416381836)\n",
      "(45, 34.95846939086914)\n",
      "(46, 42.642242431640625)\n",
      "(47, 22.09578514099121)\n",
      "(48, 27.908586502075195)\n",
      "(49, 22.158042907714844)\n",
      "(50, 35.19422149658203)\n",
      "(51, 20.533267974853516)\n",
      "(52, 31.697378158569336)\n",
      "(53, 20.95757293701172)\n",
      "(54, 40.34346008300781)\n",
      "(55, 19.272396087646484)\n",
      "(56, 35.220855712890625)\n",
      "(57, 18.384912490844727)\n",
      "(58, 16.50896644592285)\n",
      "(59, 17.898597717285156)\n",
      "(60, 18.010303497314453)\n",
      "(61, 16.867029190063477)\n",
      "(62, 16.354106903076172)\n",
      "(63, 13.5781831741333)\n",
      "(64, 12.940211296081543)\n",
      "(65, 12.138765335083008)\n",
      "(66, 14.08194637298584)\n",
      "(67, 13.58593463897705)\n",
      "(68, 13.29613208770752)\n",
      "(69, 16.464576721191406)\n",
      "(70, 12.410504341125488)\n",
      "(71, 11.859572410583496)\n",
      "(72, 9.125237464904785)\n",
      "(73, 11.318455696105957)\n",
      "(74, 8.655579566955566)\n",
      "(75, 11.811761856079102)\n",
      "(76, 8.13470458984375)\n",
      "(77, 7.836601257324219)\n",
      "(78, 10.96450138092041)\n",
      "(79, 7.321742057800293)\n",
      "(80, 10.114285469055176)\n",
      "(81, 9.630178451538086)\n",
      "(82, 6.962170124053955)\n",
      "(83, 6.901545524597168)\n",
      "(84, 8.378814697265625)\n",
      "(85, 8.219749450683594)\n",
      "(86, 8.09935474395752)\n",
      "(87, 9.046180725097656)\n",
      "(88, 8.860462188720703)\n",
      "(89, 7.73004150390625)\n",
      "(90, 8.67687702178955)\n",
      "(91, 8.149956703186035)\n",
      "(92, 7.180648326873779)\n",
      "(93, 7.2597551345825195)\n",
      "(94, 8.130077362060547)\n",
      "(95, 7.629186153411865)\n",
      "(96, 7.815554618835449)\n",
      "(97, 6.968530654907227)\n",
      "(98, 6.788381576538086)\n",
      "(99, 7.182154655456543)\n",
      "(100, 6.7291154861450195)\n",
      "(101, 6.74243688583374)\n",
      "(102, 7.366963863372803)\n",
      "(103, 7.084532260894775)\n",
      "(104, 7.185580730438232)\n",
      "(105, 6.486534595489502)\n",
      "(106, 6.498326778411865)\n",
      "(107, 6.906259059906006)\n",
      "(108, 6.820475101470947)\n",
      "(109, 6.812386512756348)\n",
      "(110, 6.697079181671143)\n",
      "(111, 6.86323881149292)\n",
      "(112, 6.404666423797607)\n",
      "(113, 6.248569011688232)\n",
      "(114, 7.393202304840088)\n",
      "(115, 7.142021179199219)\n",
      "(116, 5.954773426055908)\n",
      "(117, 5.905509948730469)\n",
      "(118, 5.8215227127075195)\n",
      "(119, 6.397548198699951)\n",
      "(120, 7.062206268310547)\n",
      "(121, 6.3631205558776855)\n",
      "(122, 5.382593631744385)\n",
      "(123, 6.272073745727539)\n",
      "(124, 6.271634578704834)\n",
      "(125, 5.3247246742248535)\n",
      "(126, 6.2194976806640625)\n",
      "(127, 6.167703628540039)\n",
      "(128, 6.039644241333008)\n",
      "(129, 5.912643909454346)\n",
      "(130, 5.2892656326293945)\n",
      "(131, 5.887610912322998)\n",
      "(132, 5.34419584274292)\n",
      "(133, 7.015701770782471)\n",
      "(134, 5.943435192108154)\n",
      "(135, 5.779914379119873)\n",
      "(136, 5.7244873046875)\n",
      "(137, 5.794328212738037)\n",
      "(138, 6.2093305587768555)\n",
      "(139, 5.689828872680664)\n",
      "(140, 5.380359649658203)\n",
      "(141, 6.055852890014648)\n",
      "(142, 5.667889595031738)\n",
      "(143, 5.576230049133301)\n",
      "(144, 5.359076499938965)\n",
      "(145, 5.603219509124756)\n",
      "(146, 5.617859840393066)\n",
      "(147, 6.048145771026611)\n",
      "(148, 5.506518363952637)\n",
      "(149, 5.462547779083252)\n",
      "(150, 5.458017826080322)\n",
      "(151, 5.635121822357178)\n",
      "(152, 5.488288402557373)\n",
      "(153, 5.583947658538818)\n",
      "(154, 5.423912048339844)\n",
      "(155, 5.4362568855285645)\n",
      "(156, 5.415647029876709)\n",
      "(157, 5.319799900054932)\n",
      "(158, 5.346197128295898)\n",
      "(159, 5.625664234161377)\n",
      "(160, 5.572397232055664)\n",
      "(161, 5.265712261199951)\n",
      "(162, 5.460799217224121)\n",
      "(163, 5.130711555480957)\n",
      "(164, 5.431112289428711)\n",
      "(165, 5.430069923400879)\n",
      "(166, 5.081043720245361)\n",
      "(167, 5.35132360458374)\n",
      "(168, 5.037204742431641)\n",
      "(169, 5.014092445373535)\n",
      "(170, 5.712575435638428)\n",
      "(171, 4.968111038208008)\n",
      "(172, 4.940601825714111)\n",
      "(173, 4.902514934539795)\n",
      "(174, 5.3134236335754395)\n",
      "(175, 4.823390007019043)\n",
      "(176, 5.298056125640869)\n",
      "(177, 4.775326728820801)\n",
      "(178, 4.757450103759766)\n",
      "(179, 4.736386299133301)\n",
      "(180, 5.170711040496826)\n",
      "(181, 5.858572483062744)\n",
      "(182, 5.085143089294434)\n",
      "(183, 4.670422554016113)\n",
      "(184, 4.676149368286133)\n",
      "(185, 5.764986515045166)\n",
      "(186, 5.293036937713623)\n",
      "(187, 4.6694817543029785)\n",
      "(188, 5.1982879638671875)\n",
      "(189, 4.636701583862305)\n",
      "(190, 5.13608980178833)\n",
      "(191, 4.990204811096191)\n",
      "(192, 5.584332466125488)\n",
      "(193, 5.022398471832275)\n",
      "(194, 4.970114707946777)\n",
      "(195, 4.928466320037842)\n",
      "(196, 5.461318016052246)\n",
      "(197, 4.75883674621582)\n",
      "(198, 4.83725643157959)\n",
      "(199, 4.78715705871582)\n",
      "(200, 4.808915138244629)\n",
      "(201, 5.319058418273926)\n",
      "(202, 4.805387496948242)\n",
      "(203, 4.769721984863281)\n",
      "(204, 4.752608776092529)\n",
      "(205, 4.740292072296143)\n",
      "(206, 4.777882099151611)\n",
      "(207, 5.195057392120361)\n",
      "(208, 5.158323287963867)\n",
      "(209, 5.096693515777588)\n",
      "(210, 4.790202617645264)\n",
      "(211, 4.686527729034424)\n",
      "(212, 4.6837615966796875)\n",
      "(213, 4.754861354827881)\n",
      "(214, 4.662863254547119)\n",
      "(215, 4.823966026306152)\n",
      "(216, 4.630679607391357)\n",
      "(217, 4.806593418121338)\n",
      "(218, 4.790313720703125)\n",
      "(219, 4.711965560913086)\n",
      "(220, 4.891071319580078)\n",
      "(221, 4.888794898986816)\n",
      "(222, 4.7029805183410645)\n",
      "(223, 4.851737022399902)\n",
      "(224, 4.633824825286865)\n",
      "(225, 4.79880952835083)\n",
      "(226, 4.76453971862793)\n",
      "(227, 4.666309833526611)\n",
      "(228, 4.66224479675293)\n",
      "(229, 4.713164329528809)\n",
      "(230, 4.7142462730407715)\n",
      "(231, 4.639189720153809)\n",
      "(232, 4.628355026245117)\n",
      "(233, 4.64970064163208)\n",
      "(234, 4.607655048370361)\n",
      "(235, 4.658052444458008)\n",
      "(236, 4.581717491149902)\n",
      "(237, 4.58195161819458)\n",
      "(238, 4.62315559387207)\n",
      "(239, 4.679662227630615)\n",
      "(240, 4.678212642669678)\n",
      "(241, 4.553762435913086)\n",
      "(242, 4.533097743988037)\n",
      "(243, 4.535967826843262)\n",
      "(244, 4.583733558654785)\n",
      "(245, 4.646662712097168)\n",
      "(246, 4.635091781616211)\n",
      "(247, 4.521191596984863)\n",
      "(248, 4.517860412597656)\n",
      "(249, 4.507574558258057)\n",
      "(250, 4.501389503479004)\n",
      "(251, 4.597059726715088)\n",
      "(252, 4.592702865600586)\n",
      "(253, 4.586195468902588)\n",
      "(254, 4.477685928344727)\n",
      "(255, 4.5928473472595215)\n",
      "(256, 4.473300933837891)\n",
      "(257, 4.574717998504639)\n",
      "(258, 4.474493503570557)\n",
      "(259, 4.464720726013184)\n",
      "(260, 4.461918354034424)\n",
      "(261, 4.542304039001465)\n",
      "(262, 4.450843334197998)\n",
      "(263, 4.445085525512695)\n",
      "(264, 4.437340259552002)\n",
      "(265, 4.429740905761719)\n",
      "(266, 4.53759765625)\n",
      "(267, 4.534741401672363)\n",
      "(268, 4.52304744720459)\n",
      "(269, 4.505080223083496)\n",
      "(270, 4.678539752960205)\n",
      "(271, 4.667533874511719)\n",
      "(272, 4.452695846557617)\n",
      "(273, 4.459876537322998)\n",
      "(274, 4.432709217071533)\n",
      "(275, 4.474026203155518)\n",
      "(276, 4.42293119430542)\n",
      "(277, 4.488847255706787)\n",
      "(278, 4.416616439819336)\n",
      "(279, 4.4113569259643555)\n",
      "(280, 4.403866291046143)\n",
      "(281, 4.5385942459106445)\n",
      "(282, 4.6126275062561035)\n",
      "(283, 4.493348121643066)\n",
      "(284, 4.4872727394104)\n",
      "(285, 4.399688720703125)\n",
      "(286, 4.402148723602295)\n",
      "(287, 4.471817970275879)\n",
      "(288, 4.406322956085205)\n",
      "(289, 4.4075469970703125)\n",
      "(290, 4.447474956512451)\n",
      "(291, 4.640151023864746)\n",
      "(292, 4.44690465927124)\n",
      "(293, 4.437010288238525)\n",
      "(294, 4.426604270935059)\n",
      "(295, 4.621254920959473)\n",
      "(296, 4.404979705810547)\n",
      "(297, 4.597728729248047)\n",
      "(298, 4.393592357635498)\n",
      "(299, 4.489526271820068)\n",
      "(300, 4.41182804107666)\n",
      "(301, 4.499270915985107)\n",
      "(302, 4.38857364654541)\n",
      "(303, 4.5359039306640625)\n",
      "(304, 4.494202613830566)\n",
      "(305, 4.517762184143066)\n",
      "(306, 4.4070611000061035)\n",
      "(307, 4.4939446449279785)\n",
      "(308, 4.479380130767822)\n",
      "(309, 4.461441993713379)\n",
      "(310, 4.485769748687744)\n",
      "(311, 4.485816955566406)\n",
      "(312, 4.474403381347656)\n",
      "(313, 4.466384410858154)\n",
      "(314, 4.465301990509033)\n",
      "(315, 4.4673590660095215)\n",
      "(316, 4.466842174530029)\n",
      "(317, 4.439452648162842)\n",
      "(318, 4.444406509399414)\n",
      "(319, 4.464662551879883)\n",
      "(320, 4.462230682373047)\n",
      "(321, 4.405836582183838)\n",
      "(322, 4.400737762451172)\n",
      "(323, 4.45412015914917)\n",
      "(324, 4.423112392425537)\n",
      "(325, 4.386907577514648)\n",
      "(326, 4.451695442199707)\n",
      "(327, 4.4122796058654785)\n",
      "(328, 4.406808376312256)\n",
      "(329, 4.380458831787109)\n",
      "(330, 4.377285003662109)\n",
      "(331, 4.549539566040039)\n",
      "(332, 4.543290615081787)\n",
      "(333, 4.5268754959106445)\n",
      "(334, 4.4746551513671875)\n",
      "(335, 4.375174045562744)\n",
      "(336, 4.476576328277588)\n",
      "(337, 4.3847527503967285)\n",
      "(338, 4.467788219451904)\n",
      "(339, 4.390443801879883)\n",
      "(340, 4.39024019241333)\n",
      "(341, 4.413168907165527)\n",
      "(342, 4.382153034210205)\n",
      "(343, 4.398918628692627)\n",
      "(344, 4.478070259094238)\n",
      "(345, 4.3776068687438965)\n",
      "(346, 4.479978561401367)\n",
      "(347, 4.391437530517578)\n",
      "(348, 4.473912239074707)\n",
      "(349, 4.470612525939941)\n",
      "(350, 4.466990947723389)\n",
      "(351, 4.45538330078125)\n",
      "(352, 4.4456610679626465)\n",
      "(353, 4.3890509605407715)\n",
      "(354, 4.411402225494385)\n",
      "(355, 4.406843185424805)\n",
      "(356, 4.417347431182861)\n",
      "(357, 4.414187431335449)\n",
      "(358, 4.4112982749938965)\n",
      "(359, 4.425508499145508)\n",
      "(360, 4.405393600463867)\n",
      "(361, 4.480299472808838)\n",
      "(362, 4.478862762451172)\n",
      "(363, 4.407166957855225)\n",
      "(364, 4.408185005187988)\n",
      "(365, 4.397118091583252)\n",
      "(366, 4.406752109527588)\n",
      "(367, 4.408385753631592)\n",
      "(368, 4.452371120452881)\n",
      "(369, 4.4468255043029785)\n",
      "(370, 4.401159763336182)\n",
      "(371, 4.418130397796631)\n",
      "(372, 4.42738676071167)\n",
      "(373, 4.419964790344238)\n",
      "(374, 4.4175262451171875)\n",
      "(375, 4.40313720703125)\n",
      "(376, 4.416152477264404)\n",
      "(377, 4.414707183837891)\n",
      "(378, 4.408082485198975)\n",
      "(379, 4.390511989593506)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380, 4.452892780303955)\n",
      "(381, 4.390689849853516)\n",
      "(382, 4.4334492683410645)\n",
      "(383, 4.385260581970215)\n",
      "(384, 4.408660888671875)\n",
      "(385, 4.380099773406982)\n",
      "(386, 4.378297328948975)\n",
      "(387, 4.413572311401367)\n",
      "(388, 4.487453937530518)\n",
      "(389, 4.383909225463867)\n",
      "(390, 4.425685405731201)\n",
      "(391, 4.472629547119141)\n",
      "(392, 4.4620537757873535)\n",
      "(393, 4.447628974914551)\n",
      "(394, 4.40129280090332)\n",
      "(395, 4.432995319366455)\n",
      "(396, 4.389641284942627)\n",
      "(397, 4.3939361572265625)\n",
      "(398, 4.396951198577881)\n",
      "(399, 4.395326614379883)\n",
      "(400, 4.422684669494629)\n",
      "(401, 4.3825602531433105)\n",
      "(402, 4.422395706176758)\n",
      "(403, 4.367880344390869)\n",
      "(404, 4.426297187805176)\n",
      "(405, 4.431412696838379)\n",
      "(406, 4.400362968444824)\n",
      "(407, 4.396788597106934)\n",
      "(408, 4.428168296813965)\n",
      "(409, 4.426473140716553)\n",
      "(410, 4.420991897583008)\n",
      "(411, 4.389948844909668)\n",
      "(412, 4.3873748779296875)\n",
      "(413, 4.381434917449951)\n",
      "(414, 4.379701614379883)\n",
      "(415, 4.377845764160156)\n",
      "(416, 4.377194881439209)\n",
      "(417, 4.371018886566162)\n",
      "(418, 4.3631911277771)\n",
      "(419, 4.359412670135498)\n",
      "(420, 4.359447956085205)\n",
      "(421, 4.358639240264893)\n",
      "(422, 4.462522506713867)\n",
      "(423, 4.462813377380371)\n",
      "(424, 4.375345706939697)\n",
      "(425, 4.491261959075928)\n",
      "(426, 4.4847025871276855)\n",
      "(427, 4.361660957336426)\n",
      "(428, 4.4376397132873535)\n",
      "(429, 4.361075401306152)\n",
      "(430, 4.365259170532227)\n",
      "(431, 4.433093070983887)\n",
      "(432, 4.426093578338623)\n",
      "(433, 4.37870979309082)\n",
      "(434, 4.414007663726807)\n",
      "(435, 4.383471965789795)\n",
      "(436, 4.402721881866455)\n",
      "(437, 4.395844459533691)\n",
      "(438, 4.390633583068848)\n",
      "(439, 4.3923187255859375)\n",
      "(440, 4.378188610076904)\n",
      "(441, 4.373323440551758)\n",
      "(442, 4.394758224487305)\n",
      "(443, 4.445034980773926)\n",
      "(444, 4.411828517913818)\n",
      "(445, 4.439828872680664)\n",
      "(446, 4.367067337036133)\n",
      "(447, 4.398656368255615)\n",
      "(448, 4.371424674987793)\n",
      "(449, 4.37160062789917)\n",
      "(450, 4.423704147338867)\n",
      "(451, 4.417077541351318)\n",
      "(452, 4.388391494750977)\n",
      "(453, 4.423107624053955)\n",
      "(454, 4.421127796173096)\n",
      "(455, 4.413768291473389)\n",
      "(456, 4.387406349182129)\n",
      "(457, 4.3991475105285645)\n",
      "(458, 4.392279624938965)\n",
      "(459, 4.398416519165039)\n",
      "(460, 4.38910436630249)\n",
      "(461, 4.394229888916016)\n",
      "(462, 4.395026683807373)\n",
      "(463, 4.381589412689209)\n",
      "(464, 4.3903374671936035)\n",
      "(465, 4.399721622467041)\n",
      "(466, 4.379012584686279)\n",
      "(467, 4.388123989105225)\n",
      "(468, 4.400422096252441)\n",
      "(469, 4.3996148109436035)\n",
      "(470, 4.396365642547607)\n",
      "(471, 4.392417907714844)\n",
      "(472, 4.387090682983398)\n",
      "(473, 4.38407564163208)\n",
      "(474, 4.406068325042725)\n",
      "(475, 4.407023906707764)\n",
      "(476, 4.402780532836914)\n",
      "(477, 4.379377365112305)\n",
      "(478, 4.395539283752441)\n",
      "(479, 4.3999152183532715)\n",
      "(480, 4.37941312789917)\n",
      "(481, 4.377434253692627)\n",
      "(482, 4.398130893707275)\n",
      "(483, 4.39787483215332)\n",
      "(484, 4.375011444091797)\n",
      "(485, 4.397485256195068)\n",
      "(486, 4.383632183074951)\n",
      "(487, 4.381467342376709)\n",
      "(488, 4.3989691734313965)\n",
      "(489, 4.3964362144470215)\n",
      "(490, 4.390561103820801)\n",
      "(491, 4.380716323852539)\n",
      "(492, 4.379085063934326)\n",
      "(493, 4.408818244934082)\n",
      "(494, 4.377119541168213)\n",
      "(495, 4.376714706420898)\n",
      "(496, 4.367066860198975)\n",
      "(497, 4.363580703735352)\n",
      "(498, 4.36845064163208)\n",
      "(499, 4.363354682922363)\n"
     ]
    }
   ],
   "source": [
    "# By Justin Johnson https://github.com/jcjohnson/pytorch-examples/blob/master/nn/dynamic_net.py\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\"\"\"\n",
    "To showcase the power of PyTorch dynamic graphs, we will implement a very strange\n",
    "model: a fully-connected ReLU network that on each forward pass randomly chooses\n",
    "a number between 1 and 4 and has that many hidden layers, reusing the same\n",
    "weights multiple times to compute the innermost hidden layers.\n",
    "\"\"\"\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we construct three nn.Linear instances that we will use\n",
    "    in the forward pass.\n",
    "    \"\"\"\n",
    "    super(DynamicNet, self).__init__()\n",
    "    self.input_linear = torch.nn.Linear(D_in, H)\n",
    "    self.middle_linear = torch.nn.Linear(H, H)\n",
    "    self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "  def forward(self, x, verbose = False):\n",
    "    \"\"\"\n",
    "    For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "    and reuse the middle_linear Module that many times to compute hidden layer\n",
    "    representations.\n",
    "    Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "    Python control-flow operators like loops or conditional statements when\n",
    "    defining the forward pass of the model.\n",
    "    Here we also see that it is perfectly safe to reuse the same Module many\n",
    "    times when defining a computational graph. This is a big improvement from Lua\n",
    "    Torch, where each Module could be used only once.\n",
    "    \"\"\"\n",
    "    h_relu = self.input_linear(x).clamp(min=0)\n",
    "    n_layers = random.randint(0, 3)\n",
    "    if verbose:\n",
    "        print(\"The number of layers for this run is\", n_layers)\n",
    "        # print(h_relu)\n",
    "    for _ in range(n_layers):\n",
    "        h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        if verbose:\n",
    "            pass\n",
    "            # print(h_relu)\n",
    "    y_pred = self.output_linear(h_relu)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 10, 1\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "  # Forward pass: Compute predicted y by passing x to the model\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Compute and print loss\n",
    "  loss = criterion(y_pred, y)\n",
    "  print(t, loss.data[0])\n",
    "\n",
    "  # Zero gradients, perform a backward pass, and update the weights.\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.3882\n",
       " 0.3893\n",
       "-1.2886\n",
       " 0.1647\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.4267\n",
       " 0.4167\n",
       "-1.3781\n",
       " 0.1412\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)[1:5] # another run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.4267\n",
       " 0.4167\n",
       "-1.3781\n",
       " 0.1412\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks consistent! Let's now try to see what's happening inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of layers for this run is', 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.4210\n",
       " 0.3838\n",
       "-1.2843\n",
       " 0.1529\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, verbose = True)[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of layers for this run is', 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.3882\n",
       " 0.3893\n",
       "-1.2886\n",
       " 0.1647\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, verbose = True)[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of layers for this run is', 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.4267\n",
       " 0.4167\n",
       "-1.3781\n",
       " 0.1412\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, verbose = True)[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of layers for this run is', 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.3882\n",
       " 0.3893\n",
       "-1.2886\n",
       " 0.1647\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, verbose = True)[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of layers for this run is', 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.3882\n",
       " 0.3893\n",
       "-1.2886\n",
       " 0.1647\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, verbose = True)[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.8895\n",
       " 0.3787\n",
       "-1.3019\n",
       " 0.1532\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
