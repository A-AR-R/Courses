\documentclass[11pt]{article}
\input{/Users/markwang/.preamble}
\begin{document}

\subsection*{Problem 1}

Consider the problem of multiplying two $n$-bit integers $x$, $y$, and suppose that $n$ is a multiple of 3 (if not, we add one or two zeroes on the left to make this true). This time, suppose that we split each integer into three equal parts instead of two.

\begin{enumerate}
  \item State the exact relationship between $x$ and each of its three parts $X_2$, $X_1$, $X_0$ (i.e., give an equation that involves $x$ and $X_2$, $X_1$, $X_0$)
    \[
      x = 2^{\rfrac{2n}{3}}X_{2} + 2^{\rfrac{n}{3}}X_{1} + X_{0}
    \]
  \item Write a divide-and-conquer algorithm to multiply two integers $x$, $y$ based on this three-way split. Justify that your algorithm produces the correct answer, i.e., show that the output of your algorithm is equal to $x\cdot y$. For full marks, your algorithm must run in time $O(n^2)$, i.e., strictly less than $O(n^2)$. Justify that this is the case by computing the running time of your algorithm (you may use the Master Theorem as long as you state clearly how it applies to your algorithm)
  \begin{solution}
    The algorithm is as follows
    \begin{enumerate}
      \item Compute $\rfrac{n}{3}$-bit long $X_2, X_1, X_0$ by shifting $x$ appropriate number of bits such that the previous formula holds. Similarly, compute $Y_2, Y_1, Y_0$
      \item Compute the following expression by recursively calling the multiplication algorithm on $\rfrac{n}{3}$-bit integers
      \begin{enumerate}
        \item $p_0\leftarrow X_0 Y_0$
        \item $p_1\leftarrow X_1 Y_1$
        \item $p_2\leftarrow X_2 Y_2$
        \item $p_3\leftarrow (X_0 + X_1)(Y_0 + Y_1)$
        \item $p_4\leftarrow (X_0 + X_2)(Y_0 + Y_2)$
        \item $p_5\leftarrow(X_1 + X_2)(Y_1 + Y_2)$
      \end{enumerate}
      \item Compute 5 parts of the resultant product with addition of the previously computed $p$s'
      \begin{enumerate}
        \item $Z_4 \leftarrow p_2$
        \item $Z_3 \leftarrow p_5 - p_1 - p_2$
        \item $Z_2 \leftarrow p_4 - p_0 - p_2 + p_1$
        \item $Z_1 \leftarrow p_3 - p_0 - p_1$
        \item $Z_0 \leftarrow p_0$
      \end{enumerate}
      \item Compute and return the resultant product by shifting appropriate number of bits on each $Z$ specified as coefficient and add them together, i.e.
      \[
        x\cdot y = 2^{\rfrac{4n}{3}}Z_4 + 2^{n}Z_3 + 2^{\rfrac{2n}{3}}Z_2 + 2^{\rfrac{n}{3}}Z_1 + Z_0
      \]
    \end{enumerate}

    Now we prove that the algorithm is correct, given
    \[
      x = 2^{\rfrac{2n}{3}}X_{2} + 2^{\rfrac{n}{3}}X_{1} + X_{0}
    \]
    \[
      y= 2^{\rfrac{2n}{3}}Y_{2} + 2^{\rfrac{n}{3}}Y_{1} + Y_{0}
    \]
    the product can be written as
    \[
      x\cdot y = 2^{\rfrac{4n}{3}}X_2 Y_2 + 2^{n}(X_2 Y_1 + X_1 Y_2) + 2^{\rfrac{2n}{3}}(X_2 Y_0 + X_1 Y_1 + X_0 Y_2) + 2^{\rfrac{n}{3}}(X_1 Y_0 + X_0 Y_1) + X_0 Y_0
    \]
    Let $Q$ be the expression containing $X$ and $Y$ such that
    \begin{enumerate}
      \item $Q_4 = X_2 Y_2$
      \item $Q_3 = X_2 Y_1 + X_1 Y_2$
      \item $Q_2 = X_2 Y_0 + X_1 Y_1 + X_0 Y_2$
      \item $Q_1 = X_1 Y_0 + X_0 Y_1$
      \item $Q_0 = X_0 Y_0$
    \end{enumerate}
    For the algorithm to be correct, we prove that for all $0\leq i \leq 4$, $Q_i = Z_i$,
    \begin{enumerate}
      \item
      \[
        Q_4 = X_2 Y_2 = p_0 = Z_4
      \]
      \item
      \[
        Q_3 = X_2 Y_1 + X_1 Y_2 = (X_1 + X_2)(Y_1 + Y_2) - X_1 Y_1 - X_2 Y_2 = p_5 - p_1 - p_2 = Z_3
      \]
      \item
      \[
        Q_2 =  X_2 Y_0 + X_1 Y_1 + X_0 Y_2 = (X_0 + X_2)(Y_0 + Y_2) - X_0 Y_0 - X_2 Y_2 + X_1 Y_1 = p_4 - p_0 - p_2 + p_1 = Z_2
      \]
      \item
      \[
        Q_1 = X_1 Y_0 + X_0 Y_1 = (X_0 + X_1)(Y_0 + Y_1) - X_0 Y_0 - X_1 - Y_1 = p_3 - p_0 - p_1 = Z_1
      \]
      \item
      \[
        Q_0 = X_0 Y_0 = p_0 = Z_0
      \]
    \end{enumerate}
    Therefore, the algorithm is correct.

    Now we show that the worst-case running time of the algorithm is $\Theta(n^{\log_3 6})$. Let $T(n)$ be worst case running time given $x,y$ are $n$-bit integers.
    \begin{enumerate}
      \item Since shifting operation is $O(1)$, computation of $X$ and $Y$ is $O(n)$
      \item Note there are constant number (6 to be specific) of addition and multiplication operations in this section. Addition is an $O(\frac{n}{3})$ operation on $\rfrac{n}{3}$-bit integers. Multiplication operation has a worst case running time of $T(\frac{n}{3})$ for $\rfrac{n}{3}$-bit integers. Therefore, the worst case running time of this step is $6T(\frac{n}{3}) + O(\frac{n}{3})$
      \item Again, we do constant number of addition and deletion operations, each taking $O(\frac{n}{3})$. Therefore this step has worst case running time $O(\frac{n}{3})$
      \item We first shift $Z$ calculated previously $O(n)$ number of times, each has a worst case running time of $O(1)$. Then we add the shifted $n$-bit $Z$s a constant number (4 to be specific) times, each an $O(n)$ operation. Hence this step has a worst case running time of $O(n)$
    \end{enumerate}
    To conclude, we arrived at a recurrence relation
    \[
      T(n) = 6T(\frac{n}{3}) + O(n)
    \]
    Let $a = 6$, $b=3$, $c=1$. Since $\log_3 6 > 1$, by master theorem we have
    \[
      T(n) = \Theta(n^{\log_3 6})
    \]

  \end{solution}
  \item Is your algorithm faster or slower than the divide-and-conquer algorithm shown in class with a running time of $\Theta(n^{\lg 3})$?

  This three-way-split algorithm for multiplying integers is slower than the two-way-split method taught in class, as
  \[
    \lg_2 3 < \log_3 6
  \]
  and so,
  \[
    \Theta(n^{\lg 3}) < \Theta(n^{\log_3 6})
  \]
  hence slower.

\end{enumerate}

\subsection*{Problem 2}
Consider a variant on the problem of Interval Scheduling where instead of wanting to schedule as many jobs as we can on one processor, we now want to schedule ALL of the jobs on as few processors as possible.\\
The input to the problem is $(s_1,f_1), (s_2,f_2), ···, (s_n,f_n)$, where $n \geq 1$ and all $s_i < f_i$ are nonnegative integers. The integers $s_i$ and $f_i$ represent the start and finish times, respectively, of job $i$.\\
A schedule specifies for each job $i$ a positive integer $P(i)$ (the “processor number” for job $i$). It must be the case that if $i \neq j$ and $P(i) = P(j)$, then jobs $i$ and $j$ do not overlap. We wish to find a schedule that uses as few processors as possible, i.e., such that $max\{P(1), P(2), \cdots , P(n)\}$ is minimal.\\


\begin{enumerate}
  \item Design an algorithm to solve the problem in time $O(n^2)$, i.e. strictly less than $O(n^2)$\\
  \begin{enumerate}
    \item Let $P$ be an empty array of size $n$ , representing $P(i)$ at index $i$
    \item Sort $(s_i, f_i)$ by start time such that for all $i\leq j$, $s_i \leq s_j$
    \item Starting from the start of the sorted job array $J$ and for each job $j_i$ do
    \begin{enumerate}
      \item Starting from processor number $k = 1$
      \item Define $J_k\subseteq J$ such that for all $j\in J_k$, $P(j) = k$. If $j_i$ is compatible with all $j\in J_k$, then assign $j_i$ processor number $k$, i.e. let $P(i) \leftarrow k$
      \item Otherwise, increment $k$ and try the previous step again until $j_i$ is assigned to either a previously used processor number or a new processor number not used before.
    \end{enumerate}
    \item Return $P$
  \end{enumerate}

  \item Prove that the above algorithm is guaranteed to compute a schedule that uses the minimum number of processors.\\

  We will prove that the greedy choice is always in some optimal solution to the problem. Then we prove that the problem exhibits optimal substructure. Here we define a \textit{compatible} processor number $k$ for job $j$ be an integer such that all jobs previously assigned $k$ are compatible with $j$. Let $J$ be the input jobs given. Let $J_t := \{ j_i \in J: s_i \geq s_t\}$ be subset of $J$ such that all jobs in $J_t$ starts after $j_t$ starts. Let $Max(P)$ be the maximum of processor numbers in $P$


  \begin{proposition*}
    Consider any subproblem $J_t$, let $j_i \in J_t$ be the job with earliest starting time, and let be $k$ be the lowest compatible processor number with $j_i$. Then assigning $k$ to $j_i$ is in some optimal ($max\{ P(1), \cdots, P(n) )\}$ minimized) solution to $J_t$
   \end{proposition*}
  \begin{proof}
    Assume $P'$ is an arbitrary optimal solution to $J_t$. Let $k' = P'(i)$. If $k = k'$, then we are done the proof since $k$ is assigned to $j_i$ by the greedy choice, which is in the optimal solution $P'$. Otherwise if $k \neq k'$, since $k$ is the lowest processor number possible (i.e. $k \leq k'$ ), then $k < k'$. Now we can construct a solution $P = P'$ where $P(i) = k$, thus $P(i) < P'(i)$. We arrive at a contradiction on the assumption that $P'$ is optimal. Hence we conclude that the greedy choice is always in some optimal solution to $J_t$
  \end{proof}

  \begin{proposition*}
    The scheduling problem exhibits optimal substructure.
  \end{proposition*}
  \begin{proof}
    Given arbitrary index $i$, we separate the problem into a greedy choice and a single subproblem, i.e. $\{ j_i\}$ and $J_{after} = J_i$. We make the choice assigning a processor number $k$ to $j_i$, Assume such assignment is in some optimal solution to the problem $P'$. Now we are left with assigning processor number to $J_{after}$ with $P_{after}$. Then the optimal solution follows
    \[
      Max(P') = Max\{k, Max(P_{after}) \}
    \]
    We claim that if $P'$ is optimal, then $P_{after}$ is also optimal, in a sense that if $Max(P_{after}) > k$, then $Max(P_{after})$ is minimized. If $Max(P_{after}) \leq k$, then solution is already optimal. Otherwise if $Max(P_{after})>k$, then suppose we can find a more optimal solution $P_{after}''$ such that $Max(P_{after}'') < Max(P_{after})$ then we can substitute $P_{after}''$ for $P_{after}$ and construct another solution set $P''$ with
    \[
      Max(P'') = Max\{k, Max(P_{after}'')\} < Max\{k, Max(P_{after})\} = Max(P')
    \]
    Hence contradicting the optimality assumption for $P'$, hence $P_{after}$ must be optimal in itself.
  \end{proof}

  We conclude by combining propositions proved earlier. By optimal substructure of the problem, given that at each step the greedy choice is optimum and we are left with finding optimal solution to a smaller subproblem, i.e. $J_{after}$, the solution to the original solution is optimal, specifically, the algorithm uses minimum number of processors.

  \item Briefly describe an efficient implementation of the algorithm, making it clear what data structures you are using. Express the running time of your implementation as a function of $n$ (the number of jobs), using appropriate asymptotic notation.\\

  We will use a min heap $H$ to store an array of finish time of currently scheduled jobs. $Q.size$ is size of the heap and assume is updated during insertion and deletion.

    \begin{algorithm}[H]
       \SetKwFunction{sa}{Schedule-All}
       \SetKwFunction{hmax}{Heap-Maximum}
       \SetKwFunction{extrmax}{Heap-Extract-Max}
       \SetKwFunction{hins}{Heap-Insert}
       \Fn{\sa$(s, f)$}{
          \KwIn{$s,f$ are arrays of size $n$, representing job $j_i = (s_i, f_i)$ at index $i$}
          \KwOut{$P$ is an array of size $n$ storing $P(i)$ at index $i$, where $max\{ P(1), \cdots, P(n) \}$ is minimized }

          $P \leftarrow$ Array of size $n$\\
          $H \leftarrow$ Min-Heap\\
          Sort $s, f$ by start time together such that $s_i \leq s_j$ for all $i\leq j$\\
          \For{$i = 1$ \KwTo $n$}{
            \While{\hmax$(H) < s_i$}{
              \extrmax$(H)$
            }
            \hins$(H, f_i)$\\
            $P(i) \leftarrow H.size$
          }
          \Return{P}
       }
     \end{algorithm}

     At line 6-7, we remove jobs' finish time from the heap $H$ such that the heap retains previously scheduled jobs that overlaps $j_i$. The size of the heap represent the smallest compatible processor number, which we record in solution $P(i)$ at each iteration afer inserting the finish time of $j_i$ to the heap. \\

     Now we analyze running time
     \begin{enumerate}
       \item Sorting takes $O(n\lg n)$
       \item By the time the procedure terminates, each jobs' finish time is inserted and removed from the heap, since  \textsc{Heap-Extract-Max} and \textsc{Heap-Insert} has worst case running time of $O(\lg n)$. Heap insertion and deletion has worst case running time of $O(2n\lg n) = O(n\lg n)$
       \item \textsc{Heap-Maximum} is called at least once per iteration of for loop for a total of $n$ iterations; and it is called at most $n$ number of times for each successful condition evaluation and subsequent deletion operation (since at most deleting a total of $n$ items). \textsc{Heap-Maximum} has worst case running time of $O(1)$ hence by the time procedure terminates, heap lookup operation has a worst case running time of $O(2n) = O(n)$
       \item Assigning $P$ at index $i$ takes $O(1)$ each iteration and since there are $n$ iterations, has a worst case running time of $O(n)$
       \item To conclude, the algorithm has a worst case running time of $O(n\lg n)$
     \end{enumerate}

\end{enumerate}


\subsection*{Problem 3}

Here is another variant on the problem of Interval Scheduling. Suppose we now have two processors, and we want to schedule as many jobs as we can. As before, the input is $(s_1,f_1), (s_2,f_2), ···, (s_n,f_n)$, where $n \geq 1$ and all $s_i < f_i$ are nonnegative integers. A \textit{schedule} is now defined as a pair of sets $(A_1,A_2)$, the intuition being that $A_i$ is the set of jobs scheduled on processor $i$. A schedule must satisfy the obvious constraints: $A_1 \subseteq \{1,2,\cdots,n\}$, $A_2\subseteq \{1, 2,\cdots , n\}$, $A_1 \cap A_2 = \emptyset$, and for all $i\neq j$ such that $i , j \in A_i$ or $i , j \in A_2$ , jobs $i$ and $j$ do not overlap.

\begin{enumerate}
  \item Design an algorithm (write a pseudocode) to solve the above problem in time $O(n^2)$, i.e., strictly less than $O(n^2)$.\\

  Let $J: \{ 1, 2, \cdots, n\}$ be the input set of jobs given. Here we define that a job $j\in J$, having start time $s$, is \textit{compatible} with $J_s\subseteq J$ if $j$ starts after every job in $J_s$ finishes, in other words,
  \[
    \forall j \in J_s: f_j \leq s
  \]
  Let subproblem $J_t$ be a set of jobs such that
  \[
    \forall j \in J_t: f_t \leq s_j
  \]
  in otherwords, $J_t$ is the set of jobs that starts after job $t$ ends. \\
  We define \textit{waste time} $W_i$ for a job $j$, having start time $s$, with respect to a compatible set of jobs $J_s$ as
  \[
    W_i = s - \underset{j\in J_s}{Max}\{ f_j \}
  \]
  in other words, the waste time is the time period between the finish time of the last finishing jobs in $J_s$ and the start time of the job $j$ in consideration

  \begin{enumerate}
    \item Let $(A_1, A_2) = (\O, \O)$
    \item Sort $(s_i, f_i)$ by finish time such that for all $i\leq j$, $f_i \leq f_j$
    \item Starting from the start of the sorted job array and for each job $j \in J$ do
    \begin{enumerate}
      \item Test if $j$ is compatible with $A_1$ and $A_2$.
      \item If $j$ is not compatible with either set, continue to next iteration
      \item If $j$ is compatible with only one of $A_1$ and $A_2$, then add $j$ to the compatible set
      \item If $j$ is compatible with both $A_1$ and $A_2$, then add $j$ to $A_i$ such that waste time for job $j$ with respect to $A_i$, $W_i$, is minimized.
    \end{enumerate}
    \item Return $(A_1, A_2)$
  \end{enumerate}


  \item Prove that the above algorithm is guaranteed to compute an optimal schedule.

  \begin{proposition*}
    Consider any subproblem $J_t$, let $j_e\in J_t$ be the job with earliest finish time with waste time $W_1$ and $W_2$. Then making the choice of assigning $j_e$ described above is in some optimal (i.e. $|A_1| + |A_2|$ maximized) solution to the problem.
  \end{proposition*}

  \begin{proof}
    Let $(O_1, O_2)$ be some optimal solution to the original problem $J$. Let $(A_1 \subseteq O_1, A_2\subseteq O_2)$ be the optimal solution to the subproblem $J_t$ with respect to $(O_1, O_2)$. and let $(B_1 \subseteq O_1, B_2\subseteq O_2)$ be the optimal solution to subproblem $J\setminus J_t$. Let $j_e\in J_t$ be job with earliest finish time. By definition of $J_t$, $j_e$ is compatible with at least one of $B_i$.
    \begin{enumerate}
      \item Suppose $j_e$ is compatible with exactly one of $B_i$, without loss of generality, suppose $B_1$ is the compatible set and $B_2$ is the in-compatible set. Let $j_a \in A_1$ be the first finishing job in $A_1$. Then we have,
      \begin{enumerate}
        \item If $j_e = j_a$, then the proposition holds
        \item If $j_e \neq j_a$. Since $j_e$ is not compatible with $B_2$, $j_e\not\in A_2$. Then consider a new solution set $A_1' = A_1 \cup \{ j_e \} \setminus \{ j_a\}$. Note $A_2$ is unchanged. jobs in $A_1'$ are disjoint because $A_1$ is disjoint, $j_a\in A_1$ is the first job to finish and $f_{j_e} \leq f_{j_a}$. Since $|A_1'| + |A_2| = |A_1| + |A_2|$, $(A_1', A_2)$ is an optimal solution to subproblem $J_t$ that contains $j_e$, hence the proposition holds.
      \end{enumerate}
      Similar argument holds if $B_2$ is the compatible set
      \item Now consider the case where $j_e$ is compatible with both $B_1$ and $B_2$. Without loss of generality, suppose $W_1 < W_2$, hence the greedy algorithm assigns $j_e$ to $B_1$. Let $j_a\in A_1$ be the earlist finishing job in $A_1$; let $j_b\in A_2$ be the earlist finishing job in $A_2$,
      \begin{enumerate}
        \item If $j_e = j_a$, then the proposition holds
        \item If $j_e \neq j_a$, Since $j_e$ is compatible with $B_2$ as well as $B_1$ there are two cases as to where $j_e$ might end up
        \begin{enumerate}
          \item If $j_e = j_b$, then consider a new solution set where $A_1' = A_2$ and $A_2' = A_1$, i.e. switching the set of jobs for processor 1 and 2. Note $A_1'$ and $A_2'$ are disjoint sets of jobs since $A_1$ and $A_2$ are disjoint sets. and $|A_1'| + |A_2'| = |A_1| + |A_2|$. Since $(A_1, A_2)$ optimal, then $(A_1', A_2')$ are optimal solutions and that $A_1'$ now contains $j_e$, because $j_e = j_b \in A_2 = A_1'$. The proposition hence holds.
          \item If $j_e$ is not in either $A_1$ or $A_2$, then consider a new solution $A_1' = A_1 \cup \{ j_e\} \setminus \{ j_a \}$. Proposition holds with same argument provided in $(a). ii.$
        \end{enumerate}
      \end{enumerate}
      Similar argument holds if $W_1 > W_2$.
    \end{enumerate}


    \begin{proposition*}
      This scheduling problem exhibits optimal substructure.
    \end{proposition*}
    \begin{proof}
      Given arbitrary index $t$, we separate the problem into a greedy choice and a single subproblem, i.e. $\{ j_e \}$ and $J_t$. Let $(A_1, A_2)$ be the optimal solution for $J_t$. We make the greedy choice of adding $j_e$ to $A_i$ or skipping $j_e$ (i.e. $j_e = \O$) to maximize time as a resource for subsequent jobs, and to minimize waste time as a resource if there is a choice to select one assuming both processors are available at the time. Assume such greedy choice is optimal, we are left with processing a smaller subproblem $J_{after} = J_t \setminus \{ j_e \}$. We claim that if $(A_1, A_2)$ is optimal, then solution to subproblem $J_{after}$, $(C_1, C_2)$ must also be optimal. Consider an alternative solution $(C_1', C_2')$ that is even more optimal, i.e. $|C_1'| + |C_2'| \geq |C_1| + |C_2|$. We can construct a new solution by replacing $(C_1, C_2)$ with $(C_1, C_2)$ and get an overall more optimal solution $(A_1' = \{ j_e \} \cup C_1', A_2' = \{ j_e \}\cup C_2')$ such that
      \[
        |A_1'| + |A_2'| > |A_1| + |A_2|
      \]
      Contradicts assumption that $(A_1, A_2)$ is optimal. Hence solution to subproblem $J_{after}$ must be optimal
    \end{proof}

    We conclude by combining propositions proved earlier. By optimal substructure of the problem, given that at each step the greedy choice is optimum and we are left with finding optimal solution to a smaller subproblem, i.e. $J_{after}$, the solution to the original solution is optimal, specifically, the algorithm schedules most jobs on two processors.
  \end{proof}

  \item Briefly describe an efficient implementation of the algorithm, making it clear what data structures you are using. Express the running time of your implementation as a function of $n$ (the number of jobs), using appropriate asymptotic notation.

  Let $A_1$ and $A_2$ be two linked list. Job $j$ may be appended to the tail of $A_1$ or $A_2$ in constant time $O(1)$. Assume that the last job added to $A_i$ can be efficiently looked up in $O(1)$ time with $A_1.tail$ operation.


  \begin{algorithm}[H]
     \SetKwFunction{st}{Schedule-On-Two-CPU}
     \SetKwFunction{ll}{Linked-List}
     \SetKwFunction{cp}{Compatible}

     \SetKwFunction{both}{Both}
     \SetKwFunction{none}{None}
     \SetKwFunction{one}{diff-one}
     \SetKwFunction{two}{diff-two}

     \SetKwFunction{test}{T}


     \Fn{\cp$(j, A_1, A_2, s, f)$}{
      \KwOut{Returns \none if $j$ not compatible with $A_1$ or $A_2$, Return \both if $j$ is compatible with both $A_1$ and $A_2$, and return the processor number, either 1 or 2, if $j$ is compatible with only $A_1$ or $A_2$. Each return statement also include the computed waste time $W_1$ and $W_2$}

      $\one = s[j] - f[A1.tail]$ \\
      $\two = s[j] - f[A2.tail]$ \\

      \If{$\one > 0$ and $\two > 0$}{
        \Return{$(\both, \one, \two)$}
      }\ElseIf{$\one \leq 0$ and $\two\leq 0$}{
        \Return{$(\none, \one, \two)$}
      }\Else{
        \If{$\one \geq 0$}{
          \Return{$(1, \one, \two)$}
        }\Else{
          \Return{$(2, \one, \two)$}
        }
      }
     }


     \Fn{\st$(s, f)$}{
        \KwIn{$s,f$ are arrays of size $n$, representing job $j_i = (s_i, f_i)$ at index $i$}
        \KwOut{$(A_1, A_2)$ is a set of solution to the problem given}

        $A_1,A_2 \leftarrow$ \ll\\
        $A_1.append(1)$ \quad \quad \quad \tcp{Add first finishing job arbitrarily to $A_1$}\\
        \For{$j=2$ \KwTo $n$}{
          $(\test, W_1, W_2) = \cp(j, A_1, A_2, s, f)$ \\
          \If{\test is \both}{
            \If{$W_1 < W_2$}{
              $A_1.append(j)$
            }\Else{
              $A_2.append(j)$
            }
          }
          \ElseIf{\test is 1}{
            $A_1.append(j)$
          }\ElseIf{\test is 2}{
            $A_2.append(j)$
          }
        }
        \Return{$(A_1, A_2)$}
     }
   \end{algorithm}

   Now we analyze running time. There are $O(n)$ iterations, and in each iteration, at most one $append$ operation and $tail$ operation, each $O(1)$, is required for Linked List operation. There is also some $O(1)$ array lookup in $s$ and $f$. Hence the algorithm has a worst case running time of $O(n)$



\end{enumerate}




\subsection*{Problem 4}
Consider the problem of making change, given a finite number of coins with various values. Formally:
\begin{enumerate}
  \item \textbf{Input} A list of positive integer coin values $c_1, c_2, \cdots, c_m$ (with repeated values allowed) and a positive integer amount $A$.
  \item \textbf{Output}  A selection of coins $\{i_1,i_2,\cdots,i_k\}\subseteq \{1,2,\cdots,m\}$ such that $c_{i_1} +c_{i_2} +\cdots+c_{i_k} = A$ and $k$ is as small as possible. If it is impossible to make change for amount $A$ exactly, then the output should be the empty set $\O$.
\end{enumerate}


\begin{enumerate}
  \item Describe a natural greedy strategy you could use to try and solve this problem, and show that your strategy does not work (The point of this question is not to try and come up with a really clever greedy strategy — rather, we simply want you to show why the “obvious” strategy fails to work.)

  \begin{solution}
    Let $C = \{ c_1, \cdots, c_m \}$ be the given list of coin values. At each step, pick $c_{j} \in C$ such that
    \[
      c_{j} = \sup\{ c\in C: c\leq A \}
    \]
    and continue to solve a smaller subproblem, on a smaller list $C' = C \setminus \{ c_{j} \}$ and with a smaller amount $A' = A - c_{j}$. The greedy algorithm is incorrect because it does not exhibit optimal substructure. Subsequent solution to the subproblems is dependent upon the greedy choice, specifically, to satisfy the constraint that all values add up to $A$. As an example, Given $G = \{ 2, 2, 3 \}$ and $A = 4$. $\{ 1, 2\}$ is the optimal solution since $2+2=4$. However, with the greedy approach described above, $\O$ is outputed because $3$ is picked first and no $1 = 4-3$ coin value is in $C$.
  \end{solution}

  \item Give a detailed dynamic programming algorithm to solve this problem. Follow the steps outlined in class, and include a brief (but convincing) argument that your algorithm is correct.


  \begin{enumerate}
    \item \textbf{Optimal Substructure} Let $C_j: \{ c_{i}\in C: i \leq j \}$. Let $O_j =\{ i_1, i_2, \cdots, i_k\}$ be an optimal solution to the subproblem given $C_j$ ($j\leq m$) and $a
    \leq A$. Let $k(j, a)$ be optimal value corresponding to $O_j$, i.e. the smallest number of coin values such that
    \[
      c_{i_1} + \cdots + c_{i_k} = a \quad \text{ where } \quad 1\leq k \leq j
    \]
    We are given the choice of including $c_i$ into the optimal solution or not,
    \begin{enumerate}
      \item If $c_i \not\in O_j$, then we consider a smaller subproblem with a reduced set of coin values
      \[
        k(j, a) = k(j-1, a)
      \]
      \item If $c_i \in O_j$, then we consider a smaller subproblem with a reduced set of coin values as well as reduced total coin amount $A - c_i$
      \[
        k(j, a) = 1 + k(j-1, a-c_j)
      \]
      \item Overall, therefore
      \[
        k(j, a) = Min\{ k(j-1, a),1 + k(j-1, a-c_j)\}
      \]
    \end{enumerate}

    \item \textbf{Define array to store computed value} Now we consider storing previously computed value in $M[0\cdots m, 0\cdots A]$, such that $M[j, a]$ holds the optimal value $k(j, a)$ specified above.

    \item \textbf{Redefine recurrence relation in terms of array} Now we define $M[j, a]$ recursively,
    \[
      M[j, a] =
      \begin{cases}
        0 & \text{If } j = 0 \\
        M[j-1, a] & \text{If } c_j > a\\
        Min\{ M[j-1, a],1 + M[j-1, a-c_j]\} & \text{If } c_j \leq a\\
      \end{cases}
    \]
    \item \textbf{Bottom-Up Approach}
    $ $\\
    \begin{algorithm}[H]
       \SetKwFunction{cv}{Find-Smallest-Coin-Number}

       \Function{\cv$(C, A)$}{\\
          \KwIn{$C$ is a list of coin values provided}
          $M \leftarrow [0\cdots m, 0\cdots A]$ \\

          \For{$j = 0$ \KwTo $m$}{
            \For{$a = 0$ \KwTo $A$}{
              $M[j, a] = 0$\\
            }
          }

          \For{$j = 1$ \KwTo $m$}{
            \For{$a = 1$ \KwTo $A$}{
              \If{$C[j] > a$}{
                $M[j, a] = M[j-1, a]$\\
              }
              \Else{
                $M[j, a] = Min\{ M[j-1, a],1 + M[j-1, a-C[j]]\} $\\
              }
            }
          }
        }
     \end{algorithm}
     \item \textbf{Actual Solution}
     Let $L$ be a linked list holding the output list. We define $S[1\cdots m, 1\cdots A]$ be an array such that $S[j, a]$ holds a boolean value $true$ if $c_j$ is in the optimal solution and $false$ otherwise. And to find the selection of coins, we iterate over $S$ starting from $S[m, A]$, and selectively append $j$ to linked list $L$, depending on value of $S[j, a]$

     \begin{algorithm}[H]
        \SetKwFunction{cv}{Find-Smallest-Coin-Number}
        \SetKwFunction{ll}{Linked-List}


        \Function{\cv$(C, A)$}{\\
           \KwIn{$C$ is a list of coin values provided}
           $M\leftarrow [0\cdots m, 0\cdots A]$ \\
           $S \leftarrow [1\cdots m, 1\cdots A]$ \\
           $L \leftarrow \ll$\\

           \For{$j = 0$ \KwTo $m$}{
             \For{$a = 0$ \KwTo $A$}{
               $M[j, a] = 0$\\
             }
           }


           \For{$j = 1$ \KwTo $m$}{
             \For{$a = 1$ \KwTo $A$}{
               $S[j, a] = false$\\
               \If{$C[j] > a$}{
                 $M[j, a] = M[j-1, a]$\\
               }
               \Else{
                 $M[j, a] = Min\{ M[j-1, a],1 + M[j-1, a-C[j]]\} $\\
                 \If{$M[j-1, a] > 1 + M[j-1, a-C[j]]$}{
                    $S[j, a] = true$
                 }
               }
             }
           }

           \If{$M[m, A]$ is 0}{
            \Return{$\O$}
           }

           $amount \leftarrow A$\\
           \For{$j = m$ \KwTo $1$}{
              \If{$S[j, amount]$ is $true$}{
                $L.prepend(j)$\\
                $amount = amount - C[j]$\\
              }
           }
           \Return{$L$}\\
         }
      \end{algorithm}
  \item \textbf{Proof of correctness for algorithm}
  \begin{proposition*}
    The dynamic programming algorithm specified above yields the optimal solution specified by the problem.
  \end{proposition*}
  \begin{proof}
    At each step $i$, the algorithm considers $C_i$, i.e. the set of coin values with index smaller or equal to $i$. For all $1\leq a \leq A$, assume $O_{i, a}$ are optimal solutions to $C_i$. The algorithm decides to keep coin $c_j$ based on two conditions. First, if the $i+1$th coin value is less than the amount specified, this is to make sure that inclusion of $i+1$ does not exceed the constraining amount $a$ instantly. And second, if the inclusion of $i+1$th coin together with the optimal solution to $C_1$ with amount reduced by exactly the coin value of $i+1$th coin, i.e. $C[j+1]$, yields a smaller list. Otherwise, the algorithm decides to skip the $i+1$-th coin value and continue until the entire coin value list is exhausted. To summarize, at each step, we are only adding $c_j$ if we arrive at a strictly smaller solution set such that the summation of coin values in the set equates to the constraining amount $A$. And since we start with specifying the size $m$ of the list and constraint $A$, the algorithm yields optimal result.
  \end{proof}

  \end{enumerate}
  \item What is the worst-case running time of your algorithm? Justify briefly.

  \begin{enumerate}
    \item At line 4, initialization of $M$ requires a constant $O(A)$
    \item The nested for loop at line 8,9 iterates $O(Am)$ times, with each doing a constant $O(1)$ operation, specifically array assignment and look up. Note this is possible because we are only accessing value at index that is previously populated, i.e. $M[j-1,]$. Hence has a worst-time running time of $O(Am)$
    \item The next for loop at line 17, iterates $O(A)$ times, doing array access, and possiblly one linked-list prepend operation, which are constant $O(1)$ operations. Hence has a worst-time running time of $O(A)$
    \item In summary, the algorithm has worst-case running time of $O(Am)$
  \end{enumerate}

\end{enumerate}


\subsection*{Problem 5}
During the renovations at Union Station, the work crews excavating under Front Street found veins of pure gold ore running through the rock! They cannot dig up the entire area just to extract all the gold: in addition to the disruption, it would be too expensive. Instead, they have a special drill that they can use to carve a single path into the rock and extract all the gold found on that path. Each crew member gets to use the drill once and keep the gold extracted during their use. You have the good luck of having an uncle who is part of this crew. What’s more, your uncle knows that you are studying computer science and has asked for your help, in exchange for a share of his gold! \\
The drill works as follows: starting from any point on the surface, the drill processes a block of rock $10cm \times 10cm \times 10cm$, then moves on to another block $10cm$ below the surface and connected with the starting block either directly or by a face, edge, or corner, and so on, moving down by $10cm$ at each “step”. The drill has two limitations: it has a maximum depth it can reach and an initial hardness that gets used up as it works, depending on the hardness of the rock being processed; once the drill is all used up, it is done even if it has not reached its maximum depth. \\
The good news is that you have lots of information to help you choose a path for drilling: a detailed geological survey showing the hardness and estimated amount of gold for each $10cm \times 10cm \times 10cm$ block of rock in the area. To simplify the notation, in this homework, you will solve a two-dimensional version of the problem defined as follows.
\begin{itemize}
  \item \textbf{Input} A positive integer $d$ (the initial drill hardness) and two $[m \times n]$ matrices $H$, $G$ containing non-negative integers. For all $i \in \{1, \cdots,m\}$, $j\in \{1,\cdots,n\}$, $H[i,j]$ is the hardness and $G[i,j]$ is the gold content of the block of rock at location $i,j$ (with $i = 1$ corresponding to the surface and $i = m$ corresponding to the maximum depth of the drill). There is one constraint on the values of each matrix: $H[i,j] = 0 \implies G[i,j] = 0$ (blocks with hardness 0 represent blocks that have been drilled already and contain no more gold).
  \item \textbf{Output} A drilling path $j_1,j_2,\cdots,j_l$ for some $l \leq m$ such that:
  \begin{enumerate}
    \item $1\leq j_k \leq n$ for $k=1,2, \cdots, l$  (horizontal coordinate is valid)
    \item $j_{k-1} - 1\leq j_k \leq j_{k-1} + 1$ for $k = 2,\cdots, l$ (each block is underneath the one just above, either directly or diagonally, always going down)
    \item $H[1,j_1]+H[2,j_2]+\cdots+H[l,j_l] \leq d$ (the total hardness of all the blocks on the path is no more than the initial drill hardness)
    \item $G[1,j_1]+G[2,j_2]+\cdots+G[l,j_l]$ is maximum (the path collects the maximum amount of gold possible)
  \end{enumerate}
\end{itemize}



\begin{solution}
  \begin{enumerate}
    \item \textbf{optimal substructure} Let $O_n = \{ j_1,\cdots, j_l\}$ be the optimal solution to the problem given. Let $OPT(n, d)$ be the maximum amount of gold to the optimal solution under the hardness limit $d$, i.e.
    \[
      OPT(n, d) := \sum_{k=1}^{l} G[k, O_n[k]] \quad \text{such that}\quad \sum_{k=1}^{l} H[k, O_n[k]] \leq d
    \]
    For every path $j$ possible, either $j\in O_n$ or $j\not\in O_n$
    \begin{enumerate}
      \item If $j_k \not\in O_n$, then we consider a smaller subproblem with the same hardness limit $d$. Since the drill moves one unit down and $v = \{ -1, 0, 1\}$ unit sideways, the possible path is therefore $[k-1, j_k + v]$. The optimal value is therefore given by the maximum optimal value of the subproblems
      \[
        OPT(k, j_k, d) = \underset{v\in\{-1,0,1\}}{Max}\{ OPT[k-1, j_k + v, d]\}
      \]
      \item If $j_k \in O_n$, then we consider a smaller subproblem with a reduced hardness limit $d - H[k, j_k]$ since we have added $j_k$ to the optimal solution. The optimal value for $j_k$ is therefore given by the maximum optimal value of the subproblems with reduced hardness limit in addition to the amount of gold contributed by drilling $j_k$
      \[
        OPT(k, j_k, d) = \underset{w\in\{-1,0,1\}}{Max}\{ G[k, j_k] + OPT[k-1, j_k + w, d-H[k, j_k]]\}
      \]
      \item Therefore,
      \[
        OPT(k, j_k, d) = \underset{v, w\in\{-1,0,1\}}{Max}\{ OPT[k-1, j_k + v, d], G[k, j_k] + OPT[k-1, j_k + w, d-H[k, j_k]]\}
      \]
    \end{enumerate}

  \item \textbf{Define array to store computed values} Now we consider storing previously computed values in an array $M[0\cdots m, 0\cdots n + 1, d]$, where $M[i, j, d]$ ($i\in 1\cdots m$, $j\in 1\cdots n$, $w\in 1\cdots d$) holds the optimal value for all path $\{ j_1, \cdots, j_k\}$, where $k = i$, $j_k = j$, and any hardness up to $d$. in other words, the largest amount of gold under hardness restriction at $[i, j]$ via any reachable path from surface with given $d$, and 0 otherwise. Note that $M[i, 0, d]$ and $M[i, n+1, d]$ are outside of the matrix $[m\times n]$ and are initialized to zero such that computing $M$ for for blocks at the left and right boundary will not experience out of bound error.
  \item \textbf{Redefine recurrence relation in terms of array} Now we can re-define $M[i, j, d]$
  \[
    M[i, j, d] =
    \begin{cases}
      0 & \text{if } i = 0\\
      0 & \text{if } H[i, j] > d \\
      \underset{v, w\in\{-1,0,1\}}{Max}\{ M[i-1, j + v, d], G[i, j] + M[i-1, j + w, d-H[i, j]]\} & \text{if } H[i, j] \leq d
    \end{cases}
  \]
  \item \textbf{Bottom-Up Approach}

  \begin{algorithm}[H]
     \SetKwFunction{dg}{Drill-Gold}

     \Function{\dg$(d, H, G)$}{\\
        $M \leftarrow [0\cdots m, 0\cdots n + 1, 0\cdots d]$ \\
        \For{$i = 1$ \KwTo $m$}{
          \For{$j = 0$ \KwTo $n + 1$}{
            \For{$w = 0$ \KwTo $d$}{
              $M[i, j, w] \leftarrow 0$
            }
          }
        }

        \For{$i = 1$ \KwTo $m$}{
          \For{$j = 1$ \KwTo $n$}{
            \For{$w = 1$ \KwTo $d$}{
              \If{$w \geq H[i, j]$}{
                $M[i, j, w] = \underset{a,b\in\{-1,0,1\}}{Max}\{ M[i-1, j + a, d], G[i, j] + M[i-1, j + b, d-H[i, j]]\}$
              }
            }
          }
        }
        \Return{$M$}
      }
   \end{algorithm}
   The worst case running time is $\Theta(mnd)$. The three nested loops run for $n, m, d$ iterations, with each iteration takes a constant time for random-access lookup in array $M, H$ and possibly $G$. This is possible because at any iteration, $M[i-1, j, d]$ for all $j=1\cdots n$, $w=1\cdots d$ is already computed in the previous iteration of the outer most loop.
   \item \textbf{Actual Solution}: Now we compute the actual solution to the problem, by keeping track of the path leading to current location. We define array $T[0\cdots m, 0\cdots n, 0\cdots d]]$ and let $T[i, j, d]$ be the corresponding horizontal translation (amongst $\{ -1, 0, 1 \}$) from the previous level yielding the largest $M[i, j, d]$, should $[i, j]$ be included in the solution. Once we processed the matrix and computed the corresponding $M$ and $T$ tables, we then find the optimal path. We first identify the block $block = [i, j]$ in the matrix such that $M[i, j, d]$ is maximized. We start from $block$ and move one level up each time, deciding the horizontal translation by referencing $T[i, j, d]$, and update $block$ to reference the previous block on the path, until we reached surface.


   \begin{algorithm}[H]
      \SetKwFunction{dg}{Drill-Gold}
      \SetKwFunction{nf}{Not-Found}
      \SetKwFunction{ll}{Linked-List}



      \Function{\dg$(d, H, G)$}{\\
         $M \leftarrow [0\cdots m, 0\cdots n + 1, 0\cdots d]$ \\
         $T \leftarrow [0\cdots m, 0\cdots n, 0\cdots d]$ \\
         \For{$i = 0$ \KwTo $m$}{
           \For{$j = 0$ \KwTo $n + 1$}{
             \For{$w = 0$ \KwTo $d$}{
               $M[i, j, w] \leftarrow 0$  \\
               $T[i, j, w] \leftarrow 0$   \\
             }
           }
         }

         \For{$i = 1$ \KwTo $m$}{
           \For{$j = 1$ \KwTo $n$}{
             \For{$w = 1$ \KwTo $d$}{

               \If{$w \geq H[i, j]$}{

                  $A = \underset{a\in\{-1,0,1\}}{Max}\{ M[i-1, j + a, d]]\}$ \\
                  $B_{-1} = G[i, j] + M[i-1, j - 1, d-H[i, j]]$ \\
                  $B_{0} = G[i, j] + M[i-1, j, d-H[i, j]]$ \\
                  $B_{1} = G[i, j] + M[i-1, j + 1, d-H[i, j]]$ \\

                  $M[i, j, w] = Max\{ A, B_{-1}, B_0, B_1\}$ \\

                  \If{$M[i, j, w] \neq A$}{
                    \If{$B_{-1} \geq B_0$ and $B_{-1} \geq B_1$}{
                      $T[i, j, w] = -1$\\
                    }
                    \If{$B_{0} \geq B_{-1}$ and $B_{0} \geq B_1$}{
                      $T[i, j, w] = 0$\\
                    }
                    \If{$B_{1} \geq B_{-1}$ and $B_{1} \geq B_0$}{
                      $T[i, j, w] = 1$\\
                    }
                  }


               }
             }
           }
         }

         $block \leftarrow \nf$\\
         $gold \leftarrow 0$\\
         $P \leftarrow \ll$\\

         \For{$i = 1$ \KwTo $m$}{
           \For{$j = 1$ \KwTo $n$}{
              \If{$M[i, j, d] > gold$}{
                $gold \leftarrow M[i, j, d]$\\
                $block.i\leftarrow i$\\
                $block.j\leftarrow j$\\
              }
           }
         }

         \While{$block$ not \nf and $block.i \neq 0$}{
            $P.prepend(block.j)$ \\
            $block.j = block.j + T[i, j, d]$\\
            $block.i = block.i - 1$\\
         }

         \Return{$P$}
       }
    \end{algorithm}


  \end{enumerate}
\end{solution}


\end{document}
