\documentclass[11pt]{article}
\input{/Users/markwang/.preamble}
\begin{document}

% arg1=pdfurl arg2=pagenum arg3=sectiontitle
\newcommand{\linksection}[3][../../linear_algebra_friedberg_insel_spence_4ed.pdf]{
    \subsection*{\href[page=#2]{#1}{#3}}
}

\newcommand{\linkinline}[3][../../linear_algebra_friedberg_insel_spence_4ed.pdf]{
    \noindent\href[page=#2]{#1}{#3}
}

\newcommand{\linksolution}[3][../../solution_compiled.pdf]{
    \noindent\href[page=#2]{#1}{#3} \\
}

\newcommand{\vecspace}{\mathcal{V}}
\newcommand{\field}{\mathcal{F}}
\newcommand{\trace}[1]{tr(#1)}
\renewcommand{\span}[1]{span(#1)}
\renewcommand{\dim}[1]{dim(#1)}
\newcommand{\nullity}[1]{nullity(#1)}
\newcommand{\rank}[1]{rank(#1)}
\newcommand{\cvec}[2]{\left[ #1 \right]_{#2}}
\renewcommand{\matr}[3]{\left[ #1 \right]_{#2}^{#3}}
\newcommand{\ltspace}[1]{\mathcal{L}(#1)}
\renewcommand{\det}[1]{det(#1)}
\newcommand{\tinvariant}[2]{\langle#2\rangle_{#1}}
\newcommand{\innerp}[2]{\langle#1,#2\rangle}
\renewcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\orthocomp}[1]{#1^{\perp}}

\linksection{341}{Chapter 6 Inner Product Spaces}
\linksection{341}{6.1 Inner Products and Norms}


\begin{defn*}
    \textbf{Inner Product} Let $V$ be a vector space over $F$. An inner product on $V$ is a function that assigns, to every ordered pair of vectors $x$ and $y$ in $V$, a scalar in $F$, denoted $\innerp{x}{y}$, such that for all $x,y,z\in VE$ and all $c\in F$, 
    \begin{enumerate}
        \item $\innerp{x+z}{y} = \innerp{x}{y} + \innerp{z}{y}$ 
        \item $\innerp{cx}{y} = c\innerp{x}{y}$
        \item $\overline{\innerp{x}{y}} = \innerp{y}{x}$ 
        \item $\innerp{x}{x} > 0$ if $x\neq 0$
    \end{enumerate}
    First two condition requires inner product be linear in the first component. Also 
    \[
        \innerp{\sum_i a_i v_i}{y} = \sum_i a_i \innerp{v_i}{y}    
    \]
\end{defn*}

\begin{defn*}
    \textbf{Conjugate Transpose or Adjoint of a Matrix} Let $A\in M_{m\times n}(F)$, the conjugate transpose or adjoint of $A$ is an $n\times m$ matrix $A^*$ such that $(A^*)_{ij} = \overline{A_{ji}}$ for all $i,j$. For $F=\R$, $A^* = A^T$
\end{defn*}



\begin{defn*}
    \textbf{Inner Product Definition Example}
    \begin{enumerate}
        \item \textbf{Standard Inner Product on $F^n$} For $x = (a_1,a_2,\cdots,a_n)$ and $y=(b_1,b_2,\cdots, b_n)$ in $F^n$, the standard inner product on $F^n$ is given by 
        \[
            \innerp{x}{y} = \sum_{i=1}^n a_i \overline{b}_i    
        \]
        \item Inner Product for \textbf{Real-valued Continuous Functions} on $[0,1]$. Let $V=C([0,1])$, $f,g\in V$, define 
        \[
            \innerp{f}{g} = \int_0^1 f(t) g(t) dt    
        \]
        \item \textbf{Frobenius Inner Product for Matrices} Let $V=M_{n\times n}(F)$, $A,B\in V$, then 
        \[
            \innerp{A}{B} = \trace{B^*A} = \sum_{i=1}^n (B^*A)_{ii}
        \]
    \end{enumerate}
\end{defn*}

\begin{defn*}
    \textbf{Inner Product Space} A vector space over $F$ endowed with a specific inner product is called an inner product space. If $F=C$, $V$ is a complex inner product space; if $F=\R$, then $V$ is a real inner product space
\end{defn*}

\begin{theorem*}
    \textbf{6.1 Properties From Inner Product Conditions} Let $V$ be an inner product space. Then for $x,y,z\in V$ and $c\in F$, the following statements are true 
    \begin{enumerate}
        \item $\innerp{x}{y+z} = \innerp{x}{y} + \innerp{x}{z}$
        \item $\innerp{x}{cy} = \overline{c} \innerp{x}{y}$
        \item $\innerp{x}{0} = \innerp{0}{x} = 0$
        \item $\innerp{x}{x} = 0$ if and only if $x=0$ 
        \item If $\innerp{x}{y} = \innerp{x}{z}$ for all $x\in V$, then $y=z$
    \end{enumerate}
    The inner product is conjugate linear in the second argument 
\end{theorem*}

\begin{defn*}
    \textbf{Norm/Length} Let $V$ be an inner product space. For $x\in V$, define norm or length of $x$ by 
    \[
        \norm{x} = \sqrt{\innerp{x}{x}}    
    \]
\end{defn*}

\begin{defn*}
    \textbf{6.2 Properties of Norm} Let $V$ be an inner product space over $F$. Then for all $x,y\in V$ and $c\in F$, the following statements are true 
    \begin{enumerate}
        \item $\norm{cx} = |c| \cdot \norm{x}$ 
        \item $\norm{x} = 0$ if and only if $x=0$. In any case, $\norm{x} \geq 0$ 
        \item \textbf{Cauchy-Schwarz Inequality} $|\innerp{x}{y}| \leq \norm{x} \cdot \norm{y}$
        \item \textbf{Triangular Inequality} $\norm{x+y} \leq \norm{x} + \norm{y}$
    \end{enumerate}
\end{defn*}

\begin{defn*}
    \textbf{Angle} For $F=\R$, $x,y \neq 0$, and $\theta$ be angle between $x$ and $y$
    \[
        \cos{\theta} = \frac{\innerp{x}{y}}{\norm{x}\norm{y}}
        \quad \quad \quad \quad 
        \theta = \cos^{-1}\left( \frac{\innerp{x}{y}}{\norm{x}\norm{y}} \right)
    \]
    Note
    \[
        \left|\frac{\innerp{x}{y}}{\norm{x}\norm{y}}\right| \leq 1
    \]
    So valid input to arccos function
\end{defn*}

\begin{defn*}
    \textbf{Orthogonal Vectors} Let $V$ be an inner product space. Vectors $x$ and $y$ in $V$ are orthogonal (perpendicular) if $\innerp{x}{y} = 0$.
\end{defn*}

\begin{defn*}
    \textbf{Orthogonal Sets and Orthonormal Sets} A subset $S$ of $V$ is orthogonal if any two distinct vectors in $S$ are orthogonal. A vector $x$ in $V$ is a unit vctor if $\norm{x} = 1$. A subset $S$ of $V$ is orthonormal if $S$ is orthogonal and consists entirely of unit vectors. 
    \begin{enumerate}
        \item $S = \{v_1,v_2,\cdots\}$, then $S$ is orthonormal if and only if $\innerp{v_i}{v_j} = \delta_{ij}$
        \item We can \textbf{normalize} an orthogonal set $S$, by multiplying $1/\norm{x}$ for each $x\in S$
    \end{enumerate}
\end{defn*}


\begin{defn*}
    \textbf{Orthonormal Set Property} Let $V$ be inner product space and \\ $S = \{s_1,s_2,\cdots\}\subseteq V$ be an orthonormal set. Let $v\in \span{S}$, then $v = a_1s_1 + \cdots + a_k s_k$. Then 
    \[
        \innerp{v}{s_j} = a_j    
    \]
    by
    \[
        \innerp{v}{s_j} = \innerp{\sum_i a_i s_i}{s_j} = \sum_i a_i \innerp{s_i}{s_j} = \sum_i a_i \delta_{ij} = a_j    
    \]
\end{defn*}


 
\linksection{353}{Gram-Schmidt Orthogonalization Process and Orthogonal Complements}

\begin{defn*}
    \textbf{Orthonormal Basis} Let $V$ be an inner product space. A subset of $V$ is an orthonormal basis for $V$ if it is an ordered basis that is orthonormal
\end{defn*}

\begin{defn*}
    \textbf{Every Inner Product Space has $n$ Orthogonal Basis} Let $V$ be an inner product space and $S = \{ v_1, v_2, \cdots, v_k\}$ be an orthogonal subset of $V$ consisting of nonzero vectors. If $y\in \span{S}$, then 
    \[
        y = \sum_{i=1}^k \frac{\innerp{y}{v_i}}{\norm{v_i}^2} v_i
    \]
\end{defn*}

\begin{corollary*}
    \textbf{Special case for Orthonormal Set} If, in addition to hypotheses of previous theorem, $S$ is orthonormal and $y\in \span{S}$, then 
    \[
        y = \sum_{i=1}^k \innerp{y}{v_i}v_i    
    \]
\end{corollary*}

\begin{corollary*}
    \textbf{Nonzero Orthonormal Set is Linearly Independent} Let $V$ be an inner product space, andf let $S$ be an orthogonal subset of $V$ consisting of nonzero vectors. Then $S$ is linearly independent
\end{corollary*}

\begin{theorem*}
    \textbf{6.4 Gram-Schmidt Process} Let $V$ be an inner product space and $S = \{w_1,w_2,\cdots, w_n\}$ be a linearly independent subset of $V$. Define $S' = \{v_1,v_2,\cdots, v_n\}$, where $v_1 = w_1$ and 
    \[
        v_k = w_k - \sum_{j=1}^{k-1} \frac{\innerp{w_k}{v_j}}{\norm{v_j}^2} v_j 
        \quad \quad 
        2\leq k\leq n
    \]
    Then $S'$ is an orthogonal set of nonzero vectors such that $\span{S'} = \span{S}$
\end{theorem*}



\begin{theorem*}
    \textbf{6.5 Every Finite Dimensional I.P.S has an Orthonormal Basis} Let $V$ be a nonzero finite-dimensional inner product space. Then $V$ has an orthonormal basis $\beta$. Furthermore, if $\beta = \{v_1, v_2,\cdots, v_n\}$ and $x\in V$, then 
    \[
        x = \sum_{x=1}^n \innerp{x}{v_i}v_i    
    \]
\end{theorem*}


\begin{corollary*}
    \textbf{Expression for Matrix Representation of Transformation on Orthonormal Basis} Let $V$ be a finite-dimensional inner product space with an orthonormal basis $\beta = \{v_1,v_2,\cdots, v_n\}$. Let $T$ be a linear operator on $V$, and let $A = \cvec{T}{\beta}$. Then for any $i$ and $j$, $A_{ij} = \innerp{T(v_j)}{v_i}$, i.e. 
    \[
        T(v_j) = \sum_{i=1}^n \innerp{T(v_j)}{v_i}  v_i  
    \]
\end{corollary*}


\begin{defn*}
    \textbf{Fourier Coefficients} Let $\beta$ be an orthonormal subset (possibly infinite) of an inner product space $V$, and let $x\in V$. We define the Fourier coefficients of $x$ relative to $\beta$ to be the scalars $\innerp{x}{y}$, where $y\in \beta$
\end{defn*}

\begin{defn*}
    \textbf{Example of infinite dimension vector space } Define inner product on vector space $H$ as
    \[
        \innerp{f}{g} = \frac{1}{2\pi}\int_{0}^{2\pi} f(t) \overline{g(t)} dt
    \]
    Let $f_n(t) = e^{int}$ where $0\leq t \leq 2\pi$ ($e^{int} = \cos nt + i\sin nt$). Let $S = \{ f_n : n \text{ is an integer} \}$. $S\subseteq H$. For $m\neq n$, we have $\innerp{f_m}{f_n} = \delta_{mn}$, therefore an orthonormal subset of $H$ \\
    \linkinline{360}{6.2 Example 7} showed that 
\end{defn*}

\begin{defn*}
    \textbf{Another example of infinite dimension vector space} \linkinline{23}{1.2 example 5} Define a vector space of sequences $\sigma(n) = \{ a_n\} = a_1, a_2,\cdots$ where $\sigma(n) \neq 0$ for finitely many elements. Define inner product as 
    \[
        \innerp{\sigma}{\mu} = \sum_{n=1}^{\infty} \sigma(n) \overline{\mu(n)}
    \]
    \linkinline{368}{6.2.23} As an example, define $e_k(n) = \delta_{nk}$. We can prove that $\{e_1, e_2, \cdots \}$ are orthonormal basis for $V$. \\
    \linkinline{406}{6.5.16} shows that $W = \span{e_2, e_4, \cdots}$ and some unitary $U$ such that $\orthocomp{W}$ is not U-invariant
\end{defn*}


\linksection{361}{Orthogonal Complements}

\begin{defn*}
    \textbf{Orthogonal Complements} Let $S$  be a nonempty subset of an inner product space $V$. We define $\orthocomp{S} = \{x\in V: \innerp{x}{y} = 0 \text{ for all } y\in S \}$. The set $\orthocomp{S}$ is called the orthogonal complement of $S$
    \begin{enumerate}
        \item $\orthocomp{\{0\}} = V$ and $\orthocomp{V} = \{0\}$ 
    \end{enumerate}
\end{defn*}

\begin{theorem*}
    \textbf{6.6 Finding Projection of a Vector onto a Subspace} Let $W$ be a finite-dimensional subspace of an inner product space $V$, and let $y\in V$. Then there exist unique vectors $u\in W$ and $z\in \orthocomp{W}$ such that $y=u+z$. Furthermore, if $\{v_1,v_2,\cdots,v_k\}$ is an orthonormal basis for $W$, then 
    \[
        u = \sum_{i=1}^k \innerp{y}{v_i}v_i    
    \]
    where $u$ is the orthogonal projection of $y$ on $W$.
\end{theorem*}


\begin{corollary*}
    \textbf{Orthogonal Projection is Unique and Closest to Projected Vector} In the notation of previous theorem, the vector $u$ the unique vector in $W$ that is closest to $y$; that is, for any $x\in W$, $\norm{y-x} \geq \norm{y-u}$, and this inequality is an equality if and only if $x=u$
\end{corollary*}


\begin{theorem*}
    \textbf{6.7 Orthonormal Basis and Subspaces} Suppose that $S = \{v_1,v_2,\cdots, v_k\}$ is an orthonormal set in an n-dimensional inner product space $V$.Then 
    \begin{enumerate}
        \item $S$ can be extended to an orthonormal basis $\{v_1,v_2,\cdots, v_k, v_{k+1}, \cdots, v_n\}$ for $V$. 
        \item If $W = \span{S}$, then $S_1 = \{v_{k+1}, \cdots, v_n\}$ is an orthonormal basis for $\orthocomp{W}$ 
        \item If $W$ is any subspace of $V$, then $\dim{V} = \dim{W} + \dim{\orthocomp{W}}$
    \end{enumerate}
\end{theorem*}

\linksection{369}{6.3 The Adjoint of a Linear Operator}

\begin{defn*}
    \textbf{Dual Space} is a space of all linear transformations from a vector space $V$ to its field $F$. 
\end{defn*}

\begin{theorem*}
    \textbf{6.8 Every Linear Transformation from $V$ to $F$ Can Be Written as a Inner Product} Let $V$ be a finite-dimensional inner product space over $F$, and let $g: V\to F$ be a linear transformation. Then there exists a unique vector $y\in V$ such that $g(x) = \innerp{x}{y}$ for all $x\in V$, where
    \[
        y = \sum_i \overline{g(v_i)} v_i \qquad \text{$\beta = \{ v_1,\cdots , v_n \}$ is orthonormal basis}
    \]
\end{theorem*}


\begin{defn*}
    \textbf{Adjoint Linear Operator} Given inner product space $V$, let $T$ be a linear operator on $V$. The adjoint of operator $T$, $T^*$, is the unique operator on $V$ satisfying 
    \[
        \innerp{T(x)}{y} = \innerp{x}{T^*(y)}    
        \qquad \text{for all $x,y\in V$}
    \]
\end{defn*}

\begin{theorem*}
    \textbf{6.9 Adjoint of an Linear Operator Exist for f.d. Inner Product Space} Let $V$ be a finite-dimensional inner product space, and let $T$ be a linear operator on $V$. Then there exists a unique function, called the adjoint of $T$, $T^*: V\to V$ such that
    \[
        \innerp{T(x)}{y} = \innerp{x}{T^*(y)}    
    \]
    for all $x,y\in V$. Furthermore, $T^*$ is linear. We can view the equation symbolically as adding an asterik $*$ to $T$  when shifting position inside the inner product symbol
\end{theorem*}



\begin{theorem*}
    \textbf{6.10 Adjoint of a Linear Operator in Matrix Form is the Adjoint of Matrix Form of that Linear Operator} Let $v$ be a finite-dimensional inner product space. Let $\beta$ be an orthonormal basis for $V$. If $T$ is a linear operator on $V$, thern 
    \[
        \cvec{T^*}{\beta} = \cvec{T}{\beta}^*
    \]
\end{theorem*}

\begin{corollary*}
    \textbf{For Left-Matrix Transformation} Let $A$ be $n\times n$ matrix, then $L_{A^*} = (L_A)^*$. (\linkinline{104}{theorem 2.16})
\end{corollary*}

\begin{theorem*}
    \textbf{6.11 Properties of Adjoint of Linear Operators} \\
    Let $V$ bewr an inner product space, and let $T$, $U$ be linear operators on $V$, then 
    \begin{enumerate}
        \item $(T+U)^* = T^* + U^*$ 
        \item $(cT)^* = \overline{c}T^*$ for any $c\in F$
        \item $(TU)^* = U^*T^*$ 
        \item $T^{**} = T$ 
        \item $I^* = I$ 
    \end{enumerate}
    assuming adjoints always exists.
\end{theorem*}

\begin{corollary*}
    \textbf{For Matrix} \\
    Let $A$ and $B$ be $n\times n$ matrix, then 
    \begin{enumerate}
        \item $(A+B)^* = A^* + B^*$ 
        \item $(cA)^* = \overline{c}A^*$ for all $c\in F$
        \item $(AB)^* = B^*A^*$ 
        \item $A^{**} = A$ 
        \item $I^* = I$ 
    \end{enumerate}
\end{corollary*}

\linksection{372}{Least Squares Approximation}

\begin{defn*}
    \textbf{Some notation} Fort $x,y\in F^n$ 
    \begin{enumerate}
        \item $\innerp{x}{y}_n$ is the standard inner product of $x$ and $y$  in $F^n$
        \item If $x$ and $y$ are column vectors, then $\innerp{x}{y}_n = y^*x$ 
    \end{enumerate}
\end{defn*}


\begin{lemma*}
    Let $A\in M_{m\times n}(F)$, $x\in F^n$ and $y\in F^m$, then 
    \[
        \innerp{Ax}{y}_m = \innerp{x}{A^*y}_n    
    \]
\end{lemma*}

\begin{lemma*}
    Let $A\in M_{m\times n}(F)$. Then $\rank{A^*A}=\rank{A}$
\end{lemma*}

\begin{corollary*}
    If $A$ is $m\times n$ matrix such that $\rank{A}=n$, then $A^*A$ is invertible
\end{corollary*}


\begin{theorem*}
    \textbf{6.12 Close Form Solution for Least Squared Problem} Let $A\in M_{m\times n}(F)$ and $y\in F^m$. Then there exists $x_0\in F^n$ such that $(A^*A)x_0 = A^*y$ and $\norm{Ax_0 - y} \leq \norm{Ax -y}$ for all $x\in F^n$. Furthermore, if $\rank{A}=n$, then $x_0 = (A^*A)^{-1}A^*y$ 
\end{theorem*}


\linksection{381}{6.4 Normal and Self-Adjoint Operators}

\begin{defn*}
    \textbf{Motivation} Condition for orthonormal basis of eigenvectors in 
    \begin{enumerate}
        \item $F = \mathbb{C}$, $T$ normal
        \item $F = \R$, $T$ self-adjoint
    \end{enumerate}
\end{defn*}

\begin{lemma*}
    \textbf{Condition on Existence of Eigenvector for Adjoint Linear Operators} Let $T$ be a linear operator on a finite-dimensional inner product space $V$. If $T$ has an eigenvector, then so does $T^*$. If $\lambda$ is an eigenvalue of $T$, then $\overline{\lambda}$ is an eigenvalue of $T^*$
    \begin{proof}
        Let $v$ be eigenvector of $T$ with corresponding eigenvalue $\lambda$, then for any $x\in V$,
        \[
            0 = \innerp{0}{x} = \innerp{(T-\lambda I)(v)}{x} = \innerp{v}{(T - \lambda I)^*(x)} = \innerp{v}{(T^* - \overline{\lambda} I)(x)}
        \]
        Let $W = \span{\{v\}}$, so $R(T^* - \overline{\lambda}I) \subseteq \orthocomp{W}$. Note $\rank{T^*-\overline{\lambda}I} \leq \dim{\orthocomp{W}} = n-1$, then $N(T^* - \overline{\lambda}I) \neq \{0\}$. So exists $u\in N(T^* - \overline{\lambda}I)$ such that $T^*(u) = \overline{\lambda}u$
    \end{proof}
\end{lemma*}

\begin{theorem*}
    \textbf{6.14 (Schur's Theorem)}
    \begin{center}
        \textbf{$P_T(t)$ Splits Implies Exists O.N. Basis st. $\cvec{T}{\beta}$ is Upper Triangular}
    \end{center}
    Let $T$ be a linear operator on a finite-dimensional inner product space $V$. Suppose that the characteristic polybnomial of $T$ splits. Then there exists an orthonormal basis $\beta$ for $V$ such that the matrix $\cvec{T}{\beta}$ is upper triangular
    \begin{proof}
        With induction, idea is to construct an orthonormal basis $\beta = \gamma \cup \{z\}$, where $\gamma$ is an orthonormal basis for $\orthocomp{W}$ and $z\in W = \span{z}$, where $z$ is unit eigenvector for $T^*$ whose existence ensured by previous lemma. The induction hypothesis mandates
        \begin{enumerate}
            \item $\orthocomp{W}$ is a T-invariant subspace as an assumption, i.e. if $y\in \orthocomp{W}, x\in W$, then $\innerp{T(y)}{x} = 0$
            \item $P_{T_{\orthocomp{W}}}(t) | P_T(t)$, so characteristic polyonmial of $T_{\orthocomp{W}}$ splits
        \end{enumerate}
        to get the orthonormal basis $\gamma$, for which $\cvec{T_{\orthocomp{W}}}{\gamma}$ is upper triangular.
    \end{proof} 
\end{theorem*}

\begin{defn*}
    \textbf{Normal Linear Operator} Let $V$ be an inner product space, and let $T$ be a linear operator on $V$. We say that $T$ is normal if $TT^* = T^*T$ . An $n\times n$ real or complex matrix $A$ is normal if $AA^* = A^*A$ (Commutivity). 
    \begin{enumerate}
        \item Motivation is that if $\cvec{T}{\beta}$ is diagonal, then $T^*$ also diagonal, hence $T$ and $T^*$ commutes
        \item $T$ is normal if and only if $\cvec{T}{\beta}$ is normal, where $\beta$ is an orthonormal basis
        \item Skew-symmetric matrix ($A^t = -A$) is normal by $A^tA = -A^2 = AA^t$
        \item Normality not sufficient to guarantee an orthonormal basis of eigenvectors. However, normality suffices if $V$ is a complex inner product space
    \end{enumerate}
\end{defn*}

\begin{theorem*}
    \textbf{6.15 Properties of Normal Operator} \\ 
    Let $V$ be an inner product space, and let $T$ be a normal operator on $V$. Then the following are true
    \begin{enumerate}
        \item $\norm{T(x)} = \norm{T^*(x)}$ for all $x\in V$
        \item $T - cI$ is normal for every $c\in F$
        \item If $x$ is an eigenvector of $T$, then $x$ is also an eigenvector of $T^*$. In fact,if $T(x) = \lambda x$, then $T^*(x) = \overline{\lambda} x$
        \item If $\lambda_1$ and $\lambda_2$ are distinct eigenvalues of $T$ with corresponding eigenvectors $x_1$ and $x_2$, then \textbf{$x_1$ and $x_2$ are orthogonal}, i.e. $\innerp{x_1}{x_2} = 0$
    \end{enumerate}
\end{theorem*}

\begin{theorem*}
    \textbf{6.16 Normal Operator iff Diagonalizable ($F=\mathbb{C}$)} \\
    Let $T$ be a linear operator on a finite-dimensional \textbf{complex} inner product space $V$. Then $T$ is normal if and only if there exists an orthonormal basis for $V$ consisting of eigenvectors of $T$.
    \begin{proof}
        Idea is the orthonormal basis that makes $T$ an upper triangular matrix (using Schur's Theorem) happens to be a set of eigenvectors
    \end{proof}
    \begin{enumerate}
        \item \linkinline{384}{example showing theorem does not work on infinite dimension vector spaces} with problem definition \linkinline{347}{here}. Specifically an example where $T$ is normal and that $T$ has no eigenvectors
        \item Normality not sufficient for existence of orthonormal basis of eigenvectors for real inner product spaces 
    \end{enumerate}
\end{theorem*}


\begin{defn*}
    \textbf{Self-Adjoint (Hermitian)} Let $T$ be a linear operator on an inner product space $V$. We say that $T$ is self-adjoint (Hermitian) if $T = T^*$. An $n\times n$ real or complex matrix $A$ is self-adjoint (Hermitian) if $A=A^*$
    \begin{enumerate}
        \item If $\beta$ is orthonormal basis, then $T$ is self-adjoint if and only if $\cvec{T}{\beta}$ is self-adjoint (symmetric matrix for $F=\R$)
        \item If $T$ is self-adjoint, then $T$ is normal
    \end{enumerate}
\end{defn*}

\begin{lemma*}
    \textbf{Properties of Self-Adjoints} \\ 
    Let $T$ be a self-adjoint operator on a finite-dimensional inner product space $V$. Then 
    \begin{enumerate}
        \item Every eigenvalue of $T$ is real 
        \item Suppose that $V$ is a real inner product space ($F=\R$). Then the characteristic polynomial of $T$ splits
    \end{enumerate}
\end{lemma*}


\begin{theorem*}
    \textbf{6.17 Self-Adjoints iff Diagonalizable ($F=\R$)} \\
    Let $T$ be a linear operator on a finite-dimensional real inner product space $V$. Then $T$ is self-adjoint if and only if there exists an orthonormal basis $\beta$ for $v$ consisting of eigenvectors of $T$.
\end{theorem*}


\begin{defn*}
    \textbf{Computing Squared Root of Imaginary Number} \\
    Relies on Euler's formula 
    \[
        e^{ix}=\cos x + i\sin x 
    \]
    Therefore we have 
    \[
        e^{i\pi} = -1 \qquad 
        i = \sqrt{-1} = e^{i\pi/2} \qquad 
        \sqrt{i} = e^{i \pi / 4} = \cos \frac{\pi}{4} + i \sin \frac{\pi}{4} = \frac{1}{\sqrt{2}} + i \frac{1}{\sqrt{2}}
    \]
\end{defn*}

\begin{defn*}
    \textbf{Positive Definite/Semidefinite} \\
    A linear operator $T$ on a finite-dimensional inner product space is called positive definite (positive semidefinite) if $T$ is 
    \begin{enumerate}
        \item self-adjoint, and 
        \item $\innerp{T(x)}{x} >0$ ($\innerp{T(x)}{x} \geq 0$) for all $x\neq 0$
    \end{enumerate}
    An $n\times n$ matrix $A$ with entries from $\R$ or $\mathbb{C}$ is positive definite (positive semidefinite) if $L_A$ is positive definite (positive semidefinite)
\end{defn*}



\linksection{391}{6.4 Unitary and Orthogonal Operators}

\begin{defn*}
    \textbf{Unitary Operator and Orthogonal Operator} Let $T$ be a linear operator on a finite-dimensional inner product space $V$ (over $F$). If $\norm{T(x)} = \norm{x}$ for all $x\in V$, we call $T$ a unitary operator if $F = \mathbb{C}$ and an orthogonal operator if $F = \R$
    \begin{enumerate}
        \item \textbf{Isomtry} for infinite-dimensional case 
        \item \textbf{Motivation} Linear operator that preserves length 
    \end{enumerate}
\end{defn*}

\begin{theorem*}
    \textbf{6.18 Property of Unitary/Orthogonal Operator} Let $T$ be a linear operator on a finite-dimensional inner product space $V$. Then the following statements are equivalent 
    \begin{enumerate}
        \item $TT^* = T^*T = I$ (unitary/orthogonal operators are \textbf{normal})
        \item $\innerp{T(x)}{T(y)} = \innerp{x}{y}$ for all $x,y\in V$
        \item If $\beta$ is an orthonormal basis for $V$, then $T(\beta)$ is an orthonormal basis for $V$ 
        \item There exists an orthonormal basis $\beta$ for $V$ such that $T(\beta)$ is an orthonormal basis for $V$ 
        \item $\norm{T(x)} = \norm{x}$ for all $x\in V$ ($T$ is a \textbf{unitary/orthogonal})
    \end{enumerate}
\end{theorem*}

\begin{lemma*}
    Let $U$ be a self-adjoint operator ona finite-dimensional inner product space $V$. If $\innerp{x}{U(x)} = 0$ for all $x\in V$, then $U = T_0$
    \begin{enumerate}
        \item \textbf{Interpretation} All vectors are orthogonal to $\{ 0 \}$, the range of $T_0$
    \end{enumerate}
\end{lemma*}

\begin{corollary*}
    \textbf{$F=\R$ Orthongal Operator has All Eigenvalues $\pm 1$} Let $T$ be a linear operator on a finite-dimensional real inner product space $V$. Then $V$ has an orthonormal basis of eigenvetors of $T$ with corresponding eigenvalues of \textbf{absolute value 1} if and only if $T$ is both \textbf{self-adjoint} and \textbf{orthogonal}
    \begin{enumerate}
        \item \textbf{Interpretation} Length-preserving operator has eigenvalue of $\pm 1$
    \end{enumerate}
\end{corollary*}

\begin{corollary*}
    \textbf{$F=\mathbb{C}$ Unitary Operator has All Eigenvalues $\pm 1$} Let $T$ be a linear operator on a finite-dimensional complex inner product space $V$. Then $V$ has an orthonormal basis of eigenvectors of $T$ with corresponding eigenvalues of absolute value 1 if and only if $T$ is unitary
\end{corollary*}

\begin{defn*}
    \textbf{Reflection} Let $L$ be a one-dimensional subspace of $\R^2$. We may view $L$ as a line in the plane through the origin. A linear operator $T$ on $\R^2$ is called a reflection of $\R^2$ about $L$ if $T(x) = x$ for all $x\in L$ and $T(x) = -x$ for all $x\in \orthocomp{L}$
    \begin{enumerate}
        \item Reflection transformation is an orthogonal operator
    \end{enumerate}
\end{defn*}

\begin{defn*}
    \textbf{Orthogonal/Unitary Matrix} A square matrix $A$ is called an orthogonal matrix if $A^T A = AA^T = I$ and unitary if $A^* A = AA^* = I$
    \begin{enumerate}
        \item \textbf{Inverse} of unitary matrix $A^{-1} = A^*$
        \item $AA^* = I$ if and only if rows of $A$ form orthonormal basis for $F^n$, similarly for columns of $A$ (prove $\innerp{r_i}{r_j} = \delta_{ij}$)
        \item For $F=\R$, if columns of $A$ consists of orthonormal basis of eigenvectors $\beta$, then rows of $A^T$ are $\beta$, so then $A$ is orthogonal
        \item T unitary/orthogonal if and only if $\cvec{T}{\beta}$ is unitary/orthogonal for some orthonormal basis $\beta$ for $V$
    \end{enumerate}
\end{defn*}

\begin{defn*}
    \textbf{Unitarily Equivalent} Matrices $A$ and $B$ are unitarily equivalent (orthogonally equivalent) if and only if there exists a unitary (orthogonal) matrix $P$ such that $A= P^* B P$
\end{defn*}

\begin{theorem*}
    \textbf{6.19 Condition for Normal ($F=\mathbb{C}$) Matrix} Let $A\in M_{n}(\mathbb{C})$. Then $A$ is normal if and only if $A$ is unitarily equivalent to a diagonal matrix
    \begin{proof}
        $(\Rightarrow)$ Since $A$ complex normal, exists orthonormal basis $\beta$ consisting of eigenvectors for $V$. So $D = Q^{-1}AQ$ where $D$ is diagonal and $Q$ has columns consisting of vectors in $\beta$, therefore $Q$ unitary/orthogonal. Therefore $A$ and $D$ is unitarily equivalent \\
        $(\Leftarrow)$ Let $A = P^*DP$ for some unitary $P$ and diagonal $D$, therefore, 
        \[
            AA^* = (P^*DP)(P^*DP)^* = P^*DPP^*D^*P = P^*DD^*P = P^*D^*DP = (P^*D^*P)(P^*DP) = A^*A
        \]
    \end{proof}
\end{theorem*}



\begin{theorem*}
    \textbf{6.20 Condition for Symmetric ($F=\R$) Matrix} Let $A \in M_{n}(\R)$. Then $A$ is symmetric if and only if $A$ is orthogonally equivalent to a real diagonal matrix.
\end{theorem*}



\linksection{286}{5.2 Direct Sums} 

\begin{defn*}
    \textbf{Sum} Let $W_1, W_2,\cdots, W_k$ be subspaces of a vector space $V$. The sum of these subspacecs is the set 
    \[
        \{v_1 + \cdots + v_k : v_i \in W_i \text{ for } 1 \leq i \leq k\}    
    \]
    which we denote by $W_1 + \cdots + W_k$ or $\textstyle\sum_{i=1}^k W_i$
\end{defn*}

\begin{defn*}
    \textbf{Direct Sum} Let $W_1, W_2, \cdots, W_k$ be subspaces of a vector space $V$. We call $V$ the direct sum of the subspaces $W_1, W_2, \cdots, W_k$ and write $V = W_1 \oplus W_2 \oplus \cdots \oplus W_k$ if 
    \[
        V = \sum_{i=1}^k W_i    
        \qquad \text{and} \qquad 
        W_j \cap \sum_{i\neq j} W_i = \{ 0\} \text{ for each } 1 \leq j \leq k
    \]
    \begin{enumerate}
        \item dimension of direct sum is sum of dimension of the subspaces in the sum
        \[
            \dim{V} = \dim{W_1} + \cdots + \dim{W_k}
        \]
    \end{enumerate}
\end{defn*}


\begin{theorem*}
    \textbf{5.10 Equivalence Condition for Direct Sum} \\
    Let $W_1, \cdots W_k$ be subspaces of finite-dimensional vector space $V$. The following results are equivalent
    \begin{enumerate}
        \item $V = W_1 \oplus \cdots \oplus W_k$ 
        \item $V = \textstyle\sum_{i=1}^k W_i$ and, for any vector $v_1, \cdots, v_k$ such that $v_i \in W_i$ ($1\leq i \leq k$), if $v_1 + \cdots v_k = 0$, then $v_i = 0$ for all $i$.
        \item Each vector $v\in V$ can be uniquely written as $v = v_1 + v_2 + \cdots + v_k$ where $v_i \in W_i$ 
        \item If $\gamma_i$ is an ordered basis for $W_i$ ($1\leq i \leq k$), then $\gamma_1 \cup \gamma_2 \cup \cdots \cup \gamma_k$ is an ordered basis for $V$
        \item For each $i = 1,2,\cdots, k$, there exists an ordered basis $\gamma_i$ for $W_i$ such that $\gamma_1 \cup \gamma_2 \cup\cdots \cup \gamma_k$ is an ordered basis for $V$ 
    \end{enumerate}
\end{theorem*}

\begin{theorem*}
    \textbf{5.11} A linear operator $T$ on a finite-dimensional vector space $V$ is diagonalizable if and only if $V$ is the direct sum of the eigenspaces of $T$
\end{theorem*}


\linksection{410}{6.6 Orthogonal Projection and the Spectral Theorem} 

\begin{defn*}
    \textbf{Projection on a Subspace} \\
    Let $V$ be a vector space and $W_1$ and $W_2$ be subspaces of $V$ such taht $V = W_1 \oplus W_2$. A function $T:V\to V$ is called the projection on $W_1$ along $W_2$ if, for $x = x_1 + x_2$ with $x_1 \in W_1$ and $x_2\in W_2$, we have $T(x) = x_1$
    \begin{enumerate}
        \item $R(T) = W_1 = \{ x\in V: T(x) = x \}$ and $N(T) = W_2$
        \item $V = R(T) \oplus N(T)$, i.e. 
        \begin{enumerate}
            \item every projection uniquely determined by its range and nullspace
            \item $W_1$ does not uniquely determine $T$
        \end{enumerate}
        \item $T$ \textbf{is a projection if and only if} $T = T^2$
    \end{enumerate}
    \begin{proof}
        Proving $T$ is a projection iff $T^2 = T$. Forward direction, 
        \[
            T^2(x) = T(T(x)) = T(x_1) = x_1 = T(x)
        \]
        For reverse direction, assume $T^2 = T$, then $T(I-T) = 0_V$. Let $W_1 = R(T)$ and $W_2 = N(T)$, now claim $V = W_1 \oplus W_2$. We first prove $N(T) = R(I-T)$. Since $T(I-T)=0$, then $R(I-T)\subseteq N(T)$. Conversely, if $x\in N(T)$, then $(I-T)(x) = x - T(x) = x$, i.e. $x\in R(I-T)$. Now write $I = T+I-T$, then $x = T(x) + (I-T)(x)$ for any $x\in V$. Then $v = w_1 + w_2$ where $w_1\in W_1$ and $w_2\in W_2$. Now we prove uniqueness, i.e. $\{ 0\} = R(T) \cap N(T)$. Let $T(x) \in R(T) \cap N(T)$ as $T(x)\in R(T)$ by default and let $T(x)\in N(T)$. Then $T(x) = 0$. Proved $V = W_1 \oplus W_2$. Then we can write $x = x_1 + x_2$ where $x_1 \in R(T)$ and $x_2 \in N(T)$, hence 
        \[
            T(x) = T(x_1 + x_2) = T(x_1) + T(x_2) = T(x_1) = x_1
        \]
        where the last equality given by letting $x_1 = T(y)$, then $T(x_1) = T^2(y) = T(y)=  x_1$
    \end{proof}
\end{defn*}



\begin{defn*}
    \textbf{Orthogonal Projection} Let $V$ be an inner product space, and let $T: V\to V$ be a a linear operator. We say that $T$ is an orthogonal projection if 
    \begin{enumerate}
        \item $T$ is a projection, and
        \item $\orthocomp{R(T)} = N(T)$ and $\orthocomp{N(T)} = R(T)$
    \end{enumerate}
    Note
    \begin{enumerate}
        \item If $V$ finite-dimensional, need to assume either condition in 2. hold. 
        \item Orthogonal projection $T$ is uniquely determmined by its range $W$, so instead call $T$ the orthogonal projection of $V$ on $W$
    \end{enumerate}
\end{defn*}

\begin{proposition*}
    \textbf{Projection of Vector to a Subspace is an Orthogonal Projection} Let $W$ be subspace of $V$. there exists $u\in W$ and $z\in \orthocomp{W}$ and $y\in V$ such that $y=u+z$. If we define linear operator $T: V\rightarrow V$ by 
    \[
        T(y) = u = \sum_{i=1}^k \innerp{y}{v_i} v_i    
    \]
    where $\{v_1, \cdots, v_n\}$ is an orthonormal basis for $W$. Then $T$ is an orthogonal projection.
    \begin{proof}
        Prove that $T^2 = T = T^*$. For any $v_j \in \beta$, we have 
        \[
            T(T(v_j)) = T(\sum_{i=1}^k \innerp{v_j}{v_i}v_i) = T(v_j)
        \]
        therefore $T^2 = T$ since linear operator characterized by basis entirely. Let $x,y\in V$,
        \[
            \innerp{T(x)}{y} = \innerp{\sum_i \innerp{x}{v_i}v_i}{y} = \sum_i \innerp{x}{v_i}\innerp{v_i}{y} = \sum_i \overline{\innerp{y}{v_i}} \innerp{x}{v_i} = \innerp{x}{\sum_i \innerp{y}{v_i}v_i} = \innerp{x}{T(y)}
        \]
        therefore $T = T^*$ by theorem 6.24, $T$ is orthogonal projection.
    \end{proof}
    \begin{proof}
        Alternatively we can prove that $N(T)$ and $R(T)$ are reciprocally orthogonal sets. Since $T$ is a projection, we have $V = R(T) \oplus N(T)$, where $N(T) = \{x\in V: \innerp{x}{v_i} = 0 \text{ for all }i\} = \orthocomp{W}$ and $R(T) = W$. Therefore $N(T) = \orthocomp{R(T)}$ and $\orthocomp{N(T)} = (R(T)^{\perp})^{\perp} \supseteq R(T)$. Now we show if $x\in \orthocomp{N(T)}$, then $x\in R(T) = W$. Now by direct sum, we can write $x = y+w$ where $y\in N(T)$ and $w\in R(T)=W$, then 
        \[
            0 = \innerp{x}{y} = \innerp{y}{y} + \innerp{w}{y} 
            \quad \rightarrow \quad 
            \innerp{y}{y} = 0 
            \quad \rightarrow \quad 
            y = 0
        \]
        therefore $x = w \in W$. 
    \end{proof}
\end{proposition*}

\begin{theorem*}
    \textbf{6.24 $T^2 = T = T^*$ iff Orthogonal Projection} \\
    Let $V$ be an inner product space, and let $T$ be a linear operator on $V$. Then $T$ is an orthogonal projection if and only if $T$ has an adjoint $T^*$ and $T^2 = T = T^*$
    \begin{enumerate}
        \item Let $T$ be orthogonal projection of $V$ on $W$, and $\beta = \{v_1, \cdots, v_n\}$ is an orthonormal basis for $V$, and $\{v_1, \cdots, v_k\}$ is a basis for $W$, then
        \[
            \cvec{T}{\beta} = 
            \begin{pmatrix}
                I_k & O \\
                O & O \\
            \end{pmatrix}    
        \]
        \item Let $U$ be any projection on $W$, then exists $\gamma$ such that $\cvec{U}{\gamma}$ has same for as above, but $\gamma$ need not be orthonormal
    \end{enumerate}
\end{theorem*}

\begin{theorem*}
    \textbf{6.25 The Spectral Theorem} \\
    Suppose $T$ a linear operator  on a finite-dimensional inner product space $V$ over $F$ with distinct eigenvalues $\lambda_1, \cdots, \lambda_k$. Assume $T$ is normal if $F = \mathbb{C}$ and that $T$ is self-adjoint if $F = \R$. (i.e. guarantees orthonormal basis of eigenvectors). For each $i$ ($1\leq i \leq k$), let $W_i$ be the eigenspace of $T$ corresponding to the eigenvalue $\lambda_i$, and let $T_i$ be the orthogonal projection of $V$ on $W_i$. Then the following statements are true 
    \begin{enumerate}
        \item $V = W_1\oplus \cdots\oplus W_k$ 
        \item If $W_i'$ denotes the direct sum of subspaces $W_j$ for $j\neq i$, then $\orthocomp{W_i} = W_i'$
        \item $T_i T_j = \delta_{ij} T_i$ for $1\leq i,j\leq k$
        \item $I = T_1 + T_2 + \cdots + T_k$ (\textbf{resolution of identity operator induced by $T$})
        \item $T = \lambda_1 T_1 + \lambda_2 T_2 + \cdots + \lambda_k T_k$ (\textbf{spectral decomposition})
    \end{enumerate}
    note 
    \begin{enumerate}
        \item Note since $T_i$ orthogonal projection, we have $N(T_i) = \orthocomp{R(T_i)} = \orthocomp{W_i} = W_i'$
        \item \textbf{Spectrum} The set $\{\lambda_1, \cdots, \lambda_k\}$ is called spectrum of $T$
        \item Let $\beta$ be union of orthonormal basis of $W_i$'s and let $m_i = \dim{W_i}$, then 
        \[
            \cvec{T}{\beta} = 
            \begin{pmatrix}
                \lambda_1 I_{m_1} & O & \cdots & O \\ 
                O & \lambda_2 I_{m_2} & \cdots & O \\
                \vdots & \vdots & & \vdots \\
                O & O & \cdots & \lambda_k I_{m_k} \\
            \end{pmatrix}    
        \]
    \end{enumerate}
\end{theorem*}

\begin{corollary*}
    \textbf{Condition for Normal} If $F=\mathbb{C}$, then $T$ is normal if and only if $T^* = g(T)$ for some polynomial 
\end{corollary*}

\begin{corollary*}
    \textbf{Condition of Unitary} If $F = \mathbb{C}$, then $T$ is unitary if and only if $T$ is normal and $|\lambda|=1$ for every eigenvalue $\lambda$ of $T$
\end{corollary*}

\begin{corollary*}
    \textbf{Condition for Self-Adjoint} If $F = \mathbb{C}$ and $T$ is normal, then $T$ is self-adjoint if and only if every eigenvalue of $T$ is real
\end{corollary*}



\end{document}
