\documentclass[11pt]{article}
\input{/Users/markwang/.preamble}
\begin{document}

% arg1=pdfurl arg2=pagenum arg3=sectiontitle
\newcommand{\linksection}[3][../../linear_algebra_friedberg_insel_spence_4ed.pdf]{
    \subsection*{\href[page=#2]{#1}{#3}}
}

\newcommand{\linkinline}[3][../../linear_algebra_friedberg_insel_spence_4ed.pdf]{
    \noindent\href[page=#2]{#1}{#3}
}

\newcommand{\linksolution}[3][../../solution_compiled.pdf]{
    \noindent\href[page=#2]{#1}{#3} \\
}

\newcommand{\vecspace}{\mathcal{V}}
\newcommand{\field}{\mathcal{F}}
\newcommand{\trace}[1]{tr(#1)}
\renewcommand{\span}[1]{span(#1)}
\renewcommand{\dim}[1]{dim(#1)}
\newcommand{\nullity}[1]{nullity(#1)}
\newcommand{\rank}[1]{rank(#1)}
\newcommand{\cvec}[2]{\left[ #1 \right]_{#2}}
\renewcommand{\matr}[3]{\left[ #1 \right]_{#2}^{#3}}
\newcommand{\ltspace}[1]{\mathcal{L}(#1)}
\renewcommand{\det}[1]{det(#1)}
\newcommand{\tinvariant}[2]{\langle#2\rangle_{#1}}
\newcommand{\innerp}[2]{\langle#1,#2\rangle}
\renewcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\orthocomp}[1]{#1^{\perp}}

\linksection{341}{Chapter 6 Inner Product Spaces}
\linksection{341}{6.1 Inner Products and Norms}


\begin{defn*}
    \textbf{Inner Product} Let $V$ be a vector space over $F$. An inner product on $V$ is a function that assigns, to every ordered pair of vectors $x$ and $y$ in $V$, a scalar in $F$, denoted $\innerp{x}{y}$, such that for all $x,y,z\in VE$ and all $c\in F$, 
    \begin{enumerate}
        \item $\innerp{x+z}{y} = \innerp{x}{y} + \innerp{z}{y}$ 
        \item $\innerp{cx}{y} = c\innerp{x}{y}$
        \item $\overline{\innerp{x}{y}} = \innerp{y}{x}$ 
        \item $\innerp{x}{x} > 0$ if $x\neq 0$
    \end{enumerate}
    First two condition requires inner product be linear in the first component. Also 
    \[
        \innerp{\sum_i a_i v_i}{y} = \sum_i a_i \innerp{v_i}{y}    
    \]
\end{defn*}

\begin{defn*}
    \textbf{Conjugate Transpose or Adjoint of a Matrix} Let $A\in M_{m\times n}(F)$, the conjugate transpose or adjoint of $A$ is an $n\times m$ matrix $A^*$ such that $(A^*)_{ij} = \overline{A_{ji}}$ for all $i,j$. For $F=\R$, $A^* = A^T$
\end{defn*}

\begin{defn*}
    \textbf{Inner Product Definition Example}
    \begin{enumerate}
        \item \textbf{Standard Inner Product on $F^n$} For $x = (a_1,a_2,\cdots,a_n)$ and $y=(b_1,b_2,\cdots, b_n)$ in $F^n$, the standard inner product on $F^n$ is given by 
        \[
            \innerp{x}{y} = \sum_{i=1}^n a_i \overline{b}_i    
        \]
        \item \textbf{Inner Product for Real-valued Continuous Functions on $[0,1]$} Let $V=C([0,1])$, $f,g\in V$, define 
        \[
            \innerp{f}{g} = \int_0^1 f(t) g(t) dt    
        \]
        \item \textbf{Frobenius Inner Product for Matrices} Let $V=M_{n\times n}(F)$, $A,B\in V$, then 
        \[
            \innerp{A}{B} = \trace{B^*A} = \sum_{i=1}^n (B^*A)_{ii}
        \]
    \end{enumerate}
\end{defn*}

\begin{defn*}
    \textbf{Inner Product Space} A vector space over $F$ endowed with a specific inner product is called an inner product space. If $F=C$, $V$ is a complex inner product space; if $F=\R$, then $V$ is a real inner product space
\end{defn*}

\begin{theorem*}
    \textbf{6.1 Properties From Inner Product Conditions} Let $V$ be an inner product space. Then for $x,y,z\in V$ and $c\in F$, the following statements are true 
    \begin{enumerate}
        \item $\innerp{x}{y+z} = \innerp{x}{y} + \innerp{x}{z}$
        \item $\innerp{x}{cy} = \overline{c} \innerp{x}{y}$
        \item $\innerp{x}{0} = \innerp{0}{x} = 0$
        \item $\innerp{x}{x} = 0$ if and only if $x=0$ 
        \item If $\innerp{x}{y} = \innerp{x}{z}$ for all $x\in V$, then $y=z$
    \end{enumerate}
    The inner product is conjugate linear in the second argument 
\end{theorem*}

\begin{defn*}
    \textbf{Norm/Length} Let $V$ be an inner product space. For $x\in V$, define norm or length of $x$ by 
    \[
        \norm{x} = \sqrt{\innerp{x}{x}}    
    \]
\end{defn*}

\begin{defn*}
    \textbf{6.2 Properties of Norm} Let $V$ be an inner product space over $F$. Then for all $x,y\in V$ and $c\in F$, the following statements are true 
    \begin{enumerate}
        \item $\norm{cx} = |c| \cdot \norm{x}$ 
        \item $\norm{x} = 0$ if and only if $x=0$. In any case, $\norm{x} \geq 0$ 
        \item \textbf{Cauchy-Schwarz Inequality} $|\innerp{x}{y}| \leq \norm{x} \cdot \norm{y}$
        \item \textbf{Triangular Inequality} $\norm{x+y} \leq \norm{x} + \norm{y}$
    \end{enumerate}
\end{defn*}

\begin{defn*}
    \textbf{Angle} For $F=\R$, $x,y \neq 0$, and $\theta$ be angle between $x$ and $y$
    \[
        \cos{\theta} = \frac{\innerp{x}{y}}{\norm{x}\norm{y}}
        \quad \quad \quad \quad 
        \theta = \cos^{-1}\left( \frac{\innerp{x}{y}}{\norm{x}\norm{y}} \right)
    \]
    Note
    \[
        \left|\frac{\innerp{x}{y}}{\norm{x}\norm{y}}\right| \leq 1
    \]
    So valid input to arccos function
\end{defn*}

\begin{defn*}
    \textbf{Orthogonal Vectors} Let $V$ be an inner product space. Vectors $x$ and $y$ in $V$ are orthogonal (perpendicular) if $\innerp{x}{y} = 0$.
\end{defn*}

\begin{defn*}
    \textbf{Orthogonal Sets and Orthonormal Sets} A subset $S$ of $V$ is orthogonal if any two distinct vectors in $S$ are orthogonal. A vector $x$ in $V$ is a unit vctor if $\norm{x} = 1$. A subset $S$ of $V$ is orthonormal if $S$ is orthogonal and consists entirely of unit vectors. 
    \begin{enumerate}
        \item $S = \{v_1,v_2,\cdots\}$, then $S$ is orthonormal if and only if $\innerp{v_i}{v_j} = \delta_{ij}$
        \item We can \textbf{normalize} an orthogonal set $S$, by multiplying $1/\norm{x}$ for each $x\in S$
    \end{enumerate}
\end{defn*}


\begin{defn*}
    \textbf{Orthonormal Set Property} Let $V$ be inner product space and $S = \{s_1,s_2,\cdots\}\subseteq V$ be an orthonormal set. Let $v\in \span{S}$, then $v = a_1s_1 + \cdots + a_k s_k$. Then 
    \[
        \innerp{v}{s_j} = a_j    
    \]
    by
    \[
        \innerp{v}{s_j} = \innerp{\sum_i a_i s_i}{s_j} = \sum_i a_i \innerp{s_i}{s_j} = \sum_i a_i \delta_{ij} = a_j    
    \]
\end{defn*}


 
\linksection{353}{Gram-Schmidt Orthogonalization Process and Orthogonal Complements}

\begin{defn*}
    \textbf{Orthonormal Basis} Let $V$ be an inner product space. A subset of $V$ is an orthonormal basis for $V$ if it is an ordered basis that is orthonormal
\end{defn*}

\begin{defn*}
    \textbf{Every Inner Product Space has $n$ Orthogonal Basis} Let $V$ be an inner product space and $S = \{ v_1, v_2, \cdots, v_k\}$ be an orthogonal subset of $V$ consisting of nonzero vectors. If $y\in \span{S}$, then 
    \[
        y = \sum_{i=1}^k \frac{\innerp{y}{v_i}}{\norm{v_i}^2} v_i
    \]
\end{defn*}

\begin{corollary*}
    \textbf{Special case for Orthonormal Set} If, in addition to hypotheses of previous theorem, $S$ is orthonormal and $y\in \spam{S}$, then 
    \[
        y = \sum_{i=1}^k \innerp{y}{v_i}v_i    
    \]
\end{corollary*}

\begin{corollary*}
    \textbf{Nonzero Orthonormal Set is Linearly Independent} Let $V$ be an inner product space, andf let $S$ be an orthogonal subset of $V$ consisting of nonzero vectors. Then $S$ is linearly independent
\end{corollary*}

\begin{theorem*}
    \textbf{6.4 Gram-Schmidt Process} Let $V$ be an inner product space and $S = \{w_1,w_2,\cdots, w_n\}$ be a linearly independent subset of $V$. Define $S' = \{v_1,v_2,\cdots, v_n\}$, where $v_1 = w_1$ and 
    \[
        v_k = w_k - \sum_{j=1}^{k-1} \frac{\innerp{w_k}{v_j}}{\norm{v_j}^2} v_j 
        \quad \quad 
        2\leq k\leq n
    \]
    Then $S'$ is an orthogonal set of nonzero vectors such that $\span{S'} = \span{S}$
\end{theorem*}



\begin{theorem*}
    \textbf{6.5 Every Finite Dimensional I.P.S has an Orthonormal Basis} Let $V$ be a nonzero finite-dimensional inner product space. Then $V$ has an orthonormal basis $\beta$. Furthermore, if $\beta = \{v_1, v_2,\cdots, v_n\}$ and $x\in V$, then 
    \[
        x = \sum_{x=1}^n \innerp{x}{v_i}v_i    
    \]
\end{theorem*}


\begin{corollary*}
    \textbf{Expression for Matrix Representation of Transformation on Orthonormal Basis} Let $V$ be a finite-dimensional inner product space with an orthonormal basis $\beta = \{v_1,v_2,\cdots, v_n\}$. Let $T$ be a linear operator on $V$, and let $A = \cvec{T}{\beta}$. Then for any $i$ and $j$, $A_{ij} = \innerp{T(v_j)}{v_i}$, i.e. 
    \[
        T(v_j) = \sum_{i=1}^n \innerp{T(v_j)}{v_i}  v_i  
    \]
\end{corollary*}


\begin{defn*}
    \textbf{Fourier Coefficients} Let $\beta$ be an orthonormal subset (possibly infinite) of an inner product space $V$, and let $x\in V$. We define the Fourier coefficients of $x$ relative to $\beta$ to be the scalars $\innerp{x}{y}$, where $y\in \beta$
\end{defn*}

\linksection{361}{Orthogonal Complements}

\begin{defn*}
    \textbf{Orthogonal Complements} Let $S$  be a nonempty subset of an inner product space $V$. We define $\orthocomp{S} = \{x\in V: \innerp{x}{y} = 0 \text{ for all } y\in S \}$. The set $\orthocomp{S}$ is called the orthogonal complement of $S$
    \begin{enumerate}
        \item $\orthocomp{\{0\}} = V$ and $\orthocomp{V} = \{0\}$ 
    \end{enumerate}
\end{defn*}

\begin{theorem*}
    \textbf{6.6 Finding Projection of a Vector onto a Subspace} Let $W$ be a finite-dimensional subspace of an inner product space $V$, and let $y\in V$. Then there exist unique vectors $u\in W$ and $z\in \orthocomp{W}$ such that $y=u+z$. Furthermore, if $\{v_1,v_2,\cdots,v_k\}$ is an orthonormal basis for $W$, then 
    \[
        u = \sum_{i=1}^k \innerp{y}{v_i}v_i    
    \]
    where $u$ is the orthogonal projection of $y$ on $W$.
\end{theorem*}


\begin{corollary*}
    \textbf{Orthogonal Projection is Unique and Closest to Projected Vector} In the notation of previous theorem, the vector $u$ the unique vector in $W$ that is closest to $y$; that is, for any $x\in W$, $\norm{y-x} \geq \norm{y-u}$, and this inequality is an equality if and only if $x=u$
\end{corollary*}


\begin{theorem*}
    \textbf{6.7 Orthonormal Basis and Subspaces} Suppose that $S = \{v_1,v_2,\cdots, v_k\}$ is an orthonormal set in an n-dimensional inner product space $V$.Then 
    \begin{enumerate}
        \item $S$ can be extended to an orthonormal basis $\{v_1,v_2,\cdots, v_k, v_{k+1}, \cdots, v_n\}$ for $V$. 
        \item If $W = \span{S}$, then $S_1 = \{v_{k+1}, \cdots, v_n\}$ is an orthonormal basis for $\orthocomp{W}$ 
        \item If $W$ is any subspace of $V$, then $\dim{V} = \dim{W} + \dim{\orthocomp{W}}$
    \end{enumerate}
\end{theorem*}

\linksection{369}{6.3 The Adjoint of a Linear Operator}

\begin{defn*}
    \textbf{Dual Space} is a space of all linear transformations from a vector space $V$ to its field $F$. 
\end{defn*}

\begin{theorem*}
    \textbf{6.8 Every Linear Transformation from $V$ to $F$ Can Be Written as a Inner Product} Let $V$ be a finite-dimensional inner product space over $F$, and let $g: V\to F$ be a linear transformation. Then there exists a unique vector $y\in V$ such that $g(x) = \innerp{x}{y}$ for all $x\in V$, where
    \[
        y = \sum_i \overline{g(v_i)} v_i \qquad \text{$\beta = \{v_1,\cdots, v_n\}$ is orthonormal basis}
    \]
\end{theorem*}


\begin{defn*}
    \textbf{Adjoint Linear Operator} Given inner product space $V$, let $T$ be a linear operator on $V$. The adjoint of operator $T$, $T^*$, is the unique operator on $V$ satisfying 
    \[
        \innerp{T(x)}{y} = \innerp{x}{T^*(y)}    
        \qquad \text{for all $x,y\in V$}
    \]
\end{defn*}

\begin{theorem*}
    \textbf{6.9 Adjoint of an Linear Operator Exist for f.d. Inner Product Space} Let $V$ be a finite-dimensional inner product space, and let $T$ be a linear operator on $V$. Then there exists a unique function, called the adjoint of $T$, $T^*: V\to V$ such that
    \[
        \innerp{T(x)}{y} = \innerp{x}{T^*(y)}    
    \]
    for all $x,y\in V$. Furthermore, $T^*$ is linear. We can view the equation symbolically as adding an asterik $*$ to $T$  when shifting position inside the inner product symbol
\end{theorem*}



\begin{theorem*}
    \textbf{6.10 Adjoint of a Linear Operator in Matrix Form is the Adjoint of Matrix Form of that Linear Operator} Let $v$ be a finite-dimensional inner product space. Let $\beta$ be an orthonormal basis for $V$. If $T$ is a linear operator on $V$, thern 
    \[
        \cvec{T^*}{\beta} = \cvec{T}{\beta}^*
    \]
\end{theorem*}

\begin{corollary*}
    \textbf{For Left-Matrix Transformation} Let $A$ be $n\times n$ matrix, then $L_{A^*} = (L_A)^*$. (\linkinline{104}{theorem 2.16})
\end{corollary*}

\begin{theorem*}
    \textbf{6.11 Properties of Adjoint of Linear Operators} \\
    Let $V$ bewr an inner product space, and let $T$, $U$ be linear operators on $V$, then 
    \begin{enumerate}
        \item $(T+U)^* = T^* + U^*$ 
        \item $(cT)^* = \overline{c}T^*$ for any $c\in F$
        \item $(TU)^* = U^*T^*$ 
        \item $T^{**} = T$ 
        \item $I^* = I$ 
    \end{enumerate}
    assuming adjoints always exists.
\end{theorem*}

\begin{corollary*}
    \textbf{For Matrix} \\
    Let $A$ and $B$ be $n\times n$ matrix, then 
    \begin{enumerate}
        \item $(A+B)^* = A^* + B^*$ 
        \item $(cA)^* = \overline{c}A^*$ for all $c\in F$
        \item $(AB)^* = B^*A^*$ 
        \item $A^{**} = A$ 
        \item $I^* = I$ 
    \end{enumerate}
\end{corollary*}

\linksection{372}{Least Squares Approximation}

\begin{defn*}
    \textbf{Some notation} Fort $x,y\in F^n$ 
    \begin{enumerate}
        \item $\innerp{x}{y}_n$ is the standard inner product of $x$ and $y$  in $F^n$
        \item If $x$ and $y$ are column vectors, then $\innerp{x}{y}_n = y^*x$ 
    \end{enumerate}
\end{defn*}


\begin{lemma*}
    Let $A\in M_{m\times n}(F)$, $x\in F^n$ and $y\in F^m$, then 
    \[
        \innerp{Ax}{y}_m = \innerp{x}{A^*y}_n    
    \]
\end{lemma*}

\begin{lemma*}
    Let $A\in M_{m\times n}(F)$. Then $\rank{A^*A}=\rank{A}$
\end{lemma*}

\begin{corollary*}
    If $A$ is $m\times n$ matrix such that $\rank{A}=n$, then $A^*A$ is invertible
\end{corollary*}


\begin{theorem*}
    \textbf{6.12 Close Form Solution for Least Squared Problem} Let $A\in M_{m\times n}(F)$ and $y\in F^m$. Then there exists $x_0\in F^n$ such that $(A^*A)x_0 = A^*y$ and $\norm{Ax_0 - y} \leq \norm{Ax -y}$ for all $x\in F^n$. Furthermore, if $\rank{A}=n$, then $x_0 = (A^*A)^{-1}A^*y$ 
\end{theorem*}


\linksection{381}{6.4 Normal and Self-Adjoint Operators}

\begin{lemma*}
    \textbf{Condition on Existence of Eigenvector for Adjoint Linear Operators} Let $T$ be a linear operator on a finite-dimensional inner product space $V$. If $T$ has an eigenvector, then so does $T^*$. If $\lambda$ is an eigenvalue of $T$, then $\overline{\lambda}$ is an eigenvalue of $T^*$
    \begin{proof}
        Let $v$ be eigenvector of $T$ with corresponding eigenvalue $\lambda$, then for any $x\in V$,
        \[
            0 = \innerp{0}{x} = \innerp{(T-\lambda I)(v)}{x} = \innerp{v}{(T - \lambda I)^*(x)} = \innerp{v}{(T^* - \overline{\lambda} I)(x)}
        \]
        Let $W = \span{\{v\}}$, so $R(T^* - \overline{\lambda}I) \subseteq \orthocomp{W}$. Note $\rank{T^*-\overline{\lambda}I} \leq \dim{\orthocomp{W}} = n-1$, then $N(T^* - \overline{\lambda}I) \neq \{0\}$. So exists $u\in N(T^* - \overline{\lambda}I)$ such that $T^*(u) = \overline{\lambda}u$
    \end{proof}
\end{lemma*}

\begin{theorem*}
    \textbf{6.14 (Schur's Theorem)}
    \begin{center}
        \textbf{$P_T(t)$ Splits Implies Exists O.N. Basis st. $\cvec{T}{\beta}$ is Upper Triangular}
    \end{center}
    Let $T$ be a linear operator on a finite-dimensional inner product space $V$. Suppose that the characteristic polybnomial of $T$ splits. Then there exists an orthonormal basis $\beta$ for $V$ such that the matrix $\cvec{T}{\beta}$ is upper triangular
    \begin{proof}
        With induction, idea is to construct an orthonormal basis $\beta = \gamma \cup \{z\}$, where $\gamma$ is an orthonormal basis for $\orthocomp{W}$ and $z\in W = \span{z}$, where $z$ is unit eigenvector for $T^*$ whose existence ensured by previous lemma. The induction hypothesis mandates
        \begin{enumerate}
            \item $\orthocomp{W}$ is a T-invariant subspace as an assumption, i.e. if $y\in \orthocomp{W}, x\in W$, then $\innerp{T(y)}{x} = 0$
            \item $P_{T_{\orthocomp{W}}}(t) | P_T(t)$, so characteristic polyonmial of $T_{\orthocomp{W}}$ splits
        \end{enumerate}
        to get the orthonormal basis $\gamma$, for which $\cvec{T_{\orthocomp{W}}}_{\gamma}$ is upper triangular.
    \end{proof} 
\end{theorem*}

\begin{defn*}
    \textbf{Normal Linear Operator} Let $V$ be an inner product space, and let $T$ be a linear operator on $V$. We say that $T$ is normal if $TT^* = T^*T$ . An $n\times n$ real or complex matrix $A$ is normal if $AA^* = A^*A$ (Commutivity). 
    \begin{enumerate}
        \item $T$ is normal if and only if $\cvec{T}{\beta}$ is normal, where $\beta$ is an orthonormal basis
        \item Skew-symmetric matrix ($A^t = -A$) is normal by $A^tA = -A^2 = AA^t$
        \item Normality not sufficient to guarantee an orthonormal basis of eigenvectors. However, normality suffices if $V$ is a complex inner product space
    \end{enumerate}
\end{defn*}

\begin{theorem*}
    \textbf{6.15 Properties of Normal Operator} \\ 
    Let $V$ be an inner product space, and let $T$ be a normal operator on $V$. Then the following are true
    \begin{enumerate}
        \item $\norm{T(x)} = \norm{T^*(x)}$ for all $x\in V$
        \item $T - cI$ is normal for every $c\in F$
        \item If $x$ is an eigenvector of $T$, then $x$ is also an eigenvector of $T^*$. In fact,if $T(x) = \lambda x$, then $T^*(x) = \overline{\lambda} x$
        \item If $\lambda_1$ and $\lambda_2$ are distinct eigenvalues of $T$ with corresponding eigenvectors $x_1$ and $x_2$, then $x_1$ and $x_2$ are orthogonal
    \end{enumerate}
\end{theorem*}

\begin{theorem*}
    \textbf{6.16 Normal Operator iff Diagonalizable ($F=\mathbb{C}$)} \\
    Let $T$ be a linear operator on a finite-dimensional \textbf{complex} inner product space $V$. Then $T$ is normal if and only if there exists an orthonormal basis for $V$ consisting of eigenvectors of $T$.
    \begin{proof}
        Idea is the orthonormal basis that makes $T$ an upper triangular matrix happens to be a set of eigenvectors. The proof consists of using fundamental theorem of algebra to show that $P_T(t)$ splits and by Schur's theorem there exists orthonrmal basis $\beta = \{v_1,v_2,\cdots, v_n\}$ for $V$ such that $\cvec{T}{\beta} = A$ is upper triangular. Then use induction on $k$ to prove that $v_k \in \beta$ is in fact eigenvectors, given that $\{v_1,\cdots, v_{k-1}\}$ are eigenvectors, with base case $k=1$, $T(v_1) = A_{11}v_1$ since $A$ is upper triangular. The converse is true by $\cvec{T}{\beta}$ diagonal and so $\cvec{T^*}{\beta}$ also diagonal, diagonal matrix commute, so $T$ is normal
    \end{proof}
    \begin{enumerate}
        \item \linkinline{384}{example showing theorem does not work on infinite dimension vector spaces} with problem definition \linkinline{347}{here}. Specifically an example where $T$ is normal and that $T$ has no eigenvectors
        \item Normality not sufficient for existence of orthonormal basis of eigenvectors for real inner product spaces 
    \end{enumerate}
\end{theorem*}


\begin{defn*}
    \textbf{Self-Adjoint} Let $T$ be a linear operator on an inner product space $V$. We say that $T$ is self-adjoint (Hermitian) if $T = T^*$. An $n\times n$ real or complex matrix $A$ is self-adjoint (Hermitian) if $A=A^*$
    \begin{enumerate}
        \item If $\beta$ is orthonormal basis, then $T$ is self-adjoint if and only if $\cvec{T}{\beta}$ is self-adjoint (symmetric matrix for $F=\R$)
        \item If $T$ is self-adjoint, then $T$ is normal
    \end{enumerate}
\end{defn*}

\begin{lemma*}
    \textbf{Properties of Self-Adjoints} \\ 
    Let $T$ be a self-adjoint operator on a finite-dimensional inner product space $V$. Then 
    \begin{enumerate}
        \item Every eigenvalue of $T$ is real 
        \item Suppose that $V$ is a real inner product space ($F=\R$). Then the characteristic polynomial of $T$ splits
    \end{enumerate}
\end{lemma*}


\begin{theorem*}
    \textbf{6.17 Self-Adjoints iff Diagonalizable ($F=\R$)} \\
    Let $T$ be a linear operator on a finite-dimensional real inner product space $V$. Then $T$ is self-adjoint if and only if there exists an orthonormal basis $\beta$ for $v$ consisting of eigenvectors of $T$.
\end{theorem*}


\begin{defn*}
    \textbf{Computing Squared Root of Imaginary Number} \\
    Relies on Euler's formula 
    \[
        e^{ix}=\cos x + i\sin x 
    \]
    Therefore we have 
    \[
        e^{i\pi} = -1 \qquad 
        i = \sqrt{-1} = e^{i\pi/2} \qquad 
        \sqrt{i} = e^{i \pi / 4} = \cos \frac{\pi}{4} + i \sin \frac{\pi}{4} = \frac{1}{\sqrt{2}} + i \frac{1}{\sqrt{2}}
    \]
\end{defn*}



\end{document}
